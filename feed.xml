<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-01-01T16:21:03-08:00</updated><id>/feed.xml</id><title type="html">Information Geometry in Machine Learning</title><subtitle>Blog website for Information Geometry in ML.</subtitle><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><entry><title type="html">Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods</title><link href="/posts/2021/12/Geomopt06/" rel="alternate" type="text/html" title="Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods" /><published>2021-12-22T00:00:00-08:00</published><updated>2021-12-22T00:00:00-08:00</updated><id>/posts/2021/12/Geomopt06</id><content type="html" xml:base="/posts/2021/12/Geomopt06/">&lt;p&gt;Warning: working in Progress (incomplete)&lt;/p&gt;

&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post should show that we can efficiently implement natural-gradient methods in many cases.&lt;/p&gt;

&lt;p&gt;We will give an informal introduction with a focus on high level of ideas.&lt;/p&gt;

&lt;h1 id=&quot;handling-parameter-constraints&quot;&gt;Handling Parameter Constraints&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Unfortunately, the connection between natural-gradient descent and mirror desecent breaks down when the natural
parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is constrained.
Since the Legendre transformation is defined in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; , which implies the dual (expectation) parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; is constrained in general.&lt;/p&gt;

&lt;p&gt;The following example illustrates this point.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example: Univariate Gaussian family&lt;/p&gt;

  &lt;p&gt;We consider this family as discussed in &lt;a href=&quot;#exponential-family&quot;&gt;the previous section&lt;/a&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&amp;gt;0 \}$&lt;/code&gt; with mean $\mu$ and variance $\sigma$.&lt;/p&gt;

  &lt;p&gt;It can be re-expressed in an exponential form as&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;amp;= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2\sigma} \\ \frac{\mu}{\sigma}  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} w^2 \\ w  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ]     )   \\
\end{aligned}
$$&lt;/code&gt;
where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sigma= -\frac{1}{2\eta_1} $&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu = -\frac{\eta_2}{2\eta_1}$&lt;/code&gt;, and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\mathbf{\eta}) = \frac{1} {2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ] = \frac{1}{2} [ \log ( 2\pi ) + \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;The natural parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta= \{ (\eta_1,\eta_2) | \eta_1&amp;lt;0 , \eta_2 \in \mathcal{R} \}$&lt;/code&gt; is a constrained open set in $\mathcal{R}^2$, where $K=2$.&lt;/p&gt;

  &lt;p&gt;The corresponding expectation parameter is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m} = E_{q(w|\eta)}[ \mathbf{T}_\eta (w) ] = [ \mu^2+\sigma , \mu ] $&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;The expectation parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m= \{ (m_1,m_2) | m_1 - m_2^2 &amp;gt;0 , m_2 \in \mathcal{R} \}$&lt;/code&gt; is a constrained open set in $\mathcal{R}^2$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Recall that when the natural parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is constrained,
the expectation space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m $&lt;/code&gt; in general is also constrained (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m \neq \mathcal{R}^K$&lt;/code&gt;).
In this case,
(constrained) mirror descent in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; in general does not give us the same update in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt; as
natural-gradient descent.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Proof by contradiction:&lt;/p&gt;

  &lt;p&gt;Suppose when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m \neq \mathcal{R}^K$&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; gives the same update in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt; in general.&lt;/p&gt;

  &lt;p&gt;By the definition of (constrained) mirror descent in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt;, the expectation parameter must satisfy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}_{k+1} \in \Omega_m$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;Therefore, the corresponding natural parameter must satisfy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\eta}_{k+1} \in \Omega_\eta$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;By our hypothesis, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\eta}_{k+1}$&lt;/code&gt; is updated according to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt; and it must satisfy the natural parameter constraint.&lt;/p&gt;

  &lt;p&gt;However, it is obvious that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt; in general does not satisfy the natural parameter constraint when the step-size $\alpha$ is
large enough since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;  is just a proper open subset of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;This is a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The following example shows that it is also possible that
the natural parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is unconstrained (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta = \mathcal{R}^K$&lt;/code&gt;) while
the expectation space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m $&lt;/code&gt; is still constrained (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m \neq \mathcal{R}^K$&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Recall that  in Part IV, we discuss 
&lt;a href=&quot;/posts/2021/11/Geomopt04/#natural-gradient-faces-of-natural-gradient-descent&quot;&gt;many faces of NGD&lt;/a&gt; in unconstrained cases.  These methods could also be exteneded in constrained cases to handle the parameter constraint.&lt;/p&gt;

&lt;h2 id=&quot;proximal-ngd--projected-ngd-and-constrained-mirror-descent&quot;&gt;Proximal NGD,  Projected NGD, and (Constrained) Mirror Descent&lt;/h2&gt;

&lt;p&gt;As we discussed before, natural-gradient descent and mirror desecent in general are &lt;strong&gt;distinct&lt;/strong&gt; methods when the natural parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is constrained.&lt;/p&gt;

&lt;p&gt;A straightforward approach from natural-gradient descent is the projected natural-gradient descent.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\eta_{k+1} \leftarrow \arg\min_{ \color{blue} {z} \in \Omega_\eta} \|\eta_k - \alpha
\mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k) -\color{blue} {\mathbf{z}} \|^2_{ \color{red}{ \mathbf{F}_\eta(\eta_k)} }
\end{aligned}\tag{6}\label{6}
$$&lt;/code&gt; where we should use 
the &lt;a href=&quot;/posts/2021/10/Geomopt02/#distance-induced-by-the-fisher-rao-metric&quot;&gt;weighted inner product&lt;/a&gt; with the same FIM highlighted in red.&lt;/p&gt;

&lt;p&gt;On the other hand, the constrained mirror descent in the expectation space remains the same as in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt;. 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{ \color{blue} {x} \in \Omega_m}\{ \langle \nabla_m \ell(\mathbf{m}_k), \color{blue}{\mathbf{x}}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\color{blue}{\mathbf{x}},\mathbf{m}_k) \}
\end{aligned}
$$&lt;/code&gt;
where 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_m \ell(\mathbf{m}_k) = \nabla_m f( \underbrace{ \eta(\mathbf{m}_k)}_{=\eta_k} )=  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We could also perform the constrained mirror descent in the natural parameter space as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{\eta}_{k+1} \leftarrow \arg \min_{\color{blue}{y} \in \Omega_\eta}\{ \langle \nabla_\eta f(\mathbf{\eta}_k), \color{blue}{\mathbf{y}}-\mathbf{\eta}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A_\eta}(\color{blue}{\mathbf{y}},\mathbf{\eta}_k) \}
\end{aligned}\tag{7}\label{7}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Recall that in
&lt;a href=&quot;/posts/2021/11/Geomopt04/#natural-gradient-descent-as-unconstrained-proximal-gradient-descent&quot;&gt;Part IV&lt;/a&gt;,
we show that natural-gradient descent can be viewed as an unconstrained proximal-gradient method, where we use the
second-order Taylor expansion of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\mathbf{y})]$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$y=\eta_k$&lt;/code&gt;.
We could also obtain proximal natural-gradient descent without the Taylor expansion as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\eta_{k+1} \leftarrow \arg\min_{\color{blue}{y} \in  \Omega_\eta  } \{ \langle \nabla_\eta f(\eta_k),\color{blue}{\mathbf{y}}-\eta_k \rangle   + \frac{1}{\alpha} \underbrace{ \mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\color{blue}{\mathbf{y}})]}_{ = \mathrm{B}_{A_\eta}(\mathbf{\eta}_k,\color{blue}{\mathbf{y}})}  \} 
\end{aligned}\tag{8}\label{8}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;These methods could be very difficult to solve since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; can be an arbitrary open subset in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;.
Moreover, in classical settings, a Bregman divergence is often defined in a closed set instead of an
open constrained subset.&lt;/p&gt;

&lt;h2 id=&quot;using-an-adaptive-step-size&quot;&gt;Using an Adaptive Step-size&lt;/h2&gt;

&lt;p&gt;When the step-size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\alpha$&lt;/code&gt; is small enough, the connection between natural-gradient descent and mirror desecent could
still hold.&lt;/p&gt;

&lt;p&gt;Therefore, one idea is to use an adaptive step-size to satisfy the parameter constraint at each iteration.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha_k \nabla_m \ell(\mathbf{m}_k)
\end{aligned}\tag{9}\label{9}
$$&lt;/code&gt; where 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_m \ell(\mathbf{m}_k) =  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$&lt;/code&gt; and the step-size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\alpha_k$&lt;/code&gt; is selected  so that
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_{k+1} \in \Omega_\eta$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; is an open set in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, this update is valid when the step-size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\alpha_k$&lt;/code&gt; is small enough.&lt;/p&gt;

&lt;p&gt;However, for a general parameter constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt;, this approach can be inefficient due to the selection precedure and will often select an extremally small step-size
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\alpha_k$&lt;/code&gt;,
which greatly slows down the progression of the method.&lt;/p&gt;

&lt;h2 id=&quot;riemannian-gradient-descent&quot;&gt;Riemannian Gradient Descent&lt;/h2&gt;

&lt;p&gt;An alternative approach is to use Riemannian gradient descent as we discussed in 
&lt;a href=&quot;/posts/2021/11/Geomopt04/#riemannian-gradient-descent-and-its-non-linear-invariance&quot;&gt;Part IV&lt;/a&gt;, which is a generalization of natural-gradient descent. 
Note that this approach cannot be derived from mirror descent.&lt;/p&gt;

&lt;p&gt;To avoid solving the geodeisc ODE to get the manifold exponential map, we could use an (inexact) geodesic, which
induces a retraction map.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\eta_{k+1} \leftarrow \mathrm{Ret}_{\eta_k} (- \alpha  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k) )  
\end{aligned}\tag{10}\label{10}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;As mentioned in 
&lt;a href=&quot;/posts/2021/11/Geomopt04/#natural-gradient-descent-as-inexact-riemannian-gradient-descent&quot;&gt;Part IV&lt;/a&gt;,
we have to carefully select a retraction map to handle the parameter constraint.&lt;/p&gt;

&lt;p&gt;For a general parameter constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt;, it can be difficult to come out an efficient retraction map to satisfy
the constraint.&lt;/p&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><category term="Exponential Family" /><summary type="html">Warning: working in Progress (incomplete)</summary></entry><entry><title type="html">Part V: Efficient Natural-gradient Methods for Exponential Family</title><link href="/posts/2021/12/Geomopt05/" rel="alternate" type="text/html" title="Part V: Efficient Natural-gradient Methods for Exponential Family" /><published>2021-12-14T00:00:00-08:00</published><updated>2021-12-14T00:00:00-08:00</updated><id>/posts/2021/12/Geomopt05</id><content type="html" xml:base="/posts/2021/12/Geomopt05/">&lt;p&gt;Warning: working in Progress (incomplete)&lt;/p&gt;

&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post should show that we can efficiently implement natural-gradient methods in many cases.&lt;/p&gt;

&lt;p&gt;We will give an informal introduction with a focus on high level of ideas.&lt;/p&gt;

&lt;h1 id=&quot;exponential-family&quot;&gt;Exponential Family&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;An exponential family takes the following (canonical) form as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p(\mathbf{w}|\mathbf{\eta}) = h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta (\mathbf{\eta}) )
\end{aligned}
$$&lt;/code&gt; where   &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$C_\eta(\eta) :=  \int h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}(\mathbf{w}) \rangle ) d \mathbf{w}
$&lt;/code&gt; is the normalization constant.   &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\mathbf{\eta}):=\log C_\eta(\eta)$&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\eta(\mathbf{w})$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{T}_\eta(\mathbf{w})$&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;  are known as the log-partition function, the base measure, the sufficient statistics, and the &lt;strong&gt;natural&lt;/strong&gt; parameter,  respectively.&lt;/p&gt;

&lt;p&gt;The parameter space of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is determined so that the normalization constant is well-defined and (strictly and finitely) positive.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regular&lt;/strong&gt; natural parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;: parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is relatively open.&lt;/p&gt;

&lt;p&gt;In this post, we only consider
regular natural parametrizations since commonly used exponential family distributions have a regular natural parametrization.&lt;/p&gt;

&lt;p&gt;This natural parametrization is special since the inner product &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$&lt;/code&gt; is &lt;strong&gt;linear&lt;/strong&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;. As we will discuss later,  this linearity is essential.&lt;/p&gt;

&lt;p&gt;Readers should be aware of the following points when using an exponential family.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The support of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt; should not depend on parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The base measure and the log-partition function are only unique up to a constant as illustrated by the following example.
    &lt;details&gt;
  &lt;summary&gt;Example: Univariate Gaussian family as an exponential family&lt;/summary&gt;
  &lt;fieldset class=&quot;field-set&quot;&gt;
        &lt;blockquote&gt;

          &lt;p&gt;Recall that in &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families&quot;&gt;Part I&lt;/a&gt;, we consider this family as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&amp;gt;0 \}$&lt;/code&gt; with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt; and variance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sigma$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $&lt;/code&gt;.&lt;/p&gt;

          &lt;p&gt;We re-express it in an exponential form as&lt;/p&gt;

          &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\eta})  &amp;amp;= \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] \\
&amp;amp;= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2\sigma} \\ \frac{\mu}{\sigma}  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} w^2 \\ w  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ]     )   \\
\end{aligned}
$$&lt;/code&gt;
Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sigma= -\frac{1}{2\eta_1} $&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu = -\frac{\eta_2}{2\eta_1}$&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\mathbf{\eta}) = \frac{1} {2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ] = \frac{1}{2} [ \log ( 2\pi ) + \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $&lt;/code&gt;.&lt;/p&gt;

          &lt;p&gt;It is also valid that $ h_\eta({w}) = \frac{1}{\sqrt{2\pi}} $ and 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\mathbf{\eta}) =  \frac{1}{2} [  \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $&lt;/code&gt;.&lt;/p&gt;

          &lt;p&gt;We easily to verify that parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta= \{ (\eta_1,\eta_2) | \eta_1&amp;lt;0 , \eta_2 \in \mathcal{R} \}$&lt;/code&gt; is open in $\mathcal{R}^2$.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/fieldset&gt;
  &lt;/details&gt;
  &lt;/li&gt;
  &lt;li&gt;The log-partition function could be differentiable w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; even when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt; is discrete.
    &lt;details&gt;
  &lt;summary&gt;Example: Bernoulli family as an exponential family&lt;/summary&gt;
  &lt;fieldset class=&quot;field-set&quot;&gt;
        &lt;blockquote&gt;

          &lt;p&gt;Recall that in &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families&quot;&gt;Part I&lt;/a&gt;, we consider this family as
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \Big| 0&amp;lt;\pi&amp;lt;1 \}$&lt;/code&gt;&lt;/p&gt;

          &lt;p&gt;We re-express it in an exponential form as&lt;/p&gt;

          &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;amp;=  \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \\
&amp;amp;=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \log \frac{\pi}{1-\pi}}_{\eta} , \underbrace{ \mathcal{I}(w=0)}_{T_\eta(w) } \rangle - \log \frac{1}{1-\pi} )
\end{aligned}
$$&lt;/code&gt;
Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi = \frac{\exp(\eta)}{1+ \exp(\eta) } $&lt;/code&gt; , we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\mathbf{\eta}) =  \log \frac{1}{1-\pi} = \log(1+\exp(\eta))$&lt;/code&gt;. &lt;br /&gt;
We easily to verify that parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta= \{ \eta | \eta \in \mathcal{R} \}$&lt;/code&gt; is open in $\mathcal{R}$.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/fieldset&gt;
  &lt;/details&gt;
  &lt;/li&gt;
  &lt;li&gt;An invertiable linear reparametrization could also be a natural parametrization. This is one of the reasons using natural-gradient descent since it is linearly invariant.
    &lt;details&gt;
  &lt;summary&gt;Example: Natural parametrization is not unique&lt;/summary&gt;
  &lt;fieldset class=&quot;field-set&quot;&gt;
        &lt;blockquote&gt;

          &lt;p&gt;For simplicity, let’s assume set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\lambda := \{ \mathbf{U}^{-1} \eta | \eta \in \Omega_\eta \} = \Omega_\eta$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{U}$&lt;/code&gt; is a constant invertiable matrix.&lt;/p&gt;

          &lt;p&gt;Consider a linear reparametrization such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda=\mathbf{U}^{-1} \mathbf{\eta}$&lt;/code&gt;, parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; is also a natural parametrization as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p(\mathbf{w}|\mathbf{\lambda})
&amp;amp;= h_\eta(\mathbf{w}) \exp( \langle \mathbf{U}\mathbf{\lambda} , \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta( \mathbf{U}\mathbf{\lambda}) ) \\
&amp;amp;=  h_\eta(\mathbf{w})\exp( \langle \mathbf{\lambda} , \mathbf{U}^T \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta(\mathbf{U}\mathbf{\lambda}) ) \\
&amp;amp;= h_\lambda(\mathbf{w})  \exp( \langle \mathbf{\lambda} ,  \mathbf{T}_\lambda(\mathbf{w}) \rangle - A_\lambda(\mathbf{\lambda}) ) 
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\lambda(\mathbf{w}):= h_\eta(\mathbf{w})$&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{T}_\lambda(\mathbf{w}):= \mathbf{U}^T\mathbf{T}_\eta(\mathbf{w})$&lt;/code&gt;, and 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\lambda(\mathbf{\lambda}):= A_\eta(\mathbf{U}\mathbf{\lambda})$&lt;/code&gt;.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/fieldset&gt;
  &lt;/details&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;minimal-parametrizations-of-exponential-family&quot;&gt;Minimal Parametrizations of Exponential Family&lt;/h2&gt;

&lt;p&gt;Now, we discuss particular parametrizations of an exponential family. We could efficiently compute natural-gradients when
using this class of parametrizations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Minimal&lt;/strong&gt; natural parametrization: the corresponding sufficient stattistics &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{T}(\mathbf{w})$&lt;/code&gt; is linearly independent.&lt;/p&gt;

&lt;p&gt;A regular, minimal, and natural parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; has many nice properties.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It is an intrinstic parametrization as we discussed in
 &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families&quot;&gt;Part I&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is an open set in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, where
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt; is the number of entires of this parameter array.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The log-partition function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\eta)$&lt;/code&gt; is infinitely differentiable and strictly convex in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The FIM &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_\eta(\eta) = \nabla_\eta^2 A_\eta(\eta)$&lt;/code&gt; is positive-definite in its domain.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will only show the first property in this post. The remaining properties can be found in the literature.
Note that the linearity in the inner product &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$&lt;/code&gt;,  plays a key role in showing these properties.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;Claim: Regular, minimal, and natural parametrization is intrinstic&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Proof by contradiction:&lt;/p&gt;

      &lt;p&gt;Recall that a parametrization is intrinstic if 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt; partial derivatives 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \partial_{\eta_i} \log p(\mathbf{w}|\eta) \} $&lt;/code&gt;  are linearly independent.
Since the parameter space is open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt; and the log-partition function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\eta)$&lt;/code&gt; is differentiable,  these partial derivatives are well-defined and can be computed as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\eta_i} \log p(\mathbf{w}|\eta) = \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle - \partial_{\eta_i}A_\eta(\eta) 
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{e}_i$&lt;/code&gt; is an one-hot/unit array which has zero in all entries except the $i$-th entry.&lt;/p&gt;

      &lt;p&gt;If these partial derivatives are linearly dependent, there exist a set of non-zero constant &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_i$&lt;/code&gt; such that
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sum_i c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta)= 0 $&lt;/code&gt;, where the value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_i$&lt;/code&gt; does not depent on  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
0 &amp;amp;= \sum_{i=1}^{K} c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta) \\
&amp;amp;= \sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle -   c_i\partial_{\eta_i}A_\eta(\eta) \\
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;Since  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_i$&lt;/code&gt; is a non-zero constant and its value does not depent on  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt;, we must have
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
0 =\sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle ,
\end{aligned}
$$&lt;/code&gt; which implies that
the sufficient stattistics &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{T}(\mathbf{w})$&lt;/code&gt; is linearly dependent. This is a contradiction since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is a minimal natural parametrization.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;p&gt;Now, we give an example of a regular, minimal and natural parametrization.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Example: Minimal parametrization for multivariate Gaussians&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;blockquote&gt;

      &lt;p&gt;Consider a $d$-dimensional Gaussian family.&lt;/p&gt;

      &lt;p&gt;We specify a parameterization $\mathbf{\tau}$ of the  family as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(\mathbf{w} |\mathbf{\mu},\mathbf{\Sigma}) \Big| \mathbf{\mu} \in \mathcal{R}^d, \mathbf{\Sigma}   \succ \mathbf{0} \}$&lt;/code&gt; with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\mu}$&lt;/code&gt; and covariance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\Sigma}$&lt;/code&gt;.&lt;/p&gt;

      &lt;p&gt;We re-express it in an exponential form as&lt;/p&gt;

      &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\lambda})  &amp;amp;= \frac{1}{\sqrt{ \mathrm{det}( 2\pi \Sigma )} } \exp [- \frac{1}{2} (\mathbf{w}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{w}-\mathbf{\mu})  ] \\
&amp;amp;= \underbrace{ \exp(0) }_{  h_\lambda({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \mathrm{vec}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\lambda} }  ,  \underbrace{\begin{bmatrix} \mathrm{vec}( \mathbf{w} \mathbf{w}^T) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\lambda ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vec}()$&lt;/code&gt; is the vectorization function
and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; is a $(d+d^2)$-dim array.&lt;/p&gt;

      &lt;p&gt;Parametrization $\lambda$ is a natural and regular parametrization. However, it is NOT a minimal
parametrization since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w} \mathbf{w}^T$&lt;/code&gt; is symmetric and therefore the sufficient statistics is linearly dependent.
It can be shown that $\Omega_\lambda$ is relatively open but not open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K=d+d^2$&lt;/code&gt;.&lt;/p&gt;

      &lt;p&gt;A minimal natural parametrization $\eta$ should be defined as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;amp;= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \mathrm{vech}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} \mathrm{vech}( \mathbf{w} \mathbf{w}^T) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}()$&lt;/code&gt; is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization&quot;&gt;half-vectorization function&lt;/a&gt; 
and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is a $(d+\frac{d(d+1)}{2})$-dim array.
It can be shown that $\Omega_\eta$ is open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K=d+\frac{d(d+1)}{2}$&lt;/code&gt;.&lt;/p&gt;

      &lt;p&gt;As we discussed in &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-steepest-direction&quot;&gt;Part II&lt;/a&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}(\Sigma^{-1})$&lt;/code&gt; is an intrinstic parameterization while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vec}(\Sigma^{-1})$&lt;/code&gt; is not.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;p&gt;The following example illustrates
a non-minimal natural parametrization&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Example: Non-minimal parametrization for Bernoulli family&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;blockquote&gt;

      &lt;p&gt;We consider this family in &lt;a href=&quot;#exponential-family&quot;&gt;the previous section&lt;/a&gt; with another
parametrization dicussed in
&lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families&quot;&gt;Part I&lt;/a&gt;.
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \frac{\pi_1}{\pi_1+\pi_2} + \mathcal{I}(w=1) \frac{\pi_2}{\pi_1+\pi_2} \Big| \pi_1&amp;gt;0, \pi_2&amp;gt;0 \}$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;We re-express it in another exponential form as&lt;/p&gt;

      &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;amp;=  \mathcal{I}(w=0) \frac{\pi_1}{\pi_1+\pi_2} + \mathcal{I}(w=1) \frac{\pi_2}{\pi_1+\pi_2} \\
&amp;amp;=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \begin{bmatrix} \log \frac{\pi_1}{\pi_1+\pi_2} \\  \log \frac{\pi_2}{\pi_1+\pi_2} \end{bmatrix} }_{\eta} , \underbrace{ \begin{bmatrix} \mathcal{I}(w=0) \\   \mathcal{I}(w=1)\end{bmatrix} }_{\mathbf{T}_\eta(w) } \rangle -\underbrace{ 0}_{A_\eta(\eta)} )
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;The natural parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta= \{ (\eta_1,\eta_2) | \exp(\eta_1)+\exp(\eta_2) = 1 \}$&lt;/code&gt; is not open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt;.&lt;/p&gt;

      &lt;p&gt;This is not a minimal natural parametrization since the sufficient statistics $\mathbf{T}_\eta(w)$ is linearly dependent as 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{I}(w=0)+\mathcal{I}(w=1)=1$&lt;/code&gt;.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;h1 id=&quot;efficient-natural-gradient-computation&quot;&gt;Efficient Natural-gradient Computation&lt;/h1&gt;

&lt;p&gt;In general, natural-gradient computation can be challenging due to the inverse of the Fisher matrix.
In cases of an exponential family,  natural-gradient computation often can be quite efficient without directly computing the inverse of the Fisher matrix.&lt;/p&gt;

&lt;p&gt;We will assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is a reguar, minimal, natural parametrization.&lt;/p&gt;

&lt;h2 id=&quot;expectation-parametrization&quot;&gt;Expectation Parametrization&lt;/h2&gt;
&lt;p&gt;We introduce a dual parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m} := E_{p(w|\eta)}[ \mathbf{T}_\eta(\mathbf{w}) ] $&lt;/code&gt;, which is known as the expectation parametrization. This new parametrization plays a key role for the efficient natural-gradient computation.&lt;/p&gt;

&lt;p&gt;Recall that in
&lt;a href=&quot;/posts/2021/11/Geomopt04/#the-hessian-is-not-a-valid-manifold-metric&quot;&gt;Part IV&lt;/a&gt;,  we use the identity of the score function. This identity also shows us a connection 
between these two parametrizations.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{0} &amp;amp; = E_{p(w|\eta)} [ \nabla_\eta \log p(\mathbf{w}|\eta) ]\\
&amp;amp;=E_{p(w|\eta)} [ \mathbf{T}_\eta(\mathbf{w}) - \nabla_\eta A_\eta(\eta) ] \\
&amp;amp;= \mathbf{m}  - \nabla_\eta A_\eta(\eta)
\end{aligned}\tag{1}\label{1}
$$&lt;/code&gt;  which is a valid Legendre (dual) transformation since 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\eta^2 A_\eta(\eta)$&lt;/code&gt; is positive-definite in its domain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;expectation-parameter-space&quot;&gt;Expectation Parameter Space&lt;/h2&gt;
&lt;p&gt;We can view
the expectation parameter as an ouput of
a continous map 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{m}(\eta):=\nabla_\eta A_\eta(\eta),
\end{aligned}\tag{2}\label{2}
$$&lt;/code&gt;
 where the input space is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We define the expectation parameter space as the output space of the map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m :=\{\mathbf{m}(\eta) | \eta \in \Omega_\eta \}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\eta^2 A_\eta(\eta)$&lt;/code&gt; is positive-definite in open set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;, we can show that there exists an one-to-one relationship between the
natural parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; and the expectation parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt;, which implies that map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}(\cdot)$&lt;/code&gt; is injective.&lt;/p&gt;

&lt;p&gt;Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, we can show that
the expectation parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; is also open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt; due to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Invariance_of_domain&quot;&gt;invariance of domain&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;natural-gradient-computation&quot;&gt;Natural-gradient Computation&lt;/h2&gt;
&lt;p&gt;Note that the FIM of the exponential family under parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}_\eta(\eta)=\nabla_\eta^2 A_\eta(\eta) = \nabla_\eta \mathbf{m}
\end{aligned}\tag{3}\label{3}
$$&lt;/code&gt; which means this FIM is a Jacobian matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{J}=\nabla_\eta \mathbf{m}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As we discussed in
&lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-steepest-direction&quot;&gt;Part II&lt;/a&gt;,
a natural-gradient w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\eta)$&lt;/code&gt; can be computed as below, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\eta=\nabla_\eta f(\eta)$&lt;/code&gt; is a Euclidean gradient w.r.t.  natural parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\hat{\mathbf{g}}_\eta &amp;amp; = \mathbf{F}^{-1}_\eta(\eta) \mathbf{g}_\eta \\
&amp;amp;= (\nabla_\eta \mathbf{m} )^{-1} [ \nabla_\eta f(\eta) ] \\
&amp;amp;= [\nabla_{m} \eta ] [ \nabla_\eta f(\eta) ] \\
&amp;amp;=  \nabla_{m} f(\eta)  
\end{aligned}\tag{4}\label{4}
$$&lt;/code&gt; where 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_{m} f( \eta )$&lt;/code&gt; is a Euclidean gradient w.r.t. expectation parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt; and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta=\eta( \mathbf{m} )$&lt;/code&gt;
can be viewed  as a function of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Therefore, we can  efficinetly compute natural-gradients w.r.t. natural parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;  if we  can  easily compute Euclidean gradients w.r.t. its expectation parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;to do: add the Gaussian example&lt;/p&gt;

&lt;h1 id=&quot;natural-gradient-descent-as-unconstrained-mirror-descent&quot;&gt;Natural-gradient Descent as Unconstrained Mirror Descent&lt;/h1&gt;

&lt;p&gt;we will assume natural parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is both regular and minimal.&lt;/p&gt;

&lt;p&gt;In exponential family cases, we will show that natural-gradient descent becomes mirror descent when the natural parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is unconstrained.&lt;/p&gt;

&lt;p&gt;Since mirror descent is defined by using a Bregman divergence, we first introduce the Bregman divergence.&lt;/p&gt;

&lt;h2 id=&quot;bregman-divergence&quot;&gt;Bregman Divergence&lt;/h2&gt;
&lt;p&gt;Given a strictly convex function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Phi(\cdot)$&lt;/code&gt; in its domain, a Bregman divergence equipped with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Phi(\cdot)$&lt;/code&gt; is defined as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{B}_\Phi(\mathbf{x},\mathbf{y}):= \Phi(\mathbf{x})- \Phi(\mathbf{y}) - \langle \nabla \Phi(\mathbf{y}), (\mathbf{x}-\mathbf{y}) \rangle
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In particular, the Kullback–Leibler (KL) divergence is a Bregman divergence under natural parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\, &amp;amp; \,\mathrm{KL} [p(\mathbf{w}| \eta_1 ) || p(\mathbf{w}|\color{red}{\eta_2})]\\
=&amp;amp; \,E_{p(w|\eta_1)} [ \log \frac{p(\mathbf{w}|\eta_1)} {p(\mathbf{w}|\eta_2)} ] \\
=&amp;amp; \,E_{p(w|\eta_1)} [  \langle \eta_1-\eta_2 , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta(\eta_1) + A_\eta(\eta_2) ] &amp;amp; ( p(\mathbf{w}|\eta) \text{ is an exponential family}) \\
=&amp;amp; \,A_\eta(\eta_2) - A_\eta(\eta_1) - E_{p(w|\eta_1)} [  \langle \eta_2-\eta_1,  \mathbf{T}_\eta (\mathbf{w}) \rangle ] \\
=&amp;amp; \,A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \underbrace{ E_{p(w|\eta_1)} [ \mathbf{T}_\eta (\mathbf{w}) ] }_{ \nabla_\eta A_\eta(\eta_1) }  \rangle    \\
=&amp;amp; \, \mathrm{B}_{A_\eta}(\color{red} {\mathbf{\eta}_2},  \mathbf{\eta}_1 ) &amp;amp; ( A_\eta(\eta) \text{ is strictly convex})
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We denote the expectation parameter as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt;.
The convex conjugate (Legendre transformation) of the log-partition function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta$&lt;/code&gt; is defined as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
A^*_\eta( \mathbf{m}) &amp;amp;:= \sup_{x} \{ \langle \mathbf{x},\mathbf{m} \rangle - A_\eta(\mathbf{x}) \} \\
&amp;amp;= \langle \mathbf{\eta},\mathbf{m} \rangle - A_\eta(\mathbf{\eta})  \,\,\,\, (\text{the supremum attains at } \mathbf{x}=\eta )\\
\end{aligned}\tag{5}\label{5}
$$&lt;/code&gt; where
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta=\eta( \mathbf{m} )$&lt;/code&gt;
should be viewed  as a function of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Notice that
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\nabla_{\mathbf{m}} A^*_\eta( \mathbf{m})
&amp;amp;= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - \nabla_{\mathbf{m}} A_\eta(\mathbf{\eta}) \\
&amp;amp;= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - [\nabla_{\mathbf{m}} \eta] \underbrace{ [\nabla_\eta A_\eta(\mathbf{\eta})] }_{ = \mathbf{m}}\\
&amp;amp;= \mathbf{\eta}
\end{aligned}\tag{6}\label{6}
$$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The convex conjugate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A^*_\eta( \mathbf{m})$&lt;/code&gt; is strictly convex w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt; since the Hessian &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_m^2 A^*_\eta( \mathbf{m})$&lt;/code&gt;
is positive-definite as shown below.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})
&amp;amp;= \nabla_{\mathbf{m}} \mathbf{\eta}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that due to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt;,
the FIM &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_\eta(\eta)$&lt;/code&gt; under natural parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\eta}$&lt;/code&gt;  is the Jacobian matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{J}= \nabla_{\eta} \mathbf{m}$&lt;/code&gt;
and 
positive-definite.&lt;/p&gt;

&lt;p&gt;Therefore, it is easy to see that
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m}) = 
\mathbf{F}^{-1}_\eta(\eta),
$&lt;/code&gt; which is 
positive-definite and therefore strictly convex.&lt;/p&gt;

&lt;p&gt;By the &lt;a href=&quot;/posts/2021/11/Geomopt03/#transformation-rule-for-the-fisher-information-matrix&quot;&gt;transformation rule&lt;/a&gt; of the FIM,  we have the following relationship.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}_{\eta} (\eta) &amp;amp; = \mathbf{J}^T \mathbf{F}_{m}(\mathbf{m}) \mathbf{J} \\
&amp;amp;= \mathbf{F}^T_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) \\
&amp;amp;= \mathbf{F}_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) &amp;amp; (\text{the FIM is symmetric})
\end{aligned}
$$&lt;/code&gt;  which implies that
the FIM under expectation parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_m(\mathbf{m})=\mathbf{F}^{-1}_\eta(\eta) = \nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})= \nabla_{\mathbf{m}} \mathbf{\eta}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Moreover, we have the following identity since by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\eta)=\langle \mathbf{\eta},\mathbf{m} \rangle- A^*_\eta( \mathbf{m}) $&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{B}_{A_\eta}(\mathbf{\eta}_2, \color{red}{\mathbf{\eta}_1 })
&amp;amp;= A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \overbrace{ \nabla_\eta A_\eta(\eta_1) }^{= \mathbf{m}_1} \rangle    \\
&amp;amp;= [  \langle \mathbf{\eta}_2,\mathbf{m}_2 \rangle- A^*_\eta( \mathbf{m}_2) ]   
-[  \langle \mathbf{\eta}_1,\mathbf{m}_1 \rangle- A^*_\eta( \mathbf{m}_1) ]
-\langle \eta_2-\eta_1, \mathbf{m}_1 \rangle \\
&amp;amp;=  A^*_\eta( \mathbf{m}_1) - A^*_\eta( \mathbf{m}_2) -    
\langle \mathbf{m}_1-\mathbf{m}_2, \underbrace{ \eta_2}_{ = \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_2)} \rangle\\
&amp;amp;= \mathrm{B}_{A^*_\eta}( \color{red}{ \mathbf{m}_1 },\mathbf{m}_2) &amp;amp; (\text{the order is changed})
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;mirror-descent&quot;&gt;Mirror Descent&lt;/h2&gt;

&lt;p&gt;Now, we give the definition of mirror descent.&lt;/p&gt;

&lt;p&gt;Consider the following optimization problem over a convex domain denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\theta$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\theta \in \Omega_\theta} \ell(\mathbf{\theta})
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Given a strictly convex function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Phi(\mathbf{\theta})$&lt;/code&gt; in the domain , mirror
descent with step-size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\alpha$&lt;/code&gt; is defined as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{\theta}_{k+1} \leftarrow \arg \min_{x \in \Omega_\theta}\{ \langle \nabla_\theta \ell(\mathbf{\theta}_k), \mathbf{x}-\mathbf{\theta}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{\Phi}(\mathbf{x},\mathbf{\theta}_k) \}
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{B}_\Phi(\cdot,\cdot)$&lt;/code&gt; is a Bregman divergence  equipped  with the strictly convex function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Phi(\cdot)$&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;natural-gradient-descent-as-mirror-descent&quot;&gt;Natural-gradient Descent as Mirror Descent&lt;/h2&gt;

&lt;p&gt;To show natural-gradient descent as mirror descent, we have to make the following assumption.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Additional&lt;/strong&gt; assumption:
&lt;strong&gt;Natural&lt;/strong&gt; parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;
is unconstrainted (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta=\mathcal{R}^K$&lt;/code&gt;), where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt; is the number of entries of parameter array &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The following example illustrates that the expectation space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; is constrained even when
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is unconstrained.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Example: $\Omega_m$ is constrained while $\Omega_\eta$ is unconstrained&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Example: Bernoulli family&lt;/p&gt;

      &lt;p&gt;We consider this family as discussed in &lt;a href=&quot;#exponential-family&quot;&gt;the previous section&lt;/a&gt;
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \Big| 0&amp;lt;\pi&amp;lt;1 \}$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;We re-express it in an exponential form as&lt;/p&gt;

      &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;amp;=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \log \frac{\pi}{1-\pi}}_{\eta} , \underbrace{ \mathcal{I}(w=0)}_{T_\eta(w) } \rangle - \log \frac{1}{1-\pi} )
\end{aligned}
$$&lt;/code&gt;
where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi = \frac{\exp(\eta)}{1+ \exp(\eta) } $&lt;/code&gt; and&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\mathbf{\eta}) =  \log \frac{1}{1-\pi} = \log(1+\exp(\eta))$&lt;/code&gt;.&lt;/p&gt;

      &lt;p&gt;The natural parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta= \{ \eta | \eta \in \mathcal{R} \}=\mathcal{R}^1$&lt;/code&gt;.&lt;/p&gt;

      &lt;p&gt;The corresponding expectation parameter is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$m = E_{q(w|\eta)}[ T_\eta (w) ] = \pi$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;The expectation parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m= \{ m| 0&amp;lt;m&amp;lt;1 \}$&lt;/code&gt; is a constrained open set in $\mathcal{R}^1$.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;p&gt;Now, consider the following mirror descent in the expectation parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{x \in \Omega_m}\{ \langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k) \}
\end{aligned}\tag{7}\label{7}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_m \ell(\mathbf{m}_k):= \nabla_m f(\eta(\mathbf{m}_k))$&lt;/code&gt; and the Bregman divergence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{B}_{A^*_\eta}(\cdot,\cdot)$&lt;/code&gt; is well-defined
since  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A^*_\eta$&lt;/code&gt; is strcitly convex in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt;.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;Claim: when $\Omega_\eta = \mathcal{R}^K$, the solution of $\eqref{7}$ is equivalent to $\eta_{k+1}  \leftarrow  \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)$ &lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Proof :&lt;/p&gt;

      &lt;p&gt;Denote 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
u(\mathbf{x}) &amp;amp;:=\langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha} [ \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k)] \\
&amp;amp; = \langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  [A^*_\eta( \mathbf{x}) - A^*_\eta( \mathbf{m}_k) -    
\langle \mathbf{x}-\mathbf{m}_k, \underbrace{ \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_k)}_{ = \eta_k} \rangle ],
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;A stationary point of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{x}}$&lt;/code&gt;,  must satisfy the following
condition.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{0} &amp;amp;= \nabla_x u(\hat{\mathbf{x}}) &amp;amp; (\mathbf{m}_k \text{ is considered to be a constant}) \\
&amp;amp;= \nabla_m \ell(\mathbf{m}_k)+ \frac{1}{\alpha}  [ \nabla_x A^*_\eta( \hat{\mathbf{x}})  -  \eta_k ],
\end{aligned}
$$&lt;/code&gt; which implies that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_x A^*_\eta( \hat{\mathbf{x}}) =  \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;We first show that there exists a stationary point in the domain (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{x}} \in \Omega_m$&lt;/code&gt;).
Let’s denote &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\eta}_{k+1}:= \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)$&lt;/code&gt;. Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is unconstrained, it
is obvious that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\eta}_{k+1} \in \Omega_\eta$&lt;/code&gt;.
By &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}(\mathbf{\eta}_{k+1}) =\nabla_\eta A_\eta( \mathbf{\eta}_{k+1}) \in
\Omega_m$&lt;/code&gt;. Notice that,  by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt;, we have  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_m A^*_\eta ( \mathbf{m}(\mathbf{\eta}_{k+1}) ) = \mathbf{\eta}_{k+1}$&lt;/code&gt;, which implies that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}(\mathbf{\eta}_{k+1})$&lt;/code&gt; is a stationary point and 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}(\mathbf{\eta}_{k+1}) \in \Omega_m$&lt;/code&gt;.&lt;/p&gt;

      &lt;p&gt;Moreover,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}(\mathbf{\eta}_{k+1})$&lt;/code&gt; is 
the unique  solution of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_x^2 u(\mathbf{x}) =\nabla_x^2 A^*_\eta(
{\mathbf{x}})$&lt;/code&gt; is positive-definite for any  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x} \in \Omega_m$&lt;/code&gt; and therefore strictly convex.
In other words, 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}_{k+1} = \mathbf{m}( {\mathbf{\eta}_{k+1}}) $&lt;/code&gt;.&lt;/p&gt;

      &lt;p&gt;In summary, when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta = \mathcal{R}^K$&lt;/code&gt;, the unique solution of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; is 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\eta_{k+1} &amp;amp; \leftarrow  \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k) \\
\mathbf{m}_{k+1} &amp;amp; \leftarrow  \nabla_\eta A_\eta( {\mathbf{\eta}_{k+1}})
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;p&gt;By the claim,
mirror descent of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; in &lt;strong&gt;expectation&lt;/strong&gt; parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; is equivalent to
the following update
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)
= \eta_k - \alpha\nabla_m f( \underbrace{ \eta(\mathbf{m}_k) }_{= \eta_k}),
\end{aligned}\tag{8}\label{8}
$$&lt;/code&gt; which is exactly natural gradient
descent in &lt;strong&gt;natural&lt;/strong&gt; parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta=\mathcal{R}^K$&lt;/code&gt; since by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt;, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_m \ell(\mathbf{m}_k) = \nabla_m f( \eta_k)= \mathbf{F}
_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$&lt;/code&gt;.&lt;/p&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><category term="Exponential Family" /><summary type="html">Warning: working in Progress (incomplete)</summary></entry><entry><title type="html">Part IV: Natural and Riemannian Gradient Descent</title><link href="/posts/2021/11/Geomopt04/" rel="alternate" type="text/html" title="Part IV: Natural and Riemannian Gradient Descent" /><published>2021-11-15T00:00:00-08:00</published><updated>2021-11-15T00:00:00-08:00</updated><id>/posts/2021/11/Geomopt04</id><content type="html" xml:base="/posts/2021/11/Geomopt04/">&lt;p&gt;Warning: working in Progress (incomplete)&lt;/p&gt;

&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post should help readers to understand natural-gradient descent and Riemannian gradient descent.
We also discuss some invariance property of natural-gradient descent, Riemannian gradient descent, and Newton’s method.&lt;/p&gt;

&lt;p&gt;We will give an informal introduction with a focus on high level of ideas.&lt;/p&gt;

&lt;h1 id=&quot;two-kinds-of-spaces&quot;&gt;Two kinds of Spaces&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;We will discuss (Riemannian) gradient spaces and parameter spaces for gradient-based updates.&lt;/p&gt;

&lt;p&gt;As we disucssed in &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt;, the parameter space $\Omega_\tau$ and the tangent space denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$T\mathcal{M}_{\tau_0}$&lt;/code&gt; at point $\tau_0$ are different spaces. 
Given  &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;intrinsic parametrization&lt;/a&gt;  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;,  the tangent space is a vector space and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$T\mathcal{M}_{\tau_0}=\mathcal{R}^K$&lt;/code&gt; while the parameter space $\Omega_\tau$ is like a local vector space in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, where $K$ is the dimension of the manifold. Moreover, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; is often an open (proper) subset of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$T\mathcal{M}_{\tau_0}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In manifold cases, we have to explicitly distinguish the difference between the representation of a point (parameter) and a vector (Riemannian gradient).
The following figure illustrates the difference between the &lt;strong&gt;domain&lt;/strong&gt; of these two spaces. Moreover, the &lt;strong&gt;norm/distance&lt;/strong&gt; in each of these two spaces is defined differently. 
Mathematically speaking, the (Riemannian) gradient space is much simpler and nicer than the parameter space.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sphere.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;natural-gradient-descent-in-an-intrinsic-parameter-space&quot;&gt;Natural-gradient Descent in an Intrinsic Parameter Space&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;Using intrinstic parametrization $\tau$, we can perform a natural-gradient update known as natural-gradient descent (NGD). 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{1}\label{1}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\tau_k}$&lt;/code&gt; is a natural/Riemannian gradient evaluated at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_{k}$&lt;/code&gt; and $\alpha&amp;gt;0$ is a step-size.&lt;/p&gt;

&lt;p&gt;This update in Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; is inspired by the standard vector update in the &lt;strong&gt;gradient space&lt;/strong&gt;.
By choosing an intrinstic parametrization, this update is also valid in the &lt;strong&gt;parameter space&lt;/strong&gt; when the step-size is small
enough.&lt;/p&gt;

&lt;p&gt;The update in the parameter space is valid since the parameter space $\Omega_\tau$ has a local vector-space structure thanks to the use of an intrinsic parametrization.
However, when $\Omega_\tau$ is a proper subset of $T\mathcal{M}_{\tau_k}$ (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau \neq T\mathcal{M}_{\tau_k} $&lt;/code&gt;), the update in Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; is valid only when the step-size $\alpha$ is small enough so that  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_{k+1} \in \Omega_\tau$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/span&gt;:
Using a small step-size could be an issue since it can greatly slow down the progression of natural-gradient
descent in practice.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;Example: (NGD in a constrained space)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;blockquote&gt;

      &lt;p&gt;Consider a 1-dimensional  Gaussian family.
We specify an intrinsic parameterization $\mathbf{\tau}$  as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&amp;gt;0 \}$&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\mu,\sigma) $&lt;/code&gt;. &lt;br /&gt;&lt;/p&gt;

      &lt;p&gt;We have to properly select the step-size $\alpha$ for natural-gradient descent in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; due to the positivity constraint in $\sigma$.&lt;/p&gt;

      &lt;p&gt;In multivariate Gaussian cases, we have to handle a positive-definite constraint.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;h1 id=&quot;natural-gradient-descent-is-linearly-invariant&quot;&gt;Natural-gradient Descent is Linearly Invariant&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;Recall that in &lt;a href=&quot;/posts/2021/11/Geomopt03/#parameter-transform-and-invariance&quot;&gt;Part III&lt;/a&gt;, we have shown that natural-gradients are invaraint under any intrinsic parameter transformation.
The parameter transformation can be non-linear.&lt;/p&gt;

&lt;p&gt;It is natural to expect that natural-gradient descent has a similar property. However, natural-gradient descent is only invariant under  an intrinsic &lt;strong&gt;linear&lt;/strong&gt; transformation. Note that Newton’s method is also linearly invariant while Euclidean gradient descent is not.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/span&gt;: Be aware of the difference of the invaraince property between natural-gradient and natural-gradient descent.&lt;/p&gt;

&lt;p&gt;Let’s consider the following (scalar) optimization problem on a manifold $\mathcal{M}$ with the Fisher-Rao metric $F$.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{x \in \mathcal{M}} h(x)
\end{aligned}\tag{2}\label{2}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that $\mathcal{M}$ in general does not have a vector-space structure. 
We consider an intrinstic parameterization $\tau$ so that the parameter space $\Omega_\tau$ at least has a local  vector-space structure.
The problem in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; can be re-expressed as below.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau \in \Omega_\tau} h_\tau(\tau)
\end{aligned}
$$&lt;/code&gt; where $h_\tau$ is the parameter representation of scalar smooth function $h$.&lt;/p&gt;

&lt;p&gt;Natural gradient descent in this parameter space $\Omega_\tau$ is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{3}\label{3}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\tau_k} := [\mathbf{F}_\tau(\tau_k) ]^{-1} \nabla_\tau h_\tau(\tau_k)$&lt;/code&gt; and the step-size $\alpha$ is small enough so that  $\tau_{k+1} \in \Omega_\tau$.&lt;/p&gt;

&lt;p&gt;Consider another intrinstic parameterization $\lambda$ so that $\lambda=\mathbf{U} \tau$, where $\mathbf{U}$ is a constant (square) invertible matrix. 
When $\lambda$ is a valid parameterization, we know that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ \mathbf{U}\tau |\tau \in\Omega_\tau \} 	\cap \Omega_\lambda \neq \emptyset$&lt;/code&gt;.
For simplicity,  we further assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ \mathbf{U}\tau |\tau \in\Omega_\tau \} = \Omega_\lambda$&lt;/code&gt;, where $\Omega_\lambda$ is the parameter space of $\lambda$. In general, we could use a smaller parameter space either $\Omega_\lambda$ or $\Omega_\tau$ so that this additional assumption holds.&lt;/p&gt;

&lt;p&gt;Natural gradient descent in this parameter space $\Omega_\lambda$ is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} -  \alpha  \hat{\mathbf{g}}_{\lambda_k} 
\end{aligned}\tag{4}\label{4}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\lambda_k} := [\mathbf{F}_\lambda(\lambda_k) ]^{-1} \nabla_\lambda h_\lambda(\lambda_k)$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Recall that we have the &lt;a href=&quot;/posts/2021/11/Geomopt03/#transformation-rules-for-natural-gradients-and-euclidean-gradients&quot;&gt;transformation rule&lt;/a&gt; for natural gradients as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\hat{\mathbf{g}}_\tau= \mathbf{Q}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$&lt;/code&gt; where $Q_{ji}=\frac{\partial \tau^j(\lambda)}{\partial \lambda^i}$.&lt;/p&gt;

&lt;p&gt;We can verify that $\mathbf{Q} = \mathbf{U}^{-1}$. Notice that $\tau_0 = \mathbf{U}^{-1} \lambda_0$ by construction.
The update in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt; at iteration $k=1$ then can be re-expressed as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{1} \leftarrow \tau_{0} -  \alpha  \hat{\mathbf{g}}_{\tau_0} = \mathbf{U}^{-1} \lambda_0 -  \alpha  \mathbf{U}^{-1}  \hat{\mathbf{g}}_{\lambda_0} = \mathbf{U}^{-1} \lambda_1
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\alpha$&lt;/code&gt; is small enough, we have  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_1 \in \Omega_\tau$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_1 \in \Omega_\lambda$&lt;/code&gt;.
It is easy to show that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_k = \mathbf{U}^{-1} \lambda_k$&lt;/code&gt; by induction.
Therefore, updates in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; are equivalent.&lt;/p&gt;

&lt;h1 id=&quot;riemannian-gradient-descent-and-its-non-linear-invariance&quot;&gt;Riemannian Gradient Descent and its (Non-linear) Invariance&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Now we discuss a gradient-based method that is invariant to any intrinsic parameter transformation.
We will first introduce the concept of a (one-dimensional) geodesic $\gamma(t)$, which is the “shortest curve” on a manifold with a Riemannian metric (e.g., the Fisher-Rao metric).
Recall that in  &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt; we only define a distance between two Riemannian gradients evaluated at the same point. We can use the length of a geodesic to define the distance between two points on the manifold. 
In statistics, the distance induced by a geodesic with the Fisher-Rao metric is known as the Rao distance .&lt;/p&gt;

&lt;p&gt;Consider an intrinsic parametrization $\tau$, where $\gamma_\tau(t)$ is the parameter representation of the geodesic.
To specify a geodesic, we need to provide a starting point $\tau_0$ on the manifold and a Riemannian gradient $\mathbf{v}_{\tau_0}$ evluated at point $\tau_0$.
The geodeisc is the solution of a system of second-order non-linear ordinary differential equations (ODE) with the following initial conditions.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\gamma_\tau(0) = \tau_0; \,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$&lt;/code&gt; where the geodesic is determined by the initial conditions and the domain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau$&lt;/code&gt; of the geodesic contains 0 and 1.&lt;/p&gt;

&lt;p&gt;We will use the following map to define Riemannian gradient descent without specifying complicated  differential equations (e.g., Christoffel symbols) in the geodesic.
We define a manifold expoential map at point $\tau_0$ for a manifold  via the geodesic as 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{Exp}_{\tau_0}\colon  T\mathcal{M}_{\tau_0} &amp;amp; \mapsto \mathcal{M}\\
\mathbf{v}_{\tau_0} &amp;amp; \mapsto \gamma_\tau(1) \,\,\,\, \textrm{s.t.} \,\,\,\,\,\, \gamma_\tau(0) = \tau_0;\,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$&lt;/code&gt; Technically, we should require  manifold &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{M}$&lt;/code&gt; to be geodesically complete so that the domain of the expoential map is the whole tangent space. 
Equiavalently, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau $&lt;/code&gt; is the whole &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^1$&lt;/code&gt; space in such cases.&lt;/p&gt;

&lt;p&gt;Under intrinsic parametrization $\tau$, (exact) Riemannian gradient descent is defined as 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \mathrm{Exp}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} ) 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The invariance of this update is due to the uniqueness of ODE and transformation rules for natural-gradients, Fisher information matrix, and Christoffel symbols. We will not discuss this further in this post to avoid complicated derivations. 
Although Riemannian gradient descent is nice, the exponential map or the geodesic often does not have a closed form expression.&lt;/p&gt;

&lt;h1 id=&quot;many-faces-of-natural-gradient-descent&quot;&gt;Many faces of Natural-gradient Descent&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;natural-gradient-descent-as-inexact-riemannian-gradient-descent&quot;&gt;Natural-gradient Descent as Inexact Riemannian Gradient Descent&lt;/h2&gt;

&lt;p&gt;Natural-gradient descent can be viewed as a first-order (linear) approximation of the geodesic, which implies that natural-gradient descent is indeed an inexact Riemannian gradient update.
Natural-gradient descent is linearly invariant due to the approximation.&lt;/p&gt;

&lt;p&gt;Consider a first-order Taylor approximation at $t=0$ of the geodesic shown below.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\gamma_\tau(t) \approx  \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (t-0)  
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/span&gt;:
This approximation does not guarantee that the approximated geodesic stays on the manifold for all $t \neq 0$.&lt;/p&gt;

&lt;p&gt;Recall that the  expoential map  is defined via the geodesic  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma_\tau(1)$&lt;/code&gt;.
We can similarly define an approximated  expoential map (A.K.A. the Euclidean retraction map)  as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{Ret}_{\tau_0}(\mathbf{v}_{\tau_0}) := \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (1-0) =\tau_0 + \mathbf{v}_{\tau_0}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Therefore, the inexact Riemannian gradient update is defined as 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \mathrm{Ret}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} )  = \tau_k  - \alpha  \hat{\mathbf{g}}_{\tau_k}
\end{aligned}
$$&lt;/code&gt; which recovers natural-gradient descent.&lt;/p&gt;

&lt;h2 id=&quot;natural-gradient-descent-as-unconstrained-proximal-gradient-descent&quot;&gt;Natural-gradient Descent as Unconstrained Proximal-gradient Descent&lt;/h2&gt;

&lt;p&gt;In this section, we will make an additional but key assumption: the parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau=\mathcal{R}^K$&lt;/code&gt; has a global (unconstrained) vector space structure.
This assumption, in general, does not hold. However, we present this viewpoint since it closely relates to optimization
methods.&lt;/p&gt;

&lt;p&gt;As we mentioned before, the &lt;strong&gt;distances&lt;/strong&gt; in the gradient space and the parameter space are defined differently. 
In the &lt;a href=&quot;#riemannian-gradient-descent-and-its-non-linear-invariance&quot;&gt;previous section&lt;/a&gt;, we use the geodesic to define the distance between two points in a parameter space.&lt;/p&gt;

&lt;p&gt;We could also use other “distances” denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{D}(.,.)$&lt;/code&gt; (e.g., Kullback–Leibler divergence or f-divergence) to define the length between two points in a parameter space.
Given such a  “distance”, we can perform the unconstrained proximal-gradient descent as shown below.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \arg\min_{y \in  \mathcal{R}^K  } \{ \langle \mathbf{g}_{\tau_k}, y\rangle   + \frac{1}{\alpha} \mathrm{D}(y,\tau_k) \}
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_{\tau_k}$&lt;/code&gt; is a Eulcidean gradient and the parameter space is unconstrained.&lt;/p&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{D}(y,\tau_k)$&lt;/code&gt; is a secord-order Taylor approximation of  the KL divergence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{KL} [p(w|\tau_k) || p(w|y)]$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$y=\tau_k$&lt;/code&gt;, this  proximal-gradient method recovers natural-gradient descent.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;/posts/2021/12/Geomopt05/&quot;&gt;Part V&lt;/a&gt;, we will show that  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{D}(y,\tau_k)$&lt;/code&gt; can also be an exact KL divergence for  expoential family.
In such cases,  unconstrained mirror descent also recovers natural-gradient descent.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/span&gt;:
The connection bewteen natural-gradient descent and proximal-gradient/mirror descent breaks down when
the parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; is constrained and open. In constrained cases, these methods are distinct from each other. We will cover more about this point in &lt;a href=&quot;/posts/2021/12/Geomopt06/&quot;&gt;Part VI&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;natural-gradient-descent-in-non-intrinstic-parameter-spaces&quot;&gt;Natural-gradient Descent in Non-intrinstic Parameter Spaces&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;As mentioned in &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;Part I&lt;/a&gt;, an intrinsic parametrization creates a nice parameter space (e.g., a local vector space structure) and guarantees a non-singular FIM.
We now discuss issues when it comes to  natural-gradient descent over non-intrinsic parametrizations including overparameterization.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We may not have a local vector space structure in a non-intrinstic parameter space. Therefore, natural-gradient descent in this parameter space is pointless since the updated parameter will leave the parameter space. Indeed, the FIM could also be ill-defined in such cases. We will illustrate this by examples.
    &lt;details&gt;
 &lt;summary&gt;Bernoulli Example: Invalid NGD&lt;/summary&gt;
 &lt;fieldset class=&quot;field-set&quot;&gt;
        &lt;blockquote&gt;

          &lt;p&gt;Consider Bernoulli family  $ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0, \pi_0+\pi_1=1  \}$ with parameter $\tau = (\pi_0,\pi_1)$.&lt;/p&gt;

          &lt;p&gt;As we shown in &lt;a href=&quot;/posts/2021/09/Geomopt01/#caveats-of-the-fisher-matrix-computation&quot;&gt;Part I&lt;/a&gt;, the FIM is ill-defined due to this eqaulity constraint.&lt;/p&gt;

          &lt;p&gt;Moreover, the NGD update will violate the eqaulity constraint.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/fieldset&gt;
 &lt;/details&gt;

    &lt;details&gt;
 &lt;summary&gt;Von Mises–Fisher Example: Invalid NGD&lt;/summary&gt;
 &lt;fieldset class=&quot;field-set&quot;&gt;
        &lt;blockquote&gt;

          &lt;p&gt;Consider $2$-dimensional Von Mises–Fisher family  $ \{p(\mathbf{w}|\tau):= C(\kappa) \exp(\kappa (w_1\mu_1+w_2\mu_2) ) \Big| \kappa&amp;gt;0, \mu_1^2+\mu_2^2 =1  \}$ with parameter $\tau = (\kappa,\mu_1,\mu_2)$, where $ C(\kappa)$ is the normalization constant, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\kappa$&lt;/code&gt; is a positive scalar, $\mathbf{w}=(w_1,w_2)$ is a random unit vector defined in a circle,
 and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\mu}=(\mu_1,\mu_2)$&lt;/code&gt; is also a unit vector.&lt;/p&gt;

          &lt;p&gt;We can show that the FIM is ill-defined under this parametrization due to this eqaulity constraint.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/fieldset&gt;
 &lt;/details&gt;
  &lt;/li&gt;
  &lt;li&gt;The FIM is singular in a non-intrinstic space. In theory, Moore–Penrose inverse could be used to compute natural-gradients so that natural-gradient descent is linearly invariant in this case. However, Moore–Penrose inverse often has to use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Singular_value_decomposition&quot;&gt;singular value decomposition&lt;/a&gt; (SVD) and  destroies structures of the FIM.  In practice, the iteration cost of Moore–Penrose inverse is very high as illustrated in the following example.
    &lt;details&gt;
 &lt;summary&gt;Example: High iteation cost&lt;/summary&gt;
 &lt;fieldset class=&quot;field-set&quot;&gt;
        &lt;blockquote&gt;

          &lt;p&gt;Consider a $d$-dimensional Gaussian mixture family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \frac{1}{C} \sum_{k=1}^{C} \mathcal{N}(w |\mathbf{\mu}_k,  \mathbf{\Sigma}_k)  \Big|  \mathbf{\mu}_k \in \mathcal{R}^d,  \mathbf{\Sigma}_k  \succ \mathbf{0}  \}$&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \{ \mu_k,\Sigma_k\}_{k=1}^{C} $&lt;/code&gt;. This is a non-intrinstic parameterization.&lt;/p&gt;

          &lt;p&gt;If we  use the following initialization such that all $K$ components have the same mean $\mu_0$ and the same covariance $\Sigma_0$, this family becomes a Gaussian family. In this case, the FIM of this mixture is singular.
The iteration cost of natural-gradient descent in this parameter space $\Omega_\tau$ will be $O(C^3 d^6)$ if  Moore–Penrose inverse is employed.&lt;/p&gt;

          &lt;p&gt;Now, consider the equivalent Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mathbf{\mu}_0,  \mathbf{\Sigma}_0)  \Big|  \mathbf{\mu}_0 \in \mathcal{R}^d,  \mathbf{\Sigma}_0  \succ \mathbf{0}  \}$&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda =( \mu_0,\Sigma_0 ) $&lt;/code&gt;, where $\lambda$ is an intrinsic parameterization of the Gaussian family.&lt;/p&gt;

          &lt;p&gt;As we will show in &lt;a href=&quot;/posts/2021/12/Geomopt05/&quot;&gt;Part V&lt;/a&gt;, the iteration cost of natural-gradient descent in this parameter space $\Omega_\lambda$ will be $O(d^3)$ if we exploit structures of the exact non-singular FIM.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/fieldset&gt;
 &lt;/details&gt;
  &lt;/li&gt;
  &lt;li&gt;It is tempting to approximate the singular FIM by an emprical FIM with a scalar damping term and use Woodbury matrix identity to reduce the iteration cost of computing natural-gradients. However, sample-based emprical approximations could be problematic.
Moreover, damping introduces an additional tuning hyper-parameter and destories the linear invariance property of  natural-gradient descent.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;euclidean-gradient-descent-is-not-linearly-invariant&quot;&gt;Euclidean Gradient Descent is NOT (Linearly) Invariant&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;For simplicity, consider an unconstrained optimization problem.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau \in \mathcal{R}^K } h_\tau(\tau)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Euclidean gradient descent (GD) in parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha {\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{5}\label{5}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\mathbf{g}}_{\tau_k} := \nabla_\tau h_\tau(\tau_k)$&lt;/code&gt; is a Euclidean gradient.&lt;/p&gt;

&lt;p&gt;Consider a reparametrization  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; so that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda=\mathbf{U} \tau$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{U}$&lt;/code&gt; is a constant (square) invertible matrix. 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\lambda \in \mathcal{R}^K } h_\lambda(\lambda):= h_\tau( \mathbf{U}^{-1} \lambda)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The Euclidean gradient descent (GD) in parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} - \alpha {\mathbf{g}}_{\lambda_k} 
\end{aligned}\tag{6}\label{6}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\mathbf{g}}_{\lambda_k} := \nabla_\lambda h_\lambda(\lambda_k)$&lt;/code&gt; is a Euclidean gradient.&lt;/p&gt;

&lt;p&gt;Note that Euclidean gradients follow the &lt;a href=&quot;/posts/2021/11/Geomopt03/#transformation-rules-for-natural-gradients-and-euclidean-gradients&quot;&gt;transformation rule&lt;/a&gt;  as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\tau^T =  \mathbf{g}_\lambda^T \mathbf{J}
\end{aligned}
$$&lt;/code&gt; where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$J_{ki}:=\frac{\partial \lambda^k(\tau) }{ \partial \tau^i }$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We can verify that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{J}=\mathbf{U}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\tau = \mathbf{U}^T \mathbf{g}_\lambda $&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Notice that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0 = \mathbf{U}^{-1} \lambda_0$&lt;/code&gt; by construction.
The update in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt; at iteration $k=1$ then can be re-expressed as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{1} \leftarrow \tau_{0} -  \alpha  {\mathbf{g}}_{\tau_0} = \mathbf{U}^{-1} \lambda_0 -  \alpha  \mathbf{U}^{T}  {\mathbf{g}}_{\lambda_0} \neq \mathbf{U}^{-1} \lambda_1
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It is easy to see that
updates in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt; are NOT equivalent.
Therefore,  Euclidean gradient descent is not invariant.&lt;/p&gt;

&lt;h1 id=&quot;newtons-method-is-linearly-invariant&quot;&gt;Newton’s Method is Linearly Invariant&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;For simplicity, consider an unconstrained convex optimization problem.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau \in \mathcal{R}^K } h_\tau(\tau)
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\tau(\tau)$&lt;/code&gt; is strongly convex and twice continuously differentiable w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Newton’s method in parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \mathbf{H}^{-1}_\tau(\tau_k) {\mathbf{g}}_{\tau_k} 
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\mathbf{g}}_{\tau_k} := \nabla_\tau h_\tau(\tau_k)$&lt;/code&gt; is a Euclidean gradient and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{H}_\tau(\tau_k):=\nabla_\tau^2 h_\tau(\tau_k)$&lt;/code&gt; is the Hessian.&lt;/p&gt;

&lt;p&gt;Consider a reparametrization  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; so that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda=\mathbf{U} \tau$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{U}$&lt;/code&gt; is a constant (square) invertible matrix. 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\lambda \in \mathcal{R}^K } h_\lambda(\lambda):= h_\tau( \mathbf{U}^{-1} \lambda)
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\lambda(\lambda)$&lt;/code&gt; is also stronly convex w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; due to  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Newton’s method  in parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} - \alpha \mathbf{H}^{-1}_\lambda(\lambda_k) {\mathbf{g}}_{\lambda_k} 
\end{aligned} 
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\mathbf{g}}_{\lambda_k} := \nabla_\lambda h_\lambda(\lambda_k)$&lt;/code&gt; is a Euclidean gradient and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{H}_\tau(\lambda_k):=\nabla_\lambda^2 h_\lambda(\lambda_k)$&lt;/code&gt; is the Hessian.&lt;/p&gt;

&lt;p&gt;As we discussed in the previous section, 
Euclidean gradients follow the &lt;a href=&quot;/posts/2021/11/Geomopt03/#transformation-rules-for-natural-gradients-and-euclidean-gradients&quot;&gt;transformation rule&lt;/a&gt;  as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\tau^T =  \mathbf{g}_\lambda^T \mathbf{J}$&lt;/code&gt;, where
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{J}=\mathbf{U}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Surprisingly, for a linear transformation, the Hessian follows the &lt;a href=&quot;/posts/2021/11/Geomopt03/#transformation-rule-for-the-fisher-information-matrix&quot;&gt;transformation rule&lt;/a&gt;  like the Fisher information
matrix as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{H}_{\tau} (\tau_k) &amp;amp;= \nabla_\tau ( \mathbf{g}_{\tau_k} ) \\
&amp;amp;=\nabla_\tau ( \mathbf{J}^T \mathbf{g}_{\lambda_k} ) \\
&amp;amp;=\mathbf{J}^T\nabla_\tau (  \mathbf{g}_{\lambda_k} ) \,\,\,\,\text{(for a linear transformation, } \mathbf{J} = \mathbf{U} \text{ is a
constant matrix)}   \\
&amp;amp;=\mathbf{J}^T\nabla_\lambda (  \mathbf{g}_{\lambda_k} )\mathbf{J} \\ 
&amp;amp;=\mathbf{J}^T \mathbf{H}_{\lambda} (\lambda_k)\mathbf{J} 
\end{aligned}\tag{7}\label{7}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Therefore, the direction in Newton’s method denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tilde{\mathbf{g}}_{\tau_k} := \mathbf{H}^{-1}_\tau(\tau_k) \mathbf{g}_{\tau_k}$&lt;/code&gt; is transformed like natural-gradients in &lt;strong&gt;linear&lt;/strong&gt; cases as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tilde{\mathbf{g}}_{\tau_k} &amp;amp;:= \mathbf{H}^{-1}_\tau(\tau_k) \mathbf{g}_{\tau_k} \\
&amp;amp;= [ \mathbf{J}^T \mathbf{H}_{\lambda} (\lambda_k)\mathbf{J} ]^{-1} \mathbf{g}_{\tau_k} \\
&amp;amp;=  \mathbf{J}^{-1} \mathbf{H}^{-1}_{\lambda} (\lambda_k)\mathbf{J}^{-T} [ \mathbf{J}^{T}\mathbf{g}_{\lambda_k} ] \\
&amp;amp;=  \mathbf{J}^{-1} \mathbf{H}^{-1}_{\lambda} (\lambda_k) \mathbf{g}_{\lambda_k}  \\
&amp;amp;=  \mathbf{J}^{-1}  \tilde{\mathbf{g}}_{\lambda_k}  \\
&amp;amp;=  \mathbf{Q}  \tilde{\mathbf{g}}_{\lambda_k}  \\
\end{aligned} 
$$&lt;/code&gt; where by the definition we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{Q}= \mathbf{J}^{-1}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The consequence is that Newton’s method like natural-gradient descent is linearly invariant.&lt;/p&gt;

&lt;h2 id=&quot;the-hessian-is-not-a-valid-manifold-metric&quot;&gt;The Hessian is not a valid manifold metric&lt;/h2&gt;

&lt;p&gt;The Hessian &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{H}_\tau(\tau_k)=\nabla_\tau^2 h_\tau(\tau_k)$&lt;/code&gt;  in general is not a valid manifold metric since it does not follow the transformation
rule of a metric in non-linear cases.&lt;/p&gt;

&lt;p&gt;Contrastingly, the Fisher information matrix (FIM) is a valid manifold metric.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Claim: the FIM $\mathbf{F}_\tau(\tau) = E_{p(w|\tau)}\big[ -\nabla_\tau^2 \log p(w|\tau) \big]$
follows the transformation
rule  even in non-linear cases.
&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Proof&lt;/p&gt;

      &lt;p&gt;Given a non-linear intrinstic reparametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt;, recall that the Jacobian matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{J}(\tau_k)$&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; is no longer a constant matrix but a square and non-singular matrix.
In this case, the FIM still follows the &lt;a href=&quot;/posts/2021/11/Geomopt03/#transformation-rule-for-the-fisher-information-matrix&quot;&gt;transformation rule&lt;/a&gt; thanks to &lt;a href=&quot;https://en.wikipedia.org/wiki/Score_(statistics)#Mean&quot;&gt;the expectation of the score function&lt;/a&gt;.&lt;/p&gt;

      &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\require{cancel}
\begin{aligned}
&amp;amp; \mathbf{F}_\tau(\tau_k)\\
=&amp;amp; E_{p(w|\tau_k)}\big[ -\nabla_\tau^2 \log p(w|\tau_k) \big]  \\
=&amp;amp; E_{p(w|\lambda_k)}\big[ -\nabla_\tau^2 \log p(w|\tau_k) \big]  \\
=&amp;amp; E_{p(w|\lambda_k)}\big[ -\nabla_\tau [ \mathbf{J}^T(\tau_k) \nabla_\lambda \log p(w|\lambda_k)]  \big]  \\
=&amp;amp; -    \mathbf{J}^T(\tau_k)   E_{p(w|\lambda_k)}\big[  \nabla_\tau  \nabla_\lambda \log p(w|\lambda_k) \big]   - [\nabla_\tau \mathbf{J}^T(\tau_k)  ]  \underbrace{  \cancelto{=0}{E_{p(w|\lambda_k)}\big[  \nabla_\lambda \log p(w|\lambda_k) \big]}  }_{ \text{ (the expectation of the score is zero)}  }   \\
=&amp;amp; -    \mathbf{J}^T(\tau_k)   E_{p(w|\lambda_k)}\big[  [\nabla_\lambda^2 \log p(w|\lambda_k) ] \mathbf{J }(\tau_k) \big] \\
=&amp;amp; -    \mathbf{J}^T(\tau_k)    E_{p(w|\lambda_k)}\big[  \nabla_\lambda^2 \log p(w|\lambda_k) \big] \mathbf{J}(\tau_k) \\
=&amp;amp;\mathbf{J }^T(\tau_k)  \mathbf{F}_\lambda(\lambda_k) \mathbf{J}(\tau_k)  
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;p&gt;We will discuss in
&lt;a href=&quot;/posts/2021/12/Geomopt05/#minimal-parametrizations-of-exponential-family&quot;&gt;Part V&lt;/a&gt;, for a special parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; (known as a natural parametrization) of &lt;a href=&quot;/posts/2021/12/Geomopt05/#exponential-family&quot;&gt;exponential family&lt;/a&gt;, the FIM under this parametrization can be computed  as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_\tau(\tau) = \nabla_\tau^2 A_\tau(\tau)$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\tau(\tau)$&lt;/code&gt; is a strictly convex function.&lt;/p&gt;

&lt;p&gt;In the literature, the expoential family with a natural parametrization is known as a Hessian manifold, where the FIM
under this kind of parametrization is called a Hessian metric.
However, a non-linear reparametrization will lead to a non-natural parametrization.&lt;/p&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><summary type="html">Warning: working in Progress (incomplete)</summary></entry><entry><title type="html">Part III: Invariance of Natural-Gradients</title><link href="/posts/2021/11/Geomopt03/" rel="alternate" type="text/html" title="Part III: Invariance of Natural-Gradients" /><published>2021-11-02T00:00:00-07:00</published><updated>2021-11-02T00:00:00-07:00</updated><id>/posts/2021/11/Geomopt03</id><content type="html" xml:base="/posts/2021/11/Geomopt03/">&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post should help readers to understand the invariance of natural-gradients.
We will also discuss why the Euclidean steepest direction is NOT invariant.&lt;/p&gt;

&lt;p&gt;We will give an informal introduction with a focus on high level of ideas.&lt;/p&gt;

&lt;h1 id=&quot;parameter-transformation-and-invariance&quot;&gt;Parameter Transformation and Invariance&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;In &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt;, we have shown that a Riemannian gradient is a parametric representation of the tangent direction of a curve in a  manifold.
Since a curve and its tangent direction are geometric obejects, they should be invariant to the choice of parametrization.
In other words, geometric properties should be persevered in any valid coordinate system. This is a coordinate-free argument.&lt;/p&gt;

&lt;p&gt;The argument could be abstract for beginners. To be more concrete, we consider the corresponding coordinate-dependent argument: geometric properties should remain unchanged if we perform a (valid) parameter transformation. This coordinate-dependent argument also gives us a rule to compute natrual-gradients under a parameter transformation.&lt;/p&gt;

&lt;p&gt;The transformation rules are summarized in the following table&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Type of  directions&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;transformation rules&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Euclidean gradient&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Euclidean steepest descent direction (normalized Euclidean gradient)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;N/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Riemannian gradient&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Riemannian steepest descent direction (normalized Riemannian gradient)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We will show that two key &lt;strong&gt;geometric properties&lt;/strong&gt; remains the same under any &lt;strong&gt;intrinsic&lt;/strong&gt; parameter transformation.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Directional derivative&lt;/li&gt;
  &lt;li&gt;Length of a Riemannian vector/gradient induced by the Fisher-Rao metric&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thanks to these properties, we will show that the optimal solution of the &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-steepest-direction&quot;&gt;Riemannian steepest direction&lt;/a&gt; considered in Part II is equivalent under an intrinsic parameter transformation. This is in contrast with the Euclidean steepest direction which is not invaraint under an intrinsic parameter transformation.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt;, we consider a point $\mathbf{x}_0$ in a manifold $\mathcal{M}$, a (1-dimensional) curve $\gamma(t)$, and a smooth scalar function $h: \mathcal{M} \to \mathcal{R}$.
Given an intrinsic parametrization $\tau$ containing the point, we consider the following parametric representations.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;geometric object&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;parametric representation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;curve  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma_\tau(t) $&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;function  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h(x_0)$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\tau(\tau_0) $&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;transformation-rules-for-natural-gradients-and-euclidean-gradients&quot;&gt;Transformation Rules for Natural Gradients and Euclidean Gradients&lt;/h2&gt;

&lt;p&gt;Intuitively, the following identity should hold for any two (intrinsic) parametrizations $\tau$ and $\lambda$.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
h(\gamma(t)) = h_\tau(\gamma_\tau(t)) = h_\lambda(\gamma_\lambda(t))
\end{aligned}
$$&lt;/code&gt; where we consider $t$ to be fixed.&lt;/p&gt;

&lt;p&gt;Technically speaking,  domain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau$&lt;/code&gt; of curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma_\tau(t)$&lt;/code&gt; and  domain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\lambda$&lt;/code&gt; of curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma_\lambda(t)$&lt;/code&gt; may be different. However, both domains are open intervals containing 0 since both &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma_\tau(0)=\tau_0$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma_\lambda(0)=\lambda_0$&lt;/code&gt;  are parametric representations of the same point $\mathbf{x}_0$.&lt;/p&gt;

&lt;p&gt;From the above expression, we can see that directional derivatives should be the same at $t=0$
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} = \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} 
\end{aligned}\tag{1}\label{1}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt;, we have shown that 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &amp;amp;=  [\nabla h_\tau(\mathbf{\tau}_0)  ]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}   \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} &amp;amp; = [\nabla h_\lambda(\mathbf{\lambda}_0) ]^T  \frac{d {\gamma}_\lambda(t) }{d t} \Big|_{t=0}  
\end{aligned}
$$&lt;/code&gt; where $\nabla$ is the standard (coordinate) derivative.&lt;/p&gt;

&lt;p&gt;Recall that in &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt;,  we have shown that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$&lt;/code&gt; is a  parametric representation of a Riemannian vector, which is a Riemannian gradient.
Notice that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla h_\lambda(\mathbf{\lambda}_0)$&lt;/code&gt; is a Euclidean gradient.&lt;/p&gt;

&lt;p&gt;We will use the following notations to simplify expressions.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Notation&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Meanings&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Euclidean gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(g_\tau)_i$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$i$-th entry  under parametrization $\tau$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Riemannian gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(\hat{g}_\tau)^j$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$j$-th entry under parametrization $\tau$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau^j$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$j$-th parameter under parametrization   $\tau$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Using these notations, the derivational derivatives then can be re-expressed as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &amp;amp;= \sum_{i}  (g_\tau)_i  (\hat{g}_\tau)^i  = \mathbf{g}_\tau^T \hat{\mathbf{g}}_\tau \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} &amp;amp; =\sum_{i}  (g_\lambda)_i (\hat{g}_\lambda)^i =  {\mathbf{g}}_\lambda^T \hat{\mathbf{g}}_\lambda
\end{aligned}\tag{2}\label{2}
$$&lt;/code&gt;
where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\lambda$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\tau$&lt;/code&gt; are Euclidean gradients (e.g.,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\tau=\nabla h_\tau(\tau_0) $&lt;/code&gt;)  while  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\lambda$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\tau$&lt;/code&gt;  are  Riemannian gradients (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\tau=\mathbf{v}_\tau(\tau_0) =\frac{d \gamma_\tau(0) }{d t}$&lt;/code&gt;) .&lt;/p&gt;

&lt;p&gt;By &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt;, we have the following identity obtained from the &lt;strong&gt;geometric property&lt;/strong&gt; of directional derivatives.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \mathbf{g}_\tau^T \hat{\mathbf{g}}_\tau  =  \mathbf{g}_\lambda^T \hat{\mathbf{g}}_\lambda 
\end{aligned}\tag{3}\label{3}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now, we discuss the parameter transformation between $\tau$ and $\lambda$, where we can express $\lambda$ in terms of $\tau$ denoted by $\lambda(\tau)$.&lt;/p&gt;

&lt;p&gt;By the (standard) chain rule for a Euclidean gradient, we has
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
(g_\tau)_i = \sum_{k}  \color{red}{ (g_\lambda)_k} \frac{\color{red}{ \partial \lambda^k(\tau) }}{ \partial \tau^i } 
\end{aligned}
\tag{4}\label{4}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$J_{ki}:=\frac{\partial \lambda^k(\tau) }{ \partial \tau^i }$&lt;/code&gt; denotes the $(k,i)$ entry of the Jacobian matrix. We illustrate our matrix notation in a 2D case as below.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\begin{matrix}
&amp;amp; \\
\mathbf{J} = 
    \left ( \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    i=1 &amp;amp; i=2 \\ \hline
    J_{11} &amp;amp; J_{12}  \\
   J_{21} &amp;amp; J_{22} \\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
&amp;amp; \\
\left . \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right )
    \begin{matrix}
    k=1 \\ k=2 
    \end{matrix}
\end{matrix}
\end{aligned}\tag{5}\label{5}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; gives us the transformation rule for Eulcidean gradients (denoted by a row vector)  as below in a vector form.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\tau^T =  \mathbf{g}_\lambda^T \mathbf{J}
\end{aligned}\tag{6}\label{6},
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note: &lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;row&lt;/strong&gt;&lt;/span&gt; vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\mathbf{g}}_\tau^T$&lt;/code&gt; can be computed via a vector-Jacobian product in any standard Auto-Diff toolbox given that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\mathbf{g}}_\lambda$&lt;/code&gt; is pre-computed.&lt;/p&gt;

&lt;p&gt;By Eq &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt;, we obtain the transformation rule for Riemannian gradients as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \hat{\mathbf{g}}_\tau= \mathbf{J}^{-1}  \hat{\mathbf{g}}_\lambda 
\end{aligned}\tag{7}\label{7}
$$&lt;/code&gt; where $\mathbf{Q}:=\mathbf{J}^{-1}$ is also a Jacobian matrix and $Q_{ki}=\frac{\partial \tau^k(\lambda)}{\partial \lambda^i}$ is the $(k,i)$ entry of matrix $\mathbf{Q}$.&lt;/p&gt;

&lt;p&gt;Note: &lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;column&lt;/strong&gt;&lt;/span&gt; vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\tau$&lt;/code&gt; can be computed via a Jacobian-vector product used in forward-mode differentiation given that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\lambda$&lt;/code&gt; is pre-computed.&lt;/p&gt;

&lt;p&gt;The elementwise expression of the transformation rule for Riemannian gradients (denoted by a column vector) is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
(g_\tau)^k
 = \sum_{i}  \frac{ \partial \tau^k(\lambda)  }{\color{red} {\partial  \lambda^i} }  \color{red} {(g_\lambda)^i}
\end{aligned},
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that these transformation rules are valid  when the Jacobian matrix is square and non-singular.
As we discussed in Part I about &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;intrinsic parameterizations&lt;/a&gt;, the transformation map between $\tau$ and $\lambda$ must be bi-jective, which implies the Jacoabian matrix is square. 
Moreover, the map and its inverse map should be smooth, which implies that the Jacobian matrix is well-defined and non-singular.&lt;/p&gt;

&lt;h2 id=&quot;transformation-rule-for-the-fisher-information-matrix&quot;&gt;Transformation Rule for the Fisher Information Matrix&lt;/h2&gt;

&lt;p&gt;Now, we give a transformation rule for the Fisher information matrix as defined at &lt;a href=&quot;/posts/2021/09/Geomopt01/#fisher-rao-metric&quot;&gt;Part I&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log p(w|\tau) \Big) ]
\end{aligned}
$$&lt;/code&gt;
Since $ \log p(w|\tau )$ can be considered as a scalar function $h$ defined on the manifold for any valid $w$, we have 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \log p(w|\tau_0  ) = h_\tau(\tau_0) = h_\lambda(\lambda_0) =  \log p(w|\lambda_0  )
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Thus, the Fisher metric can be computed as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 F_{ij}(\tau_0) &amp;amp;= E_{p(w|\tau_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log p(w|\tau_0) \Big) ] \\
&amp;amp;=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log p(w|\tau_0) \Big) ]\\
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Recall that by the standard chain rule, we have 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\tau_i} \log p(w|\tau_0 ) = \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \Big( \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Moreover, the Jacobian matrix does not depent on $w$. Therefore, we have
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 F_{ij}(\tau_0) 
&amp;amp;=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log p (w|\tau_0) \Big) ]\\
&amp;amp;=  E_{p(w|\lambda_0) }  [ \Big( \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \sum_l \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&amp;amp;= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  E_{p(w|\lambda_0) }  [ \Big(  \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&amp;amp;= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } F_{kl}(\lambda_0)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We can re-express the above expression in a matrix form as below. This is the transformation rule for the Fisher information matrix.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}_{\tau} (\tau_0) = \underbrace{\mathbf{J}^T}_{  \frac{ \partial \lambda^i(\tau_0) }{ \partial \tau^k }  } \mathbf{F}_{\lambda} (\lambda_0) \underbrace{\mathbf{J}}_{  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  } 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;By using this transformation rule, we can show that another &lt;strong&gt;geometric property&lt;/strong&gt;: the length of a Riemannian vector is preserved.&lt;/p&gt;

&lt;p&gt;We can see that the length of a Riemannian vector is also invariant.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\| \hat{\mathbf{g}}_\tau \|^2_{F_{\tau_0}} &amp;amp;= [\hat{\mathbf{g}}_\tau]^T \mathbf{F}_{\tau} (\tau_0) \hat{\mathbf{g}}_\tau \\
&amp;amp;= [\mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda]^T \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda \\
&amp;amp;= [\hat{\mathbf{g}}_\lambda]^T [ \mathbf{J}^{-T}  \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} ] \hat{\mathbf{g}}_\lambda \\
&amp;amp;= [\hat{\mathbf{g}}_\lambda]^T  \mathbf{F}_{\lambda} (\lambda_0)  \hat{\mathbf{g}}_\lambda = \| \hat{\mathbf{g}}_\lambda \|^2_{F_{\lambda_0}}
\end{aligned}\tag{8}\label{8}
$$&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&quot;riemannian-steepest-direction-is-invariant&quot;&gt;Riemannian Steepest Direction is Invariant&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Now, we can show that the optimal solution of &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-steepest-direction&quot;&gt;Riemannian steepest direction&lt;/a&gt; considered in Part II  under parametrization $\tau$ and $\lambda$ are equivalent since both the length and the directional derivative remain the same.&lt;/p&gt;

&lt;p&gt;Denote Euclidean gradients as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\lambda:= \nabla  f_\lambda(\mathbf{\lambda}_0) $&lt;/code&gt; and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\tau:= \nabla  f_\tau(\mathbf{\tau}_0)  = \nabla  f_\lambda(\mathbf{\lambda}(\tau_0)) $&lt;/code&gt;, which follows the parameter transformation rule in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now, consider Riemannian gradients as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\lambda:= \mathbf{F}_{\lambda}^{-1}(\mathbf{\lambda}_0)  \mathbf{g}_\lambda $&lt;/code&gt; and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\tau:= \mathbf{F}_{\tau}^{-1}(\mathbf{\tau}_0) \mathbf{g}_\tau $&lt;/code&gt;. We can verify that these Riemannian gradients follow the parameter transformation rule in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Recall that the optimal solution of the Riemannian steepest direction is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{v}_{\lambda}^{(opt)}= -\frac{ \mathbf{F_\lambda}^{-1}(\mathbf{\lambda}_0) \nabla_\lambda f(\mathbf{\lambda}_0) }{\| \mathbf{F_\lambda}^{-1}(\mathbf{\lambda}_0)\nabla_\lambda f(\mathbf{\lambda}_0) \|_{F_{\lambda_0}}} = -\frac{\hat{\mathbf{g}}_\lambda}{\|\hat{\mathbf{g}}_\lambda\|_{ F_{\lambda_0} } } \\
\mathbf{v}_{\tau}^{(opt)}= -\frac{ \mathbf{F_\tau}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0) }{\| \mathbf{F_\tau}^{-1}(\mathbf{\tau}_0)\nabla_\tau f(\mathbf{\tau}_0) \|_{F_{\tau_0}}} = -\frac{\hat{\mathbf{g}}_\tau}{\|\hat{\mathbf{g}}_\tau\|_{ F_{\tau_0} } } 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We can easily to verify the following identities since
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \|\hat{\mathbf{g}}_\lambda\|_{ F_{\lambda_0} } =  \|\hat{\mathbf{g}}_\tau\|_{ F_{\tau_0} }  $&lt;/code&gt;  in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{8}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\tau^T   \mathbf{v}^{(opt)}_{\tau}   &amp;amp; =   \mathbf{g}_\lambda^T  \mathbf{v}^{(opt)}_{\lambda} &amp;amp; \,\,\, \text{(invariance of a directional derivative)}  \\
\|  \mathbf{v}^{(opt)}_{\tau} \|^2_{\color{red}{F_{\tau_0}}} &amp;amp; = \|  \mathbf{v}^{(opt)}_{\lambda} \|^2_{\color{red}{F_{\lambda_0}} } &amp;amp; \,\,\, \text{(invariance of the length of the Riemannian steepest direction)}  \\
\mathbf{v}^{(opt)}_{\tau} &amp;amp; = \mathbf{J}^{-1}  \mathbf{v}^{(opt)}_{\lambda} &amp;amp; \,\,\, \text{(transformation rule for the Riemannian steepest direction)}  
\end{aligned}.
$$&lt;/code&gt; 
In other words, the Riemannian steepest direction (which is indeed  a normalized Riemannian gradient) is also transformed according to the transformation rule for a Riemannian gradient in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As we will discuss in &lt;a href=&quot;/posts/2021/11/Geomopt04/#natural-gradient-descent-is-linearly-invariant&quot;&gt;Part IV&lt;/a&gt;, this invariance property implies that natural-gradient descent is linearly invariant.&lt;/p&gt;

&lt;h1 id=&quot;euclidean-steepest-direction-is-not-invariant&quot;&gt;Euclidean Steepest Direction is NOT Invariant&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Recall that we have shown that a Euclidean gradient is the optimal solution of  &lt;a href=&quot;/posts/2021/10/Geomopt02/#euclidean-steepest-direction-and directional-derivative&quot;&gt;Euclidean steepest direction&lt;/a&gt; in Part II.&lt;/p&gt;

&lt;p&gt;We can show that the (standard) length of a Euclidean gradient is NOT invariant under a parameter transformation due to the Jacobian matrix (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\| \mathbf{g}_\tau \| \neq \| \mathbf{g}_\lambda \|$&lt;/code&gt;). This is a reason why we use the &lt;a href=&quot;/posts/2021/10/Geomopt02/#distance-induced-by-the-fisher-rao-metric&quot;&gt;weighted inner product&lt;/a&gt; to define the length of a gradient vector.&lt;/p&gt;

&lt;p&gt;Note that we denote the Euclidean gradients as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\lambda:= \nabla  f_\lambda(\mathbf{\lambda}_0) $&lt;/code&gt; and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\tau:= \nabla  f_\tau(\mathbf{\tau}_0)  = \nabla  f_\lambda(\mathbf{\lambda}(\tau_0)) $&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Recall that the optimal solution of the &lt;a href=&quot;/posts/2021/10/Geomopt02/#euclidean-steepest-direction-and-directional-derivative&quot;&gt;Euclidean steepest direction&lt;/a&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{v}_{\lambda}^{(opt)}= -\frac{\nabla_\lambda f_\lambda(\mathbf{\lambda}_0) }{\|\nabla_\lambda f_\lambda(\mathbf{\lambda}_0)  \|} = -\frac{\mathbf{g}_\lambda}{\|\mathbf{g}_\lambda\|} \\
\mathbf{v}_{\tau}^{(opt)}= -\frac{\nabla_\tau f_\tau(\mathbf{\tau}_0) }{\|\nabla_\tau f_{\tau}(\mathbf{\tau}_0) \|} = -\frac{\mathbf{g}_\tau}{\|\mathbf{g}_\tau\|} 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, the Euclidean steepest direction  does NOT obey the parameter transformation rule for Euclidean gradients in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
(\mathbf{v}_{\tau}^{(opt)})^T \neq (\mathbf{v}_{\lambda}^{(opt)})^T \mathbf{J} 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Moreover, the optimal value of the Euclidean steepest direction is NOT invariant under a parameter transformation as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\lambda^T \mathbf{v}^{(opt)}_{\lambda}  = - \|\mathbf{g}_\lambda\| \neq - \|\mathbf{g}_\tau\| = \mathbf{g}_\tau^T \mathbf{v}^{(opt)}_{\tau} 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In summary, the Euclidean steepest direction (which is a normalized Euclidean gradient) is NOT transformed according to the transformation rule for a Euclidean gradient.
This also implies that Euclidean gradient descent is not invariant under a parameter transformation. We will cover more about this in &lt;a href=&quot;/posts/2021/11/Geomopt04/&quot;&gt;Part IV&lt;/a&gt;&lt;/p&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><summary type="html">Goal This blog post should help readers to understand the invariance of natural-gradients. We will also discuss why the Euclidean steepest direction is NOT invariant.</summary></entry><entry><title type="html">Part II: Natural-Gradients Evaluted at one Point</title><link href="/posts/2021/10/Geomopt02/" rel="alternate" type="text/html" title="Part II: Natural-Gradients Evaluted at one Point" /><published>2021-10-04T00:00:00-07:00</published><updated>2021-10-04T00:00:00-07:00</updated><id>/posts/2021/10/Geomopt02</id><content type="html" xml:base="/posts/2021/10/Geomopt02/">&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post should help readers to understand natural-gradients, which are known as Riemannian gradients with the Fisher-Rao metric.
The main propose of this post is to show how to compute/define a natural-gradient.
The space of natural-gradients evaluated at the same point is called a tangent space at that point.&lt;/p&gt;

&lt;p&gt;We will give an informal introduction with a focus on high level of ideas.&lt;/p&gt;

&lt;h1 id=&quot;euclidean-steepest-direction-and-directional-derivative&quot;&gt;Euclidean steepest direction and directional derivative&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Before we discuss natural-gradients, we first revisit Euclidean gradients.&lt;/p&gt;

&lt;p&gt;We will show a Euclidean gradient can be viewed as the Euclidean steepest direction. Later, we extend the steepest direction in Riemannian cases and show that the Riemannian steepest direction w.r.t. the Fisher-Rao metric is indeed a natural-gradient.&lt;/p&gt;

&lt;p&gt;Given a smooth scalar function $\min_{\tau \in \mathcal{R}^K } \,\,f(\mathbf{\tau})$ in a &lt;strong&gt;vector space&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, we can define the (Euclidean) steepest direction at current &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; as the optimal solution to the following optimization problem. We can express the optimization problem in terms of a &lt;strong&gt;directional derivative&lt;/strong&gt; along vector $\mathbf{v}$. We assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\tau f(\mathbf{\tau}_0)  \neq \mathbf{0}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\|v\|^2=1} \lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t} = ( \nabla_\tau f(\mathbf{\tau}_0) )^T \mathbf{v} 
\end{aligned}\tag{1}\label{1}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It is easy to see that the optimal solution of Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{opt}}= -\frac{\nabla_\tau f(\mathbf{\tau}_0) }{\|\nabla_\tau f(\mathbf{\tau}_0) \|}$&lt;/code&gt;, which is the (Euclidean) steepest direction at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;distance-induced-by-the-fisher-rao-metric&quot;&gt;Distance induced by the Fisher-Rao metric&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;To generalize  the steepest direction at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; in a Riemannian manifold, we first formulate a similar optimization problem like Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; in the manifold case.
To do so, we have to define the length of a vector in manifold cases. In &lt;a href=&quot;/posts/2021/11/Geomopt03/#standard-euclidean-gradients-are-not-invariant&quot;&gt;Part III&lt;/a&gt;, we will show that the (standard) length does not perseve under a parameter transform while the length induced by the Fisher-Rao metric does.&lt;/p&gt;

&lt;p&gt;As mentioned at &lt;a href=&quot;/posts/2021/09/Geomopt01/#fisher-rao-metric&quot;&gt;Part I&lt;/a&gt;, the FIM $\mathbf{F}$ should be positive definite. We can use FIM to define the length/norm of a vector (e.g., a Riemannian gradient) $\mathbf{v}$ at a point in a manifold via a weighted inner product.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\|\mathbf{v}\|_F := \sqrt{\mathbf{v}^T \mathbf{F} \mathbf{v}}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The positive-definiteness of FIM is essential since we do not want a non-zero vector has a zero length.&lt;/p&gt;

&lt;p&gt;The distance (and orthogonality) between two &lt;span style=&quot;color:red&quot;&gt;vectors&lt;/span&gt; at the &lt;span style=&quot;color:red&quot;&gt;same&lt;/span&gt;  point is also induced by FIM since we can define them by the inner product as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
d(\mathbf{v},\mathbf{w}) := \|\mathbf{v}-\mathbf{w}\|_F
\end{aligned}
$$&lt;/code&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt; are two vectors evaluted at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In manifold cases, we have to distinguish the difference between a point (e.g., parameter array $\tau_0$) and a vector (e.g., Riemannian gradient under a parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;).
This point is crucial to (natural) gradient-based methods in &lt;a href=&quot;/posts/2021/11/Geomopt04/#two-kinds-of-spaces&quot;&gt;Part IV&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/span&gt;: We do NOT define the distance between two points in the manifold, which will be discussed &lt;a href=&quot;#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;here&lt;/a&gt;.
We also do NOT define the distance between a vector at one point and another vector at a distinct point.&lt;/p&gt;

&lt;h1 id=&quot;directional-derivatives-in-a-manifold&quot;&gt;Directional derivatives in a manifold&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;As we shown before, the objective function in Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; is a directional derivative in Euclidean cases.
The next step is to generalize the concept of directional derivatives in a manifold.&lt;/p&gt;

&lt;p&gt;Recall that a manifold should be locally like a vector space under &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;&lt;strong&gt;intrinsic&lt;/strong&gt; parameterization&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}$&lt;/code&gt;.
Using this parameterization, consider an optimization problem $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$, where the parameter space $\Omega_\tau$ is determined by the parameterization and the manifold. Recall that we have a local vector space structure denoted by $E$ if we parametrize the manifold with an intrinsic parameterization.&lt;/p&gt;

&lt;p&gt;Therefore, we can similarly define a directional derivative at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; along Riemannian vector $\mathbf{v}$ as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t}$&lt;/code&gt;, where $t$ is a scalar real number. The main point is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v}$&lt;/code&gt; stays in the parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; thanks to the &lt;strong&gt;local vector space&lt;/strong&gt; structure.&lt;/p&gt;

&lt;p&gt;Recall that we allow a &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;small perturbation&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$E$&lt;/code&gt; contained in  parameter space  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$E \subset \Omega_\tau$&lt;/code&gt;) since  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}$&lt;/code&gt; is an intrinsic parameterization.
Therefore, when $|t|$ is small enough, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v} $&lt;/code&gt; stays in the parameter space and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\mathbf{\tau}_0+t\mathbf{v})$&lt;/code&gt; is well-defined.
Note that we only require &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v} \in \Omega_\tau$&lt;/code&gt; when $|t|$ is small enough. When $|t|$ is small enough, this is possible since a line segment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{\tau}_0+t\mathbf{v} \in E$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$E \subset \Omega_\tau$&lt;/code&gt;.
 Technically, this is because  $\Omega_\tau$ is an open set in $\mathcal{R}^K$, where $K$ is the number of entires of parameter array &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Under &lt;strong&gt;intrinsic&lt;/strong&gt; parameterization $\mathbf{\tau}$, the directional derivative remains the same as in the Euclidean case thanks to the &lt;strong&gt;local vector space&lt;/strong&gt; structure.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$\begin{aligned} \lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t} = ( \nabla_\tau f(\mathbf{\tau}_0))^T \mathbf{v} \end{aligned}$$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The following example illustrates directional derivatives in manifold cases.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;Valid case:&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;blockquote&gt;

      &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is a &lt;strong&gt;local intrinsic&lt;/strong&gt; parameterization for the unit sphere.&lt;/p&gt;

      &lt;p&gt;The line segment from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v} $&lt;/code&gt;  is shown in blue, which is the parameter representation of the yellow curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt; in the manifold.
We will show later that Riemannian gradient vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; under this parametrization at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; is the &lt;strong&gt;parameter representation&lt;/strong&gt; of the tangent vector of curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt; at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt;.&lt;/p&gt;

      &lt;p&gt;&lt;img src=&quot;/img/sphere_simple.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

      &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt; is NOT the shortest curve in the manifold between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_1$&lt;/code&gt;.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Invalid case:&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;blockquote&gt;

      &lt;p&gt;A directional derivative can be ill-defined under a &lt;strong&gt;non-intrinsic&lt;/strong&gt; parameterization.&lt;/p&gt;

      &lt;p&gt;We use &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;parameterization 3&lt;/a&gt; for unit circle &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{S}^1$&lt;/code&gt;, where the red line segment passes through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0=(0,1) \in \mathcal{S}^1 $&lt;/code&gt;.&lt;/p&gt;

      &lt;p&gt;&lt;img src=&quot;/img/tangent_non.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

      &lt;p&gt;Any other point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0 + t\mathbf{v}$&lt;/code&gt; in the line segment leaves the manifold for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$t\neq 0$&lt;/code&gt; and thus, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\mathbf{\tau}_0+t\mathbf{v})$&lt;/code&gt; is not well-defined.
The main reason is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is not an intrinsic parameterization.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;h1 id=&quot;riemannian-steepest-direction&quot;&gt;Riemannian steepest direction&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Recall that we have defined the length of a Riemannian vector and directional derivatives in a manifold.
Now, we can introduce the Riemannian steepest direction. We will use this to define/compute natrual-gradients.&lt;/p&gt;

&lt;p&gt;Given  a smooth scalar funcion defined in a manifold $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$ under an intrinsic parameterization $\mathbf{\tau}$. We can define the Riemannian steepest direction as the optimal solution to the following optimization problem.  The optimization problem is expressed in terms of a directional derivative along Riemannian vector $\mathbf{v}$, where we assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\tau f(\mathbf{\tau}_0)  \neq \mathbf{0}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{ \color{red} {\|v\|_{F}^2=1} } ( \nabla_\tau f(\mathbf{\tau}_0) )^T  \mathbf{v} 
\end{aligned} \tag{2}\label{2}
$$&lt;/code&gt; 
where $\mathbf{v}$ can be any (Riemannian) vector at current point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; satisfied the norm constraint.&lt;/p&gt;

&lt;p&gt;The Lagrangian function of this problem is given below, where $\lambda$ is a Lagrange multiplier. 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
L(\mathbf{v},\lambda) =  ( \nabla_\tau f(\mathbf{\tau}_0))^T \mathbf{v} + \lambda (\|v\|_{F}^2 - 1) = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0) + \lambda (\mathbf{v}^T \mathbf{F}(\mathbf{\tau}_0) \mathbf{v}  - 1) 
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}(\mathbf{\tau}_0)$&lt;/code&gt; is FIM evaluated at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;One of the Karush–Kuhn–Tucker (KKT) necessary conditions implies that
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{0} = \nabla_{v} L(\mathbf{v}_{\text{opt}},\lambda) = \nabla_\tau f(\mathbf{\tau}_0) + 2 \lambda \mathbf{F}(\mathbf{\tau}_0) \mathbf{v}_{\text{opt}}
\end{aligned}
$$&lt;/code&gt;
When $\lambda \neq 0$, vector 	&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{opt}}$&lt;/code&gt; should be proportional to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0)$&lt;/code&gt;, where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}^{-1}(\mathbf{\tau}_0)$&lt;/code&gt; is well-defined since FIM &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}(\mathbf{\tau}_0)$&lt;/code&gt; is positive definite.&lt;/p&gt;

&lt;p&gt;We can show that the optimal solution of Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{opt}}= -\frac{ \mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0) }{\| \mathbf{F}^{-1}(\mathbf{\tau}_0)\nabla_\tau f(\mathbf{\tau}_0) \|_F}$&lt;/code&gt;, which gives us the Riemannian steepest direction at current &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Euclidean&lt;/strong&gt; steepest direction &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{euclid}}= -\frac{ \nabla_\tau f(\mathbf{\tau}_0) }{\| \nabla_\tau f(\mathbf{\tau}_0) \|_F}$&lt;/code&gt; is &lt;strong&gt;not&lt;/strong&gt; the optimal solution of  Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}(\tau_0) \neq \mathbf{I}$&lt;/code&gt;.
We will illustrate this by using an example.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;Euclidean steepest direction is not the optimal solution of  Eq. $\eqref{2}$&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;blockquote&gt;

      &lt;p&gt;Consider &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}(\tau_0)=\begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; \frac{1}{2} \end{bmatrix}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\tau f(\mathbf{\tau}_0)=\begin{bmatrix} 1\\1 \end{bmatrix}$&lt;/code&gt;.
We have the following results
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\| F^{-1} \nabla_\tau f(\mathbf{\tau}_0) \|_F^2  =  \nabla_\tau^T f(\mathbf{\tau}_0) \mathbf{F}^{-1}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) = 3; \,\,\,
\| \nabla_\tau f(\mathbf{\tau}_0) \|_F^2  =  \nabla_\tau^T f(\mathbf{\tau}_0) \mathbf{F}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) = \frac{3}{2}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{v}_{\text{opt}} = -\begin{bmatrix} \frac{1}{\sqrt{3}} \\ \frac{2}{\sqrt{3}} \end{bmatrix}; \,\,\,
\mathbf{v}_{\text{euclid}}=
-\begin{bmatrix} \sqrt{\frac{2}{3}} \\ \sqrt{\frac{2}{3}} \end{bmatrix}\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{v}_{\text{opt}}^T \nabla_\tau f(\mathbf{\tau}_0)= -\sqrt{3}  &amp;lt;  -\frac{2\sqrt{2}}{\sqrt{3}} = \mathbf{v}_{\text{euclid}}^T \nabla_\tau f(\mathbf{\tau}_0) 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;Therefore, the Euclidean steepest direction &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{euclid}}$&lt;/code&gt; is not the optimal solution of  Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt;.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;p&gt;Given a scalar function $f(\mathbf{\tau})$, if its &lt;strong&gt;Euclidean&lt;/strong&gt; (steepest) gradient is $\nabla_\tau f(\mathbf{\tau})$, its &lt;strong&gt;Riemannian&lt;/strong&gt; (steepest) gradient is defined as $ \mathbf{F}^{-1}(\mathbf{\tau}) \nabla_\tau f(\mathbf{\tau})$ in literature.
We use a learning-rate to control the length of a gradient instead of normalizing its length. 
Since we use the Fisher-Rao metric, the Riemannian gradient is also known as the &lt;strong&gt;natural&lt;/strong&gt; gradient.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;Example: multivariate Gaussian&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;blockquote&gt;

      &lt;p&gt;Consider a $d$-dimensional Gaussian family with zero mean and precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt; discussed in &lt;a href=&quot;/posts/2021/09/Geomopt01/#manifold-dimension&quot;&gt;Part I&lt;/a&gt;. &lt;br /&gt;
We specify an intrinsic parameterization $\mathbf{\tau}$ of the family as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(\mathbf{w} |\mathbf{0},\mathbf{S}^{-1}) \Big| \mathrm{MatH}(\tau) = \mathbf{S}   \succ \mathbf{0} \}$&lt;/code&gt; with parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \mathrm{vech}(\mathbf{S})$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is a $\frac{d(d+1)}{2}$-dim array and map $\mathrm{MatH}()$ is the inverse map of  the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization&quot;&gt;half-vectorization function&lt;/a&gt; $\mathrm{vech}()$.&lt;/p&gt;

      &lt;p&gt;Technically speaking, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt; is NOT an intrinsic parameter due to the symmetry constraint. In other words, FIM w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt; will be singular if  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt; is considered as a matrix parameter with $d^2$ degrees of freedom.&lt;/p&gt;

      &lt;p&gt;In the literature, a natural gradient w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt; is defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{MatH}(\mathbf{v})$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; is a natural-gradient w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}(\mathbf{S})$&lt;/code&gt;.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;h1 id=&quot;riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Riemannian gradients as tangent vectors (optional)&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;In the previous section, we only consider Riemannian vectors/gradients under a parametrization $\tau$.
Now, we will disucss a more abstract concept of Riemannian vectors without a parametrization. This concept is often used to show the invariance of Riemannian gradients, which will be discussed in &lt;a href=&quot;/posts/2021/11/Geomopt03/#parameter-transform-and-invariance&quot;&gt;Part III&lt;/a&gt;.  In physics, this invariance means that a law of physics should be independent of the choice of (reference) coordinate systems.&lt;/p&gt;

&lt;p&gt;A Riemannian gradient denoted by $\mathbf{v}(\tau)$ is indeed a tangent vector $\mathbf{v}$ of a smooth curve in the manifold under the parametrization $\tau$. 
The set of tangent vectors evaluated at $\mathbf{\tau}_0$ is called the tangent space at the corresponding point. 
We will illustrate this by an example.&lt;/p&gt;

&lt;p&gt;Let’s denote the unit sphere by $\mathcal{M}$, where we set the origin to be the center of the sphere. Point $\mathbf{x_0}=(0,0,1)$ is the north pole.
We use the following parameterization, where the top half of the sphere can be locally expressed as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{(\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2})|  \tau_x^2 + \tau_y^2 &amp;lt;1 \}$&lt;/code&gt; with parameter $\mathbf{\tau}=(\tau_x,\tau_y)$. 
Under parametrization $\mathbf{\tau}$, we have the following parametric representations.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;     &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Parametric representation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;North pole  $\mathbf{x_0}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$\mathbf{\tau}_0=(0,0)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Intrinsic parameter space&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;red space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau:=\{ (\tau_x,\tau_y)| \tau_x^2 + \tau_y^2 &amp;lt;1 \}$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Tangent space at $\mathbf{x_0}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;green space  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Yellow curve from $\mathbf{x_0}$ to $\mathbf{x_1}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;blue line segment from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v}(\tau_0)$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/img/sphere.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt; is a parameter array, which is a representation of a point $\mathbf{x}_0$ while $\mathbf{v}(\tau_0)$ is  a Riemannian gradient, which is a representation of the tangent vector of curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma$&lt;/code&gt; at point $\mathbf{x}_0$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Be aware of the differences shown in the table.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;     &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;parametric representation of&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;supported operations&lt;/th&gt;
      &lt;th&gt;distance  discussed in this post&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt; (vector/natural-gradient) space&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;tangent vector space at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;real scalar product, vector addition&lt;/td&gt;
      &lt;td&gt;defined&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; (point/parameter) space&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;top half of the manifold&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt; &lt;strong&gt;local&lt;/strong&gt; &lt;/span&gt; scalar product, &lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;local&lt;/strong&gt; &lt;/span&gt; vector addition&lt;/td&gt;
      &lt;td&gt;undefined&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Under &lt;strong&gt;intrinsic&lt;/strong&gt; parametrization $\tau$, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau \subset \mathcal{R}^2$&lt;/code&gt;. Thus, we can perform this operation in $\Omega_\tau$ space: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0 +t\mathbf{v}(\tau_0) \in \Omega_\tau$&lt;/code&gt; when scalar &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$|t|$&lt;/code&gt; is small enough. Note that we only define the &lt;a href=&quot;#distance-induced-by-the-fisher-rao-metric&quot;&gt;distance&lt;/a&gt; between two vectors in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt; space. The distance between two points in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; space is undefined in this post.&lt;/p&gt;

&lt;h2 id=&quot;parameterization-free-repesentation-of--vector-mathbfv&quot;&gt;Parameterization-free repesentation of  vector $\mathbf{v}$&lt;/h2&gt;

&lt;p&gt;The tangent vector $\mathbf{v}$ at point $\mathbf{x_0}$  can be viewed as the &lt;strong&gt;tangent direction&lt;/strong&gt; of a (1-dimensional) smooth curve $\gamma(t) \in \mathcal{M}$, where $\gamma(0)=\mathbf{x_0}$,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d {\gamma}(t) }{d t} \Big|_{t=0}=\mathbf{v}$&lt;/code&gt; and the support of $\gamma(t)$ denoted by $\mathbf{I}$ is an open interval in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^1$&lt;/code&gt; containing 0. 
Since a curve $\gamma(t)$ is a geometric object, this is a parameterization-free repesentation of a vector.&lt;/p&gt;

&lt;h2 id=&quot;parameterization-dependent-repesentation-of-vector-mathbfv&quot;&gt;Parameterization-dependent repesentation of vector $\mathbf{v}$&lt;/h2&gt;

&lt;p&gt;Given intrinsic parametrization $\tau$, we can define the parametric representation of the curve denoted by ${\gamma}_\tau(t)$, where the domain is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau \subset \mathcal{R}^1$&lt;/code&gt;.
The parametric representation of vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; is defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0):= \frac{d {\gamma}_{\tau}(t) }{d t} \Big|_{t=0}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\gamma}_{\tau}(0)=\tau_0$&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example&lt;/p&gt;

  &lt;p&gt;Consider the yellow curve $\gamma(t) = (t v_{x}, t v_{y}, \sqrt{1 - t^2(v_{x}^2 + v_{y}^2) } ) \in \mathcal{M} $ 
and the blue line segment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\gamma}_{\tau}(t)= (t v_{x} , t v_y  ) \in \Omega_\tau $&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$|t|$&lt;/code&gt; must be small enough.&lt;/p&gt;

  &lt;p&gt;The parametric  representation of the vector is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0):= \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}=(v_x,v_y)$&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A Riemannian gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0)$&lt;/code&gt; can be viewed as a parametric representation of tangent vector  $\mathbf{v}$ as shown below.&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;Consider a smooth scalar function defined in the manifold $h: \mathcal{M} \to \mathcal{R}$. In the unit sphere case, consider &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h(\mathbf{x})$&lt;/code&gt; subject to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}^T \mathbf{x}=1$&lt;/code&gt;.
Under parameterization $\mathbf{\tau}$, we can locally re-expressed the function as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\tau(\mathbf{\tau}):=h( (\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2}) )$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau \in \Omega_\tau$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;By the definition of a directional derivative, the following identity holds for any smooth scalar function $h$: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$[\nabla h_\tau(\mathbf{\tau}_0)]^T \mathbf{v}(\mathbf{\tau}_0) =\frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0}$&lt;/code&gt;, where $h_\tau$ is the parametric representation of  $h$ . Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(h_\tau \circ {\gamma}_\tau) (t)$&lt;/code&gt; is a function defined from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau $&lt;/code&gt; to $\mathcal{R}^1$, where domain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau \subset \mathcal{R}^1$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;The &lt;span style=&quot;color:red&quot;&gt; &lt;strong&gt;key&lt;/strong&gt; &lt;/span&gt; observation is that function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(h_\tau \circ {\gamma}_\tau) (t)$&lt;/code&gt; becomes a scalar function defined in $\mathcal{R}^1$ thanks to parametrization $\tau$. Thus, we can use the standard chain rule.&lt;/p&gt;

  &lt;p&gt;By the chain rule, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0}=[\nabla h_\tau(\mathbf{\tau}_0)]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\gamma}_\tau(0)=\tau_0$&lt;/code&gt;. Thus,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0) =  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$&lt;/code&gt; since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla h_\tau(\mathbf{\tau}_0)$&lt;/code&gt; can be arbitrary.&lt;/p&gt;

  &lt;p&gt;In summary, a Riemannian gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0)$&lt;/code&gt; can be viewed as a parametric representation of the tangent vector 
 of curve $\gamma(t)$ at $\mathbf{x}_0$ since  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\gamma}_\tau(t)$&lt;/code&gt; is the parametric representation of $\gamma(t)$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;riemannian-gradient-space-has-a-vector-space-structure&quot;&gt;(Riemannian) gradient space has a vector-space structure&lt;/h2&gt;

&lt;p&gt;We can also define vector additions and real scalar products in a tangent vector space by using tangent directions of curves in the manifold with/without a parameterization.&lt;/p&gt;

&lt;p&gt;The key takeway is that a vector space structure is an integral part of a tangent &lt;strong&gt;vector&lt;/strong&gt; space. On the other hand, we have to use an intrinsic parametrization $\tau$ to artificially create a local vector space structure in parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt;. Recall that a parameter space is a parametric representation of a set of &lt;strong&gt;points&lt;/strong&gt; in a manifold.
We will discuss more about this in &lt;a href=&quot;/posts/2021/11/Geomopt04/#two-kinds-of-spaces&quot;&gt;Part IV&lt;/a&gt;&lt;/p&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><summary type="html">Goal This blog post should help readers to understand natural-gradients, which are known as Riemannian gradients with the Fisher-Rao metric. The main propose of this post is to show how to compute/define a natural-gradient. The space of natural-gradients evaluated at the same point is called a tangent space at that point.</summary></entry><entry><title type="html">Part I: Manifolds with the Fisher-Rao Metric</title><link href="/posts/2021/09/Geomopt01/" rel="alternate" type="text/html" title="Part I: Manifolds with the Fisher-Rao Metric" /><published>2021-09-06T00:00:00-07:00</published><updated>2021-09-06T00:00:00-07:00</updated><id>/posts/2021/09/Geomopt01</id><content type="html" xml:base="/posts/2021/09/Geomopt01/">&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post focuses on the Fisher-Rao metric, also known as the Fisher information matrix (FIM). The purpose is to show that,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The FIM plays an essential role in statistics and machine learning&lt;/li&gt;
  &lt;li&gt;For a parametric distribution, it induces a &lt;strong&gt;Riemannian&lt;/strong&gt; geometric-structure&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The discussion here is informal and focuses on more on intuitions, rather than rigor.&lt;/p&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Let’s start with some motivation: why should we care about geometric structures of probability distributions?
Probability distributions form the backbone of majority of machine-learning approaches, for example, any approach that uses probabilistic modeling is built upon such distributions. 
For all such cases, we can exploit the underlying geometric structure, induced by the Fisher-Rao metric, which is the topic of this blog. 
Below, we give some common examples from machine learning, where probability  distributions naturally arise.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Least Square (Empirical Risk Minimization):&lt;/p&gt;

  &lt;p&gt;Given N input-output pairs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(x_i,y_i)$&lt;/code&gt;,  the least-square loss can be viewed as expectation under a probability distribution.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau} \frac{1}{2n} \sum_{i=1}^{n}   (y_i-x_i^T\tau)^2  &amp;amp; = \max_{\tau} \frac{1}{n}  \sum_{i=1}^{n}  \log \mathcal{N}(y_i | x_i^T\tau,1) \\
&amp;amp; \approx \int [ \log p(x,y | \tau) ]  d p(x,y | \tau) =  E_{ \color{red}  { p(x,y | \tau) } } [ \log  p(x,y | \tau)    ] 
\end{aligned} \tag{1}\label{1}
$$&lt;/code&gt;
Here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(x,y | \tau) = \mathcal{N}(y | x^T\tau,1) p(x) $&lt;/code&gt; is assumed to be the data-generating distribution, and the least-square loss is the finite-sample approximation of the expectation. The normal distribution is denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathcal{N} (y | x^T\tau,1) $&lt;/code&gt; with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ x^T\tau $&lt;/code&gt; and variance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ 1 $&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;Algorithms such as  &lt;strong&gt;Fisher scoring&lt;/strong&gt; and &lt;strong&gt;(emprical) natural-gradient descent&lt;/strong&gt; are commonly used methods that exploit the geometric structure of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$p(x,y | \tau)$&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--- \stackrel{\eqref{1}} (Eq. `$\eqref{1}$`) ---&gt;

&lt;blockquote&gt;
  &lt;p&gt;Variational Inference:&lt;/p&gt;

  &lt;p&gt;Given a prior &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(z) $&lt;/code&gt; and a likelihood &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(\mathcal{D} | z ) $&lt;/code&gt; over an latent vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$z$&lt;/code&gt; and known data &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathcal{D} $&lt;/code&gt;, we can approximate the exact posterior &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p( z | \mathcal{D} ) =\frac{p(z,\mathcal{D})}{p(\mathcal{D})} $&lt;/code&gt; by optimizing a variational objective with respect to  an approximated distribution &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ q(z | \tau) $&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau} \mathrm{KL} [ { q(z | \tau) || p( z | \mathcal{D} ) }   ] = E_{ \color{red}  {q(z | \tau)} } [ \log q(z | \tau) - \log p( z , \mathcal{D} )    ]  + \log p(\mathcal{D} ) 
\end{aligned} \tag{2}\label{2}
$$&lt;/code&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathrm{KL} [ q(z) ||  p(z) ] :=  E_{ {q(z)} } [ \log \big(\frac{q(z)}{p(z)}\big) ]$&lt;/code&gt; is the Kullback–Leibler divergence.&lt;/p&gt;

  &lt;p&gt;The &lt;strong&gt;natural-gradient variatioal inference&lt;/strong&gt; is an algorithm that speeds up the inference by exploiting the geometry of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(z|\tau)$&lt;/code&gt; induced by the Fisher-Rao metric.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Evolution Strategies (Gradient-free Search):&lt;/p&gt;

  &lt;p&gt;In gradient-free optimization, we often use a search distribution &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi(a | \tau ) $&lt;/code&gt; to find the optimal solution of an objective funtion &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h(a)$&lt;/code&gt; by solving the following problem:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau}  E_{ \color{red}  {\pi(a | \tau)} } [ h(a)  ]
\end{aligned} \tag{3}\label{3}
$$&lt;/code&gt;
The &lt;strong&gt;natural evolution strategies&lt;/strong&gt; is an algorithm that speeds up the search process by exploiting the geometry of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi(a|\tau)$&lt;/code&gt;.
In the context of reinforcement learning,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi(a | \tau ) $&lt;/code&gt; is known as the policy distribution to generate actions and the natural evolution strategies is known as the &lt;strong&gt;natural policy gradient&lt;/strong&gt; method.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In these examples, we can express the objective function in terms of an expectation under a parametric family highlighted in red. 
By doing so, we can exploit the geometric structure of a parametric family denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w|\tau) $&lt;/code&gt; induced by the Fisher-Rao metric.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Example       &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$w$&lt;/code&gt;        &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w|\tau) $&lt;/code&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Least Square&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;observation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(x,y)$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$p(x,y|\tau)$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Variational Inference&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;latent variable $z$&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(z|\tau)$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Evolution Strategies&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;decision variable $a$&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi(a|\tau)$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We will mainly focus on the geometric structure of (finite-dimensional) parametric families.
For example, let’s consider a 1-dimensional Gaussian family.
The following figure illustrates four distributions in a Gaussian family denoted by
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&amp;gt;0 \}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w | \tau ): = \mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $&lt;/code&gt;  and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau :=(\mu,\sigma) $&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/gauss1d.png&quot; alt=&quot;Figure 2&quot; title=&quot;Source:Wikipedia&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;intrinsic-parameterizations&quot;&gt;Intrinsic Parameterizations&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;We start by discussing parameterizations for a distribution family. As we will see, a proper parameterization plays an important role in defining FIM.
We can not use an arbitrary differentiable parameterization to represent a geometric structure.
This structure is also known as a manifold structure. A (smooth) manifold should be locally like a “flat” vector space. For example, consider the (curved) Earth surface. It looks like a (locally) flat space to us.&lt;/p&gt;

&lt;p&gt;The main reason of using an intrinsic parameterization is (1) the topology of a parameter space is nice. (2) The (exact) FIM is non-singular and well-defined (finite).
These properties will play a key role in &lt;a href=&quot;/posts/2021/11/Geomopt04/#natural-gradient-descent-in-an-intrinsic-parameter-space&quot;&gt;Part IV&lt;/a&gt; for natural-gradient descent.&lt;/p&gt;

&lt;p&gt;We require that a manifold should be locally like a vector space denoted by $ E $ under a parameterization.
A local vector-space structure means that we can do &lt;strong&gt;local vector additions&lt;/strong&gt; and &lt;strong&gt;local real scalar products&lt;/strong&gt; (see &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt; for the details.)&lt;/p&gt;

&lt;p&gt;Intuitively, this vector-space structure means that a local (small) perturbation $ E $ at each point should not take out of the parameter space.
Technically, the parameter space denoted by $\Omega_\tau$ should be an &lt;strong&gt;open&lt;/strong&gt; set in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt; is the number of entries of a parameter array.
As we will see soon, FIM is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt;-by-&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt; matrix.&lt;/p&gt;

&lt;p&gt;To illustrate this, let’s consider a unit circle in $\mathcal{R}^2$ as shown in the Figure.
Clearly, a point $ (0,1) $ highlighted in green is in the circle, where we consider its center as the origin.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/circle.png&quot; title=&quot;Source:Wikipedia&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Parametrization 1 (Intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;A (local) parametrization at $ (0,1) $ highlighted in red for the circle is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ (t,\sqrt{1-t^2}) | -h&amp;lt;t&amp;lt;h \} $&lt;/code&gt;, where $h=0.1$. We use &lt;strong&gt;one&lt;/strong&gt; (scalar) parameter in this parametrization.&lt;/p&gt;

  &lt;p&gt;The manifold is (locally) “flat” since we can always find a small &lt;strong&gt;1-dimensional&lt;/strong&gt; perturbation $E$ in the &lt;strong&gt;1-dimensional&lt;/strong&gt; parameter space  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_t=\{t|-h&amp;lt;t&amp;lt;h \} $&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/img/1d-perturbation.png&quot; title=&quot;Fig&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

  &lt;p&gt;This parametrization is called an &lt;strong&gt;intrinsic&lt;/strong&gt; parameterization.&lt;/p&gt;

  &lt;p&gt;We can similarly define a (local) parametrization at each point of the circle. In fact, we can use  four (local) parameterizations to represent the circle as shown below.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/img/charts.png&quot; title=&quot;Source:Wikipedia&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Parametrization 2 (Non-intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;Let’s define a map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f : [0,2\pi) \rightarrow \mathcal{S}^1 $&lt;/code&gt; such that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\theta) = (\sin \theta, \cos \theta ) $&lt;/code&gt;, where we use $\mathcal{S}^1$ to denote the circle.&lt;/p&gt;

  &lt;p&gt;A (global) parametrization of the circle is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ f(\theta) | \theta \in [0,2\pi)  \}$&lt;/code&gt;, where we use one (scalar) parameter.&lt;/p&gt;

  &lt;p&gt;This map $f$ is bijective and smooth. However, the parameter space is not open in $\mathcal{R}$. Moreover, its inverse map $f^{-1}$ is &lt;strong&gt;not&lt;/strong&gt; continous at point $(0,1) \in  \mathcal{S}^1$.&lt;/p&gt;

  &lt;p&gt;This smoothness requirement of the inverse map is essential when it comes to reparametrization (A.K.A. parameter transformation). The requirement gives us a way to generate new intrinsic parameterizations. In other words, the Jacobian matrix is non-singular if the requirement is satisfied. Thus, we can safely use the chain rule and inverse function theorem.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Parametrization 3 (Non-intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;The circle does &lt;strong&gt;not&lt;/strong&gt; look like a flat space under the following parametrization
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ (x,y) | x^2+y^2=1, x,y \in \mathcal{R} \} $&lt;/code&gt;. The number of entries in this parameter array is 2.&lt;/p&gt;

  &lt;p&gt;The reason is that we cannot find a small &lt;strong&gt;2-dimensional&lt;/strong&gt; perturbation $E$ in the &lt;strong&gt;2-dimensional&lt;/strong&gt; parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_{\tau}=\{(x,y)|x^2+y^2=1 \} $&lt;/code&gt; due to the constraint $x^2+y^2=1$. In other words, $\Omega_\tau$ is not open in $\mathcal{R}^2$.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/img/2d-perturbation.png&quot; title=&quot;Fig&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;intrinsic-parameterizations-for-parametric-families&quot;&gt;Intrinsic Parameterizations for Parametric families&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Now, we discuss how to choose a parameterization given a parametric family so that we can exploit the geometric structure induced by the Fisher-Rao metric.&lt;/p&gt;

&lt;p&gt;Given a parametric distribution family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w|\tau) $&lt;/code&gt; indexed by its parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w|\tau) $&lt;/code&gt; should be smooth w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \tau $&lt;/code&gt; by considering &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ w $&lt;/code&gt; to be fixed.
We say a parametrization is &lt;strong&gt;intrinsic&lt;/strong&gt; if the following condition for parameter (array) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau $&lt;/code&gt; holds:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regularity Condition&lt;/strong&gt;:  The set of partial derivatives 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \partial_{\tau_i} \log p(w|\tau) \} $&lt;/code&gt;  should be linearly independent. In other words, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sum_i c_i \partial_{\tau_i} \log p(w|\tau)= 0 $&lt;/code&gt;, where constant &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_i$&lt;/code&gt; must be zero and the value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_i$&lt;/code&gt; does not depent on  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$w$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Note that this regularity condition implicitly assumes that the parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; is an open set in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt; due to the definition of the partial derivatives, where K is the number of entries in parameter array &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;.
We will discuss more about this at &lt;a href=&quot;#caveats-of-the-fisher-matrix-computation&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We will use the following examples to illustrate this condition.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 1 (Intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;We will show that the regularity condition holds. Consider a 1-dimensional Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&amp;gt;0 \}$&lt;/code&gt; with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt;, variant &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sigma$&lt;/code&gt;, and parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\mu,\sigma) $&lt;/code&gt;.
The partial derivatives are
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \partial_{\mu} \log \mathcal{N}(w |\mu,\sigma) = \frac{w-\mu}{\sigma}, \,\,\, \partial_{\sigma} \log \mathcal{N}(w |\mu,\sigma) = \frac{ (w-\mu)^2 }{2 \sigma^2} - \frac{1}{2\sigma} 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \partial_{\mu} \log \mathcal{N}(w |\mu,\sigma) \Big|_{\mu=0,\sigma=1}= w, \,\,\, \partial_{\sigma} \log \mathcal{N}(w |\mu,\sigma) \Big|_{\mu=0,\sigma=1} = \frac{ w^2 -1 }{2}  
\end{aligned}
$$&lt;/code&gt;
If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ c_1 w + c_2  (\frac{ w^2 -1 }{2}) = 0$&lt;/code&gt; holds for any $w$, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_1=c_2=0$&lt;/code&gt;, which implies  linear independence.&lt;/p&gt;

  &lt;p&gt;Similarly, we can show that for any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu \in \mathcal{R}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sigma &amp;gt;0$&lt;/code&gt;, 
the partial derivatives are linearly independent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 2 (Non-intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;We will show that the regularity condition fails. Consider a Bernoulli family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0  \}$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1) $&lt;/code&gt;, where function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathcal{I}(\cdot) $&lt;/code&gt; is the indicator function.
The partial derivatives are&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \partial_{\pi_0} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{\pi_1}{(\pi_0+\pi_1)^2}   
\end{aligned}
$$&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\pi_1} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} 
\end{aligned}
$$&lt;/code&gt;
Note that when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_0 = \pi_0 \neq 0 $&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_1= \pi_1 \neq 0$&lt;/code&gt;, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_0 \frac{\pi_1}{(\pi_0+\pi_1)^2}  + c_1 \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} = 0$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;Therefore, we can show that 
the partial derivatives are linearly dependent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 3 (Non-intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;We will soon show that the  condition fails for Bernoulli family  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0, \pi_0+\pi_1=1  \}$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1)$&lt;/code&gt;. 
The main reason is that the parameter space is not open in $\mathcal{R}^2$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 4  (Intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;We can show that the condition holds for Bernoulli family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) (1-\pi_0)  \Big| 0&amp;lt;\pi_0&amp;lt;1  \}$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \pi_0$&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;fisher-rao-metric&quot;&gt;Fisher-Rao Metric&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Given an intrinstic parameterization, we can define the Fisher-Rao metric under this parameterization as:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau) \Big) ]$&lt;/code&gt;.
Note that the metric could be ill-defined since the expectation may not exist.&lt;/p&gt;

&lt;p&gt;Given a parameterization,  we can express the metric in a matrix form as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{F}(\tau) := E_{p(wtau) }  [ \Big( \nabla_{\tau} \log p(w|\tau ) \Big)  \Big(\nabla_{\tau} \log (w|\tau) \Big)^T ]$&lt;/code&gt;,
where $K$ is the number of entries of parameter array $\tau$ and 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \nabla_{\tau} \log p(w|\tau ) := [ \partial_{\tau_1} \log p(w|\tau ), \cdots, \partial_{\tau_K} \log p(w|\tau ) ]^T  $&lt;/code&gt; is a column vector.&lt;/p&gt;

&lt;p&gt;The matrix form is also known as the &lt;strong&gt;Fisher information matrix&lt;/strong&gt; (FIM). Obviously, FIM depends on the choice of parameterizations. In many cases,  we could also compute FIM as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{F}(\tau) := - E_{p(w|\tau) }  [  \nabla_{\tau}^2 \log p(w|\tau )  ]$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The regularity condition guarantees that FIM is non-singular if the matrix exists.
The condition is also related to the model identification in maximum likelihood estimation.&lt;/p&gt;

&lt;p&gt;In the following discussion, we will assume the metric is well-defined.
In such cases, the Fisher-Rao metric is a valid Riemannian metric since the corresponding FIM is positive definite everywhere in an &lt;strong&gt;intrinsic&lt;/strong&gt; parameter space.
The Fisher-Rao metric is &lt;strong&gt;special&lt;/strong&gt; since it is closely related to  maximum likelihood estimation, central limit theorem, and principle of maximum entropy.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/span&gt;: An arbitrary Riemannian metric often is NOT useful for applications in machine learning.&lt;/p&gt;

&lt;p&gt;Recall that when we perform a parameter transformation, we require both the transformation map and its inverse map to be smooth.
If this smoothness requirement is satisfied, we can obtain a new intrinsic parameterization via the transformation.
In &lt;a href=&quot;/posts/2021/11/Geomopt03/#Pparameter-transform-and-invariance&quot;&gt;Part III&lt;/a&gt;,
we will show FIM is also positive definite under this new intrinsic parameterization.&lt;/p&gt;

&lt;h1 id=&quot;caveats-of-the-fisher-matrix-computation&quot;&gt;Caveats of the Fisher matrix computation&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;There are some caveats when it comes to the Fisher matrix computation. In particular, the regularity condition should be satisfied.
It is possible to define FIM under a non-intrinstic parameterization. However, FIM often is singular or ill-defined under a non-intrinstic  parameterization as shown below.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 1 (Ill-defined FIM):&lt;/p&gt;

  &lt;p&gt;Consider Bernoulli family  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0, \pi_0+\pi_1=1  \}$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1)$&lt;/code&gt;.
The following computation is not correct. Do you make similar mistakes like this?&lt;/p&gt;

  &lt;p&gt;Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  p(w|\tau ) =  \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1)$&lt;/code&gt;. The derivative is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$  \begin{aligned} \nabla_{\tau} \log p(w|\tau ) = \frac{1}{p(w|\tau )} [\mathcal{I}(w=0) ,  \mathcal{I}(w=1)]^T  \end{aligned} \tag{4}\label{4}$$&lt;/code&gt;
Thus, by Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt;, FIM under this  parameterization is&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$  \begin{aligned} F(\tau) = E_{p(w|\tau) } [ \frac{1}{p^2(w|\tau)} \begin{bmatrix} \mathcal{I}^2(w=0) &amp;amp;  \mathcal{I}(w=1)  \mathcal{I}(w=0) \\  \mathcal{I}(w=0)  \mathcal{I}(w=1) &amp;amp;  \mathcal{I}^2(w=1) \end{bmatrix} ]   = \begin{bmatrix} \frac{1}{\pi_0} &amp;amp;  0 \\ 0 &amp;amp;  \frac{1}{\pi_1} \end{bmatrix} \end{aligned}$$&lt;/code&gt;
This computation is not correct. Do you know why it is not correct?&lt;/p&gt;

  &lt;p&gt;The key reason is the equality constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi_0+\pi_1=1 $&lt;/code&gt;. Thus, Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; is &lt;strong&gt;incorrect&lt;/strong&gt;.&lt;/p&gt;

  &lt;p&gt;By definition, a Bernoulli distribution is valid only when the constraint holds. 
Thus, the constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi_0+\pi_1=1 $&lt;/code&gt; must be satisifed when we compute the Fisher matrix since the computation involves the expectation of this distribution.&lt;/p&gt;

  &lt;p&gt;Note that the gradient is defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \nabla_{\tau} \log p(w|\tau ) := [ \partial_{\pi_0} \log p(w|\tau ), \partial_{\pi_1} \log p(w|\tau ) ]^T $&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;Unfortunately, these partial derivatives do not exist. By the definition of  partial derivative &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \partial_{\pi_0} \log p(w|\tau )$&lt;/code&gt;, we fix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi_1$&lt;/code&gt; and allow &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi_0$&lt;/code&gt; to change.
However, given that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi_1$&lt;/code&gt; is fixed and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi_0 $&lt;/code&gt; is fully determined by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi_1$&lt;/code&gt; due to the equality constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi_0+\pi_1=1 $&lt;/code&gt;. 
Therefore, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  \partial_{\pi_0} \log p(w|\tau ) $&lt;/code&gt; is not well-defined.
In other words, the above Fisher matrix computation is not correct since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \nabla_{\tau} \log p(w|\tau ) $&lt;/code&gt; does not exist.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 2  (Singular FIM):&lt;/p&gt;

  &lt;p&gt;Consider Bernoulli family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0  \}$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1) $&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;We can show that FIM under this  parameterization is singular.&lt;/p&gt;

  &lt;p&gt;A Bernoulli family with a non-singular FIM can be defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) (1-\pi_0)  \Big| 0&amp;lt; \pi_0 &amp;lt; 1  \}$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \pi_0$&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;manifold-dimension&quot;&gt;Manifold Dimension&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;We can define the dimension of a manifold by using the dimension of an intrinsic parametrization. Mathematically, we can show that any intrinsic parametrization of a manifold has the same degrees of freedom.
We now illustrate this by examples.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;unit circle&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;open unit ball&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;closed ball&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/circle-org.png&quot; alt=&quot;Source:Wikipedia&quot; width=&quot;200&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/open-ball.png&quot; alt=&quot;Source:Wikipedia&quot; width=&quot;200&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/closed-ball.png&quot; alt=&quot;Source:Wikipedia&quot; width=&quot;200&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1-dim manifold&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2-dim manifold&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;non-manifold, which is indeed a manifold with (closed) boundary&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We will disucss more about the following statistical  manifolds in &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-steepest-direction&quot;&gt;Part II&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;1-dim Gaussian with zero mean&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$d$-dimensional Gaussian with zero mean&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |0,s^{-1}) \Big|  s&amp;gt;0 \}$&lt;/code&gt; with precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$s$&lt;/code&gt; &lt;br /&gt; under intrinsic parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = s $&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(\mathbf{w} |\mathbf{0},\mathbf{S}^{-1}) \Big| \mathrm{MatH}(\tau) = \mathbf{S}   \succ \mathbf{0} \}$&lt;/code&gt; with precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;  &lt;br /&gt; under intrinsic parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \mathrm{vech}(\mathbf{S})$&lt;/code&gt; is  a $\frac{d(d+1)}{2}$-dim array.  &lt;br /&gt;  Map $\mathrm{MatH}()$ is the inverse map of &lt;br /&gt; the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization&quot;&gt;half-vectorization function&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}()$&lt;/code&gt;. &lt;br /&gt;   &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}(\mathbf{S})$&lt;/code&gt; is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d(d + 1)}{2}$&lt;/code&gt;-dim array &lt;br /&gt;  obtained by vectorizing only the lower triangular part of (symmetric) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1-dim statistical manifold&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d(d+1)}{2}$&lt;/code&gt;-dim statistical  manifold&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><summary type="html">Goal This blog post focuses on the Fisher-Rao metric, also known as the Fisher information matrix (FIM). The purpose is to show that, The FIM plays an essential role in statistics and machine learning For a parametric distribution, it induces a Riemannian geometric-structure</summary></entry><entry><title type="html">Structured Natural Gradient Descent (ICML 2021)</title><link href="/posts/2021/07/ICML/" rel="alternate" type="text/html" title="Structured Natural Gradient Descent (ICML 2021)" /><published>2021-07-05T00:00:00-07:00</published><updated>2021-07-05T00:00:00-07:00</updated><id>/posts/2021/07/GeomProj01</id><content type="html" xml:base="/posts/2021/07/ICML/">&lt;p&gt;More about this work &lt;a class=&quot;citation&quot; href=&quot;#lin2021tractable&quot;&gt;[1]&lt;/a&gt;: &lt;a href=&quot;https://www.youtube.com/watch?v=vEY1ZxDJX8o&amp;amp;t=11s&quot;&gt;(Youtube) talk&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2102.07405&quot;&gt;ICML paper&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2107.10884&quot;&gt;workshop paper&lt;/a&gt;,
&lt;a href=&quot;/img/poster.pdf&quot;&gt;poster&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Natural-gradient descent (NGD) on structured parameter spaces  is computationally challenging.
We propose a flexible and efficient NGD method to incorporate structures via matrix Lie groups.&lt;/p&gt;

&lt;p&gt;Our NGD method&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;generalizes the exponential natural evolutionary strategy &lt;a class=&quot;citation&quot; href=&quot;#glasmachers2010exponential&quot;&gt;[2]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;recovers existing  Newton-like algorithms&lt;/li&gt;
  &lt;li&gt;yields new structured 2nd-order methods and adaptive-gradient methods with group-structural invariance &lt;a class=&quot;citation&quot; href=&quot;#lin2021structured&quot;&gt;[3]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;gives new NGD updates to learn structured covariances of Gaussian, Wishart and their mixtures&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Applications of our method:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;deep learning (structured adaptive-gradient),&lt;/li&gt;
  &lt;li&gt;non-convex optimization (structured 2nd-order),&lt;/li&gt;
  &lt;li&gt;evolution strategies (gradient-free),&lt;/li&gt;
  &lt;li&gt;variational inference (multimodal density with Monte Carlo reparameterization gradients).&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;ngd-for-optimization-inference-and-search&quot;&gt;NGD for Optimization, Inference, and Search&lt;/h1&gt;

&lt;p&gt;A unified view for problems in optimization, inference, and search
as optimization over a parametric family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)$&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
   \min_{ \tau \in \Omega_\tau } \mathcal{L}(\tau):= \mathrm{E}_{q(\text{w}| \tau )} \big[ \ell(\mathbf{w}) \big] + \gamma \mathrm{E}_{q(\text{w} |\tau )} \big[ \log q(w|\tau) \big] 
\end{aligned} \tag{1}\label{1}
$$&lt;/code&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt; is the decision variable,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\ell(\mathbf{w})$&lt;/code&gt; is a loss function, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; is the parameter space of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q$&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma\ge 0$&lt;/code&gt; is a constant.&lt;/p&gt;

&lt;p&gt;Using gradient descent and natural-gradient descent to solve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\textrm{GD: } &amp;amp;\tau_{t+1} \leftarrow \tau_t - \alpha \nabla_{\tau_t} \mathcal{L}(\tau) \\
\textrm{Standard NGD: } &amp;amp; \tau_{t+1} \leftarrow \tau_t - \beta\,\, \big[ \mathbf{F}_{\tau} (\tau_t) \big]^{-1} \nabla_{\tau_t} \mathcal{L}(\tau)
\end{aligned} 
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Advantages of NGD:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;recovers a Newton-like update for Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(\mathbf{w}|\mu,\mathbf{S})$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=(\mu,\mathbf{S})$&lt;/code&gt;, mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt;, and precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mu_{t+1}  &amp;amp; \leftarrow \mu_t - \beta \mathbf{S}_{t}^{-1} E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w \ell( \mathbf{w}) \big] } \\
\mathbf{S}_{t+1}  &amp;amp; \leftarrow (1-\beta \gamma)\mathbf{S}_t + \beta  E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) \big] }
\end{aligned} \tag{2}\label{2}
$$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;is less sensitive to parameter transformations  than GD&lt;/li&gt;
  &lt;li&gt;converges faster than GD&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/icml2021-fig01.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Challenges of standard NGD:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;NGD could violate parameterization constraints (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}_{t+1}$&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; may not be positive-definite)&lt;/li&gt;
  &lt;li&gt;Singular Fisher information matrix (FIM) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_{\tau}(\tau)$&lt;/code&gt; of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Limited precision/covariance structures&lt;/li&gt;
  &lt;li&gt;Ad-hoc approximations for cost reductions&lt;/li&gt;
  &lt;li&gt;Complicated and inefficient NG computation&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;ngd-using-local-parameterizations&quot;&gt;NGD using Local Parameterizations&lt;/h1&gt;

&lt;p&gt;Our method performs NGD updates in local parameter spaces while maintaining structures via matrix groups in auxiliary parameters. This decoupling enables a tractable NGD that exploits the structures in auxiliary parameter spaces.&lt;/p&gt;

&lt;p&gt;We consider the following three kinds of parameterizations.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Global (original) parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;New auxiliary parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt;  with a surjective map: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau= \psi(\lambda)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Local parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; at a current value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t$&lt;/code&gt; with a local map:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda = \phi_{\lambda_t} (\eta)$&lt;/code&gt;, where&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\phi_{\lambda_t}$&lt;/code&gt; is
&lt;span style=&quot;color:red&quot;&gt; tight &lt;/span&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t$&lt;/code&gt;: $\lambda_t \equiv \phi_{\lambda_t} (\eta_0)$, and  we assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0 =\mathbf{0}$&lt;/code&gt; to be a relative origin.&lt;/li&gt;
&lt;/ul&gt;

&lt;fieldset class=&quot;field-set&quot;&gt;
  &lt;legend class=&quot;leg-title&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;Our NGD:&lt;/span&gt;&lt;/legend&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$ 
\begin{aligned} 
\lambda_{t+1} &amp;amp; \leftarrow  \phi_{\lambda_t} \big( \overbrace{\eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} }^{ \text{NGD in local space} } \big) \,\,\,\, \textrm{(structure-preserving update in auxiliary space)}\\
\tau_{t+1} &amp;amp; \leftarrow \psi\big( \lambda_{t+1} \big) \,\,\,\, \textrm{(constraint-satisfaction update in global space)}
\end{aligned} 
$$&lt;/code&gt;&lt;/p&gt;
&lt;/fieldset&gt;
&lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\eta_0}^{(t)}$&lt;/code&gt; is
 the natural-gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\eta_0}^{(t)}$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0$&lt;/code&gt; tied to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t$&lt;/code&gt;, which is computed by the chain rule,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned} 
        \hat{\mathbf{g}}_{\eta_0}^{(t)} &amp;amp;=  \color{green} {\mathbf{F}_{\eta}(\eta_0)^{-1} } 
         \,\, \big[ \nabla_{\eta_0} \big[ \psi \circ \phi_{\lambda_t} (\eta) \big]
        \nabla_{\tau_t}\mathcal{L}(\tau) \big]  
\end{aligned} 
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_{\eta}(\eta_0)$&lt;/code&gt;  is the (exact) FIM for  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0$&lt;/code&gt; tied to  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t$&lt;/code&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/img/icml2021-fig03.png&quot; width=&quot;500&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; space has a local vector-space structure, &lt;br /&gt; standard NGD in $\tau$ space is a speical case of our NGD,  &lt;br /&gt; where we choose &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\psi$&lt;/code&gt; to be the identity map and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\phi_{\lambda_t}$&lt;/code&gt; to be a linear map.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;gaussian-example-with-full-precision&quot;&gt;Gaussian Example with Full Precision&lt;/h1&gt;

&lt;p&gt;Notations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{GL}^{p\times p}$&lt;/code&gt;: Invertible Matrices (General Linear Group),&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{D}^{p\times p}$&lt;/code&gt;: Diagonal Matrices,&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{D}_{++}^{p\times p}$&lt;/code&gt;: Diagonal and invertible Matrices (Diagonal Matrix Group),&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{S}_{++}^{p\times p}$&lt;/code&gt;: (Symmetric) positive-definite Matrices,&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{S}^{p\times p}$&lt;/code&gt;: Symmetric Matrices.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Consider a Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\mu,\mathbf{S})$&lt;/code&gt; with mean $\mu$ and precision $\mathbf{S}=\Sigma^{-1}$.&lt;/p&gt;

&lt;p&gt;The global, auxiliary, and local parameterizations are:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
        \tau &amp;amp;= \Big\{\mu \in \mathcal{R}^p, \mathbf{S} \in \mathcal{S}_{++}^{p\times p} \Big\},  &amp;amp; \mathbf{S}: \text{positive-definite matrix space } \\
        \lambda &amp;amp; = \Big\{ \mu \in  \mathcal{R}^p , \mathbf{B} \in\mathrm{GL}^{p\times p} \Big\}, &amp;amp;\mathbf{B}: \text{ (closed) matrix Lie group}\\
        \eta &amp;amp;= \Big\{ \delta\in  \mathcal{R}^p, \mathbf{M} \in\mathcal{S}^{p\times p}  \Big\}, &amp;amp; \mathbf{M}: \text{ Lie sub-algebra }
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Define &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{h}(\mathbf{M}):=\mathbf{I}+\mathbf{M}+\frac{1}{2} \mathbf{M}^2$&lt;/code&gt;. 
Maps &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\psi$&lt;/code&gt; and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\phi_{\lambda_t}$&lt;/code&gt; are :
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
        \Big\{ \begin{array}{c} \mu \\ \mathbf{S} \end{array} \Big\} = \psi(\lambda) &amp;amp; := \Big \{ \begin{array}{c} \mu \\ \mathbf{B}\mathbf{B}^\top \end{array} \Big \}, \\
       \Big \{ \begin{array}{c} \mu \\ \mathbf{B} \end{array} \Big \} = \phi_{\lambda_t}(\eta) &amp;amp; := \Big \{ \begin{array}{c} \mu_t + \mathbf{B}_t^{-T} \delta \\ \mathbf{B}_t \mathbf{h} (\mathbf{M}) \end{array} \Big \}.
\end{aligned} \tag{3}\label{3}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Our NGD update in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  \lambda $&lt;/code&gt; space is shown below, where we assume $\eta_0=\mathbf{0}$.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
  \Big\{ \begin{array}{c} \mu_{t+1} \\ \mathbf{B}_{t+1} \end{array} \Big\} = \lambda_{t+1} = 
  \phi_{\lambda_t} \big( \eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} \big)
  =\Big\{ \begin{array}{c}  \mu_t - \beta \mathbf{B}_{t}^{-T} \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ \mathbf{B}_t  \mathbf{h}\big(\beta \mathbf{B}_t^{-1}\mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \end{array} \Big\} 
 \end{aligned}
$$&lt;/code&gt;
where &lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;tractable&lt;/strong&gt;&lt;/span&gt; natural-gradient  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\eta_0}^{(t)}$&lt;/code&gt;  at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0=\{\delta_0, \mathbf{M}_0\}$&lt;/code&gt; tied to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t=\{\mu_t,\mathbf{B}_t\}$&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
    \hat{\mathbf{g}}_{\eta_0}^{(t)} =
 \Big(  \begin{array}{c} \hat{\mathbf{g}}_{\delta_0}^{(t)}\\ \mathrm{vec}(  \hat{\mathbf{g}}_{M_0}^{(t)})\end{array}   \Big)
= \underbrace{  \color{green}{\Big(\begin{array}{cc} \mathbf{I}_p &amp;amp; 0 \\ 0 &amp;amp; 2 \mathbf{I}_{p^2} \end{array} \Big)^{-1}}  }_{ \text{inverse of exact FIM } } \Big[\begin{array}{c}  \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ \mathrm{vec}( -2\mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T}) \end{array} \Big]
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\mu$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_{\Sigma}$&lt;/code&gt; are Euclidean gradients of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; computed via Stein’s lemma &lt;a class=&quot;citation&quot; href=&quot;#lin2019stein&quot;&gt;[4]&lt;/a&gt; :
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\mu = \nabla_{\mu}\mathcal{L}(\tau) = E_{q}{ \big[ \nabla_w \ell( \mathbf{w} ) \big] }, \,\,\,\,\,
\mathbf{g}_{\Sigma}  = \nabla_{S^{-1}}\mathcal{L}(\tau)
 = \frac{1}{2}  E_{q}{ \big[ \nabla_w^2 \ell( \mathbf{w}) \big] } - \frac{\gamma}{2} \mathbf{S} 
 \end{aligned} \tag{4}\label{4}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Our update on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}_{t+1}=\mathbf{B}_{t+1}\mathbf{B}_{t+1}^T$&lt;/code&gt; and $\mu_{t+1}$ is like update of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
&amp;amp; \mu_{t+1}   = \mu_t - \beta \mathbf{S}_{t}^{-1} E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w \ell( \mathbf{w} ) \big] } \\
&amp;amp;\mathbf{S}_{t+1} =   \underbrace{ \overbrace{(1-\beta \gamma)\mathbf{S}_t + \beta  E_{q(w|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) \big] }}^{\text{standard NGD on $\mathbf{S}$ }} + \color{red}{ \frac{\beta^2}{2} \mathbf{G}_t \mathbf{S}_t^{-1}\mathbf{G}_t}
}_{\color{red}{\text{ RGD with retraction}}}+ O(\beta^3)
 \end{aligned}
$$&lt;/code&gt; where $\mathbf{B}$ is a &lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;dense&lt;/strong&gt;&lt;/span&gt; matrix in matrix group &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{GL}^{p\times p}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{G}_t := E_{q(w|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) ] } -\gamma \mathbf{S}_t$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The second-order term shown in red is used for the positive-definite constraint &lt;a class=&quot;citation&quot; href=&quot;#lin2020handling&quot;&gt;[5]&lt;/a&gt; known as a retraction in Riemannian gradient descent (RGD).  The higher-order term &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(\beta^3)$&lt;/code&gt; will be used for structured precision in the next section.&lt;/p&gt;

&lt;p&gt;Well-known structures in matrix $\mathbf{B}$ are illustrated in the following figure.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Dense&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Cholesky&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Diagonal&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/icml2021-group-full.png&quot; width=&quot;250&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/icml2021-group-tri.png&quot; width=&quot;250&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/icml2021-group-diag.png&quot; width=&quot;250&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;gaussian-example-with-flexiable-structured-precision&quot;&gt;Gaussian Example with Flexiable Structured Precision&lt;/h1&gt;

&lt;p&gt;More flexiable and sparse (group) structures in matrix $\mathbf{B}$:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Lower-triangular&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Upper-triangular&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Hierarchical&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Kronecker product&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Triangular-Toeplitz&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Sparse Cholesky&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/icml2021-group-low.png&quot; width=&quot;220&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/icml2021-group-up.png&quot; width=&quot;220&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/icml2021-group-hie.png&quot; width=&quot;220&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/icml2021-group-kro.png&quot; width=&quot;220&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/icml2021-group-tri-Toep.png&quot; width=&quot;220&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/icml2021-group-sparse.png&quot; width=&quot;220&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} \mathbf{B}_A &amp;amp;  \mathbf{0}  \\ \mathbf{B}_C &amp;amp;  \mathbf{B}_D  \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} \mathbf{B}_A &amp;amp;  \mathbf{B}_B  \\ \mathbf{0} &amp;amp;  \mathbf{B}_D  \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} \mathbf{B}_A &amp;amp;  \mathbf{0}  \\ \mathbf{B}_C &amp;amp;  \begin{bmatrix} \mathbf{B}_{D} &amp;amp; \mathbf{0} \\ \mathbf{B}_{3} &amp;amp; \mathbf{B}_{4} \end{bmatrix} \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} d &amp;amp;  0  \\ s &amp;amp;  t  \end{bmatrix} \otimes \begin{bmatrix} r &amp;amp;  0 &amp;amp; 0 \\ {b}_1 &amp;amp; {o}_1 &amp;amp; 0 \\ {b}_2 &amp;amp; 0 &amp;amp; {o}_2     \end{bmatrix} $&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} r &amp;amp; 0 &amp;amp; 0 &amp;amp;0 \\ y &amp;amp;  r &amp;amp; 0 &amp;amp; 0  \\ g &amp;amp; y &amp;amp; r &amp;amp; 0 \\ b &amp;amp; g &amp;amp; y &amp;amp; r \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} \mathbf{B}_{D_1} &amp;amp; \mathbf{0} &amp;amp; \mathbf{0} \\ \mathbf{B}_{A} &amp;amp; \mathbf{B}_{B} &amp;amp; \mathbf{0} \\ \mathbf{B}_{D_2} &amp;amp; \mathbf{0} &amp;amp; \mathbf{B}_{D_3} \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A structured Gaussian example:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Auxiliary  parameter $\mathbf{B}$ lives in  a structured space (matrix Lie group):  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{B}_{\text{up}}}(k)$&lt;/code&gt;,
a &lt;span style=&quot;color:red&quot;&gt;block upper-triangular&lt;/span&gt; sub-group of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{GL}^{p \times p}$&lt;/code&gt;;&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
{\cal{B}_{\text{up}}}(k)  := \Big\{ 
\begin{bmatrix}
\mathbf{B}_A &amp;amp;  \mathbf{B}_B  \\
 \mathbf{0} &amp;amp; \mathbf{B}_D
      \end{bmatrix} \Big| &amp;amp; \mathbf{B}_A \in \mathrm{GL}^{k \times k},\,
 \mathbf{B}_D  \in{\cal D}^{(p-k) \times (p-k)}_{++}  \Big\},\,\,
 \end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$k=0$&lt;/code&gt;, the space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{B}_{\text{up}}}(0) = {\cal D}^{p \times p}_{++}$&lt;/code&gt; becomes  the diagonal case.
When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$k=p$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{B}_{\text{up}}}(p) = \mathrm{GL}^{p\times p}$&lt;/code&gt; becomes the dense case.&lt;/p&gt;

  &lt;p&gt;Consider a local parameter space (Lie sub-algebra): &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{M}_{\text{up}}}(k)$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
{\cal{M}_{\text{up}}}(k):  = \Big\{ 
\begin{bmatrix}
\mathbf{M}_A &amp;amp;  \mathbf{M}_B  \\
 \mathbf{0} &amp;amp; \mathbf{M}_D
      \end{bmatrix} \Big| &amp;amp;  \mathbf{M}_A \in{\cal S}^{k \times k}, \,
 \mathbf{M}_D  \in{\cal D}^{(p-k) \times (p-k)} \Big\}
 \end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;The global, auxiliary, and local parameterizations :
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
       \tau &amp;amp;= \Big\{\mu \in \mathcal{R}^p,  \mathbf{S}=\mathbf{B} \mathbf{B}^T \in \mathcal{S}_{++}^{p\times p} | \mathbf{B} \in {\cal{B}_{\text{up}}}(k) \Big\}, \\
        \lambda &amp;amp; = \Big\{ \mu \in \mathcal{R}^p,  \mathbf{B} \in {\cal{B}_{\text{up}}}(k) \Big\},\\
        \eta &amp;amp;= \Big\{ \delta\in \mathcal{R}^p,  \mathbf{M} \in {\cal{M}_{\text{up}}}(k)   \Big\}.
 \end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;Maps &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\psi$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\phi_{\lambda_t}$&lt;/code&gt; are defined in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt;.
Our NGD update in the auxiliary space is shown below, where we assume $\eta_0=\mathbf{0}$.&lt;/p&gt;
  &lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;legend class=&quot;leg-title&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;Structure-preserving update in $\lambda$ space&lt;/span&gt;&lt;/legend&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
  \Big\{ \begin{array}{c} \mu_{t+1} \\ \mathbf{B}_{t+1} \end{array} \Big\} = \lambda_{t+1} = 
  \phi_{\lambda_t} \big( \eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} \big)
  =\Big\{ \begin{array}{c}  \mu_t - \beta \mathbf{B}_{t}^{-T} \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ 
\mathbf{B}_t  \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \kappa_{\text{up}}\big( 2 \mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \Big)  \end{array} \Big\}   
 \end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
  &lt;/fieldset&gt;
  &lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\odot$&lt;/code&gt; is the elementwise product ,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\kappa_{\text{up}}(\mathbf{X}) \in {\cal{M}_{\text{up}}}(k)$&lt;/code&gt; extracts non-zero entries of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{M}_{\text{up}}}(k)$&lt;/code&gt; from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{X}$&lt;/code&gt;, and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{C}_{\text{up}} = 
 \begin{bmatrix}
\frac{1}{2} \mathbb{1} &amp;amp;  \mathbb{1}   \\
 \mathbf{0} &amp;amp; \frac{1}{2} \mathbf{I}_D
      \end{bmatrix}  \in {\cal{M}_{\text{up}}}(k)$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{B}_{t+1} \in {\cal{B}_{\text{up}}}(k)$&lt;/code&gt; thanks to the property of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\cal{M}_{\text{up}}(k)$&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{h}\big(\mathbf{M}\big) \in {\cal{B}_{\text{up}}}(k) $&lt;/code&gt; for any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{M} \in \cal{M}_{\text{up}}(k)$&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In summary, our NGD method:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;is a systemic approach to incorporate structures&lt;/li&gt;
  &lt;li&gt;induces exact and non-singular FIMs&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Existing NG approach for rank-one covariance&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Our NGD for rank-one covariance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/icml2021-fig02.png&quot; width=&quot;500&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/icml2021-fig04.png&quot; width=&quot;500&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;applications&quot;&gt;Applications&lt;/h1&gt;

&lt;h2 id=&quot;structured-2nd-order-methods-for-non-convex-optimization&quot;&gt;Structured 2nd-order Methods for Non-convex Optimization&lt;/h2&gt;

&lt;p&gt;Given an optimization problem
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\mu \in \mathcal{R}^p} \ell(\mu),
 \end{aligned}\tag{5}\label{5}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;we formulate a new problem over Gaussian &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(\mathbf{w}|\tau)$&lt;/code&gt; with structured precision, which is a speical case of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma=1$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
   \min_{\tau \in \Omega_\tau} E_{q(w|\tau)} \big[ \ell(\mathbf{w}) \big] +  E_{q(w|\tau)} \big[ \log q(\mathbf{w}|\tau)\big],
 \end{aligned}\tag{6}\label{6}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B} \in {\cal{B}_{\text{up}}}(k)$&lt;/code&gt; is a block upper-triangular group member,  $\tau=(\mu,\mathbf{S})$ with mean $\mu$ and precision matrix $\mathbf{S}=\mathbf{B}\mathbf{B}^T$.&lt;/p&gt;

&lt;p&gt;Using our NGD to solve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;gives the following update
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mu_{t+1}  &amp;amp; \leftarrow \mu_{t} - \beta \mathbf{S}_t^{-1} \mathbf{g}_{\mu_t},\\
\mathbf{B}_{t+1}  &amp;amp; \leftarrow   \mathbf{B}_t \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \kappa_{\text{up}}\big( 2 \mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \Big)
 \end{aligned}
$$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;obtains an update to solve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt; with group-structural invariance:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \mu_{t+1} &amp;amp;  \leftarrow  \mu_t - \beta \mathbf{S}_{t}^{-1} \nabla_{\mu_t} \ell( \mu), \\ 
\mathbf{B}_{t+1} &amp;amp; \leftarrow 
   \mathbf{B}_t \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \color{red}{\kappa_{\text{up}}\big(  \mathbf{B}_t^{-1} \nabla_{\mu_t}^2 \ell( \mu)  \mathbf{B}_t^{-T} - \mathbf{I} \big)} \Big)
 \end{aligned}\tag{7}\label{7}
$$&lt;/code&gt; by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; evaluated at the mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu_t$&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_{\mu_t} \approx \nabla_{\mu_t} \ell( \mu),\,\,\,\,
 \mathbf{g}_{\Sigma_t} \approx \frac{1}{2} \big[ \nabla_{\mu_t}^2 \ell( \mu) - \mathbf{S}_t\big].
 \end{aligned}\tag{8}\label{8}
$$&lt;/code&gt; where $\Sigma=\mathbf{S}^{-1}$ is the covariance.&lt;/li&gt;
&lt;/ul&gt;

&lt;details&gt;
	&lt;summary&gt;Group-structural invariance:&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;p&gt;Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B} \in {\cal{B}_{\text{up}}}(k)$&lt;/code&gt;,  the update in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; is invariant under any (group) transform &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{R}^T \in  {\cal{B}_{\text{up}}}(k)$&lt;/code&gt; of  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt;  such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\min_{y \in \mathcal{R}^p}  \ell(\mathbf{R} \, \mathbf{y})$&lt;/code&gt;.&lt;/p&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;details&gt;
	&lt;summary&gt;Time complexity:&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(k^2 p)$&lt;/code&gt; for triangular structure,&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O( (k_1^2+k_2^2) p)$&lt;/code&gt; for hierarchical structure.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Implementation using Hessian-vector products (HVPs);&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Off-diagonal: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$k$&lt;/code&gt; HVPs (triangular), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(k_1+k_2)$&lt;/code&gt; HVPs (hierarchical),&lt;/li&gt;
      &lt;li&gt;Diagonal: compute/approximate diagonal entries of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_{\mu_t}^2 \ell( \mu)$&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;details&gt;
	&lt;summary&gt;Classical non-convex optimization:&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;p&gt;200-dim non-separable non-convex functions:&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/icml2021-rbfun.png&quot; width=&quot;500&quot; /&gt; |  &lt;img src=&quot;/img/icml2021-dpfun.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;h2 id=&quot;structured-adaptive-gradient-methods-for-deep-learning&quot;&gt;Structured Adaptive-gradient Methods for Deep Learning&lt;/h2&gt;
&lt;p&gt;At each NN layer,
consider a  Gaussian family
       &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(\mathbf{w}|\mu,\mathbf{S})$&lt;/code&gt; with a Kronecker product structure, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=(\mu,\mathbf{S})$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Our method gives adaptive-gradient updates with group-structural invariance by
 approximating &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_{\mu_t}^2 \ell( \mu)$&lt;/code&gt;  in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{8}$&lt;/code&gt; using the Gauss-Newton.&lt;/p&gt;

&lt;p&gt;The Kronecker product (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}=\mathbf{B}_1 \otimes \mathbf{B}_2$&lt;/code&gt;) of two sparse structured groups (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}
_1$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}_2$&lt;/code&gt;) further reduces the time complexity, where precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}=\mathbf{B}\mathbf{B}^T= (\mathbf{B}_1 \mathbf{B}_1^T) \otimes (\mathbf{B}_2 \mathbf{B}_2^T)$&lt;/code&gt;&lt;/p&gt;

&lt;details&gt;
	&lt;summary&gt;Time complexity:&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(k p)$&lt;/code&gt; for our Kronecker product with triangular groups, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$0&amp;lt;k&amp;lt;p^{1/2}$&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O( (k_1+k_2) p)$&lt;/code&gt; for our  Kronecker product with hierarchical groups, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$0&amp;lt;k_1+k_2&amp;lt;p^{1/2}$&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(p)$&lt;/code&gt; for Adam and our diagonal groups&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(p^{3/2})$&lt;/code&gt; for KFAC and our Kronecker product with dense groups&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Implementation:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Automatically parallelized by Auto-Differentiation&lt;/li&gt;
      &lt;li&gt;No sequential conjugate-gradient (CG) steps&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;details&gt;
	&lt;summary&gt;Image classification problems:&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;p&gt;Kronecker product of lower-triangular groups for a CNN&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/icml2021-stl10.png&quot; width=&quot;500&quot; /&gt; |  &lt;img src=&quot;/img/icml2021-cifar10.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;h2 id=&quot;variational-inference-with-gaussian-mixtures&quot;&gt;Variational Inference with Gaussian Mixtures&lt;/h2&gt;

&lt;p&gt;Our NGD&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;can use structured Gaussian mixtures as flexiable variational distributions: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(\mathbf{w}|\tau)=\frac{1}{K}\sum_{k=1}^{K}q(\mathbf{w}|\mu_k,\mathbf{S}_k)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;gives efficient stochastic natural-gradient variational methods beyond mean-field/diagonal covariance&lt;/li&gt;
&lt;/ul&gt;

&lt;details&gt;
	&lt;summary&gt;Approximating multimodal distributions (80-dim mixture of Student's Ts):&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;p&gt;First 8 marginal distributions of our approximation (mixture with a block upper-triangular group, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$k=5$&lt;/code&gt;)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/icml2021-tmm80d-01.png&quot; width=&quot;500&quot; /&gt; |  &lt;img src=&quot;/img/icml2021-tmm80d-02.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;
  &lt;/fieldset&gt;
&lt;/details&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p class=&quot;bibliography&quot;&gt;&lt;p&gt;&lt;span id=&quot;lin2021tractable&quot;&gt;[1] W. Lin, F. Nielsen, M. E. Khan, &amp;amp; M. Schmidt, &quot;Tractable structured natural gradient descent using local parameterizations,&quot; &lt;i&gt;International Conference on Machine Learning (ICML)&lt;/i&gt; (2021).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;glasmachers2010exponential&quot;&gt;[2] T. Glasmachers, T. Schaul, S. Yi, D. Wierstra, &amp;amp; J. Schmidhuber, &quot;Exponential natural evolution strategies,&quot; &lt;i&gt;Proceedings of the 12th annual conference on Genetic and evolutionary computation&lt;/i&gt; (2010), pp. 393–400.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2021structured&quot;&gt;[3] W. Lin, F. Nielsen, M. E. Khan, &amp;amp; M. Schmidt, &quot;Structured second-order methods via natural gradient descent,&quot; &lt;i&gt;arXiv preprint arXiv:2107.10884&lt;/i&gt; (2021).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2019stein&quot;&gt;[4] W. Lin, M. E. Khan, &amp;amp; M. Schmidt, &quot;Stein’s Lemma for the Reparameterization Trick with Exponential Family Mixtures,&quot; &lt;i&gt;arXiv preprint arXiv:1910.13398&lt;/i&gt; (2019).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2020handling&quot;&gt;[5] W. Lin, M. Schmidt, &amp;amp; M. E. Khan, &quot;Handling the positive-definite constraint in the bayesian learning rule,&quot; &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (PMLR, 2020), pp. 6116–6126.&lt;/span&gt;&lt;/p&gt;&lt;/p&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Matrix Lie Groups" /><summary type="html">More about this work [1]: (Youtube) talk, ICML paper, workshop paper, poster</summary></entry></feed>