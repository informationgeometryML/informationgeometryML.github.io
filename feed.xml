<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-02-15T03:12:35-08:00</updated><id>/feed.xml</id><title type="html">Information Geometry in Machine Learning</title><subtitle>Blog website for Information Geometry in ML.</subtitle><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><entry><title type="html">Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods</title><link href="/posts/2021/12/Geomopt06/" rel="alternate" type="text/html" title="Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods" /><published>2021-12-22T00:00:00-08:00</published><updated>2021-12-22T00:00:00-08:00</updated><id>/posts/2021/12/Geomopt06</id><content type="html" xml:base="/posts/2021/12/Geomopt06/">&lt;p&gt;Warning: working in Progress (incomplete)&lt;/p&gt;

&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;In this blog post, we discuss about how to handle parameter constraints of exponential family.&lt;/p&gt;

&lt;h1 id=&quot;handling-parameter-constraints&quot;&gt;Handling Parameter Constraints&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;Recall that  in Part IV, we discuss 
&lt;a href=&quot;/posts/2021/11/Geomopt04/#many-faces-of-natural-gradient-descent&quot;&gt;the many faces of NGD&lt;/a&gt; in unconstrained cases.  These methods could also be exteneded in constrained cases to handle the parameter constraint.&lt;/p&gt;

&lt;p&gt;Given a natural parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; with parameter constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;, consider the following problem
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\eta \in \Omega_\eta} \ell_\eta(\mathbf{\eta}) = E_{p(w|\eta)} [h(w) + \log p(w|\eta)]
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We could also consider the reparameterized problem by using the expectation parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{m \in \Omega_m} \ell_m(\mathbf{m}) = E_{p(w| \eta(m) )} [h(w) + \log p(w|\eta(\mathbf{m}))] = \ell_\eta(\eta(\mathbf{m}))
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;Recall that updates in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt; are
equivalent in exponential family cases even when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; is constrained as long as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is unconstrained.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is constrained, updates in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; are &lt;strong&gt;distinct&lt;/strong&gt; methods.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Since both &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;  can be arbitrary open subsets in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt; in general,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; can be difficult to solve.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;h2 id=&quot;projected-natural-gradient-descent&quot;&gt;Projected Natural Gradient Descent&lt;/h2&gt;
&lt;p&gt;A straightforward approach from natural-gradient descent is the projected natural-gradient descent, where we  use 
the FIM &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_\eta(\eta_k)$&lt;/code&gt; evaluted at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_k$&lt;/code&gt; as a projection metric and use the &lt;a href=&quot;/posts/2021/10/Geomopt02/
#distance-induced-by-the-fisher-rao-metric&quot;&gt;weighted inner product&lt;/a&gt; to measure the distance for the projection.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
&amp;amp; \eta_{k+1}  \leftarrow  \arg\min_{ \color{red} {y} \in \Omega_\eta} \| \color{red} {\mathbf{y}} - \eta_k + \alpha
\mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta \ell_\eta(\eta_k) \|^2_{ \color{green}{ \mathbf{F}_\eta(\eta_k)} }\\
=&amp;amp; \arg\min_{ y \in \Omega_\eta} \big[ (\mathbf{y}-\eta_k) + \alpha\mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta \ell_\eta(\eta_k)
\big]^T \mathbf{F}_{\eta}(\eta_k) \big[ (\mathbf{y}-\eta_k) + \alpha\mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta \ell_\eta(\eta_k)
\big]\\
=&amp;amp; \arg\min_{ y \in \Omega_\eta} 2\alpha \big[ \frac{1}{2\alpha} (\mathbf{y}-\eta_k)^T \mathbf{F}_{\eta}(\eta_k) (\mathbf{y}-\eta_k) + (\mathbf{y}-\eta_k)^T  \nabla_\eta \ell_\eta(\eta_k) + \underbrace{ \frac{\alpha}{2} \nabla_\eta^T \ell_\eta(\eta_k) \mathbf{F}^{-1}_\eta(\eta_k) \nabla_\eta \ell_\eta(\eta_k)}_{\text{constant w.r.t. } y} \big] \\
=&amp;amp; \arg\min_{\color{red}{y} \in  \Omega_\eta  } \{ \langle \nabla_\eta \ell_\eta(\eta_k),\color{red}{\mathbf{y}}-\eta_k \rangle   + \frac{1}{2\alpha} (\mathbf{y}-\eta_k)^T \mathbf{F}_{\eta}(\eta_k) (\mathbf{y}-\eta_k) \} 
\end{aligned}\tag{1}\label{1}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This approach is closely related to proximial-gradient descent.
Recall that in
&lt;a href=&quot;/posts/2021/11/Geomopt04/#natural-gradient-descent-as-unconstrained-proximal-gradient-descent&quot;&gt;Part IV&lt;/a&gt;,
we show that natural-gradient descent can be viewed as an  proximal-gradient method, where we use the
second-order Taylor  approximation of any f-divergence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{D}_f(\mathbf{y},\eta_k)$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$y=\eta_k$&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{D}_f(\mathbf{y},\eta_k) \approx \frac{1}{2} (\mathbf{y}-\eta_k)^T \mathbf{F}_{\eta}(\eta_k) (\mathbf{y}-\eta_k)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;proximal-gradient-descent&quot;&gt;Proximal Gradient Descent&lt;/h2&gt;

&lt;p&gt;We could also obtain proximal gradient descent by using a f-divergence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{D}_f(\mathbf{y},\eta_k)$&lt;/code&gt; without the Taylor approximation.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\eta_{k+1} \leftarrow \arg\min_{\color{red}{y} \in  \Omega_\eta  } \{ \langle \nabla_\eta \ell_\eta(\eta_k),\color{red}{\mathbf{y}}-\eta_k \rangle   + \frac{1}{\alpha} \mathrm{D}_f(\color{red}{ \mathbf{y}},\eta_k)  \} 
\end{aligned}\tag{2}\label{2}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We have the following additional results, when the f-divergence is chosen to be a KL divergence.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The KL divergence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\color{red}{\mathbf{y}})]=\mathrm{D}_f(\color{red}{\mathbf{y}},\eta_k)=\mathrm{B}_{A_\eta}(\eta_k,\color{red}{\mathbf{y}})$&lt;/code&gt; is a f-divergence and a Bregman divergence. The
second-order Taylor approximation   at 
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{y}=\eta_k$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\color{red}{\mathbf{y}})] \approx \frac{1}{2} (\mathbf{y}-\eta_k)^T \mathbf{F}_{\eta}(\eta_k) (\mathbf{y}-\eta_k)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The second-order Taylor approximation  of the &lt;strong&gt;reverse&lt;/strong&gt; KL divergence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{KL} [p(\mathbf{w}|\color{red} {\mathbf{y}} ) || p(\mathbf{w}|\eta_k)]$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{y}=\eta_k$&lt;/code&gt; gives the same approximation:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{KL} [p(\mathbf{w}|\color{red}{\mathbf{y}}) || p(\mathbf{w}|\eta_k)] \approx \frac{1}{2} (\mathbf{y}-\eta_k)^T \mathbf{F}_{\eta}(\eta_k) (\mathbf{y}-\eta_k)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;The KL divergence is the only divergence &lt;a class=&quot;citation&quot; href=&quot;#amari2016information&quot;&gt;[1]&lt;/a&gt; that is both a  f-divergence and a Bregman divergence.&lt;/p&gt;

  &lt;p&gt;We have the following identity for the KL divergence.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\color{red}{\mathbf{y}})] = 
\mathrm{D}_f(\color{red}{\mathbf{y}},\eta_k) = \mathrm{B}_{A_\eta}(\eta_k,\color{red}{\mathbf{y}}) =  \mathrm{B}_{A^*_\eta}(\color{red}{\mathbf{x}},\mathbf{m}_k)
= \mathrm{KL} [p(\mathbf{w}|\mathbf{m}_k) || p(\mathbf{w}|\color{red}{\mathbf{x}})] 
\end{aligned}
$$&lt;/code&gt;  where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{y}$&lt;/code&gt; is a natural parameter and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}$&lt;/code&gt; is the corresponding expectation parameter.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;mirror-descent&quot;&gt;Mirror Descent&lt;/h2&gt;
&lt;p&gt;Mirror descent in the expectation space remains the same as in &lt;a href=&quot;/posts/2021/12/Geomopt05/#natural-gradient-descent-as-mirror-descent&quot;&gt;Part V&lt;/a&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{ \color{red} {x} \in \Omega_m}\{ \langle \nabla_m \ell_m(\mathbf{m}_k), \color{red}{\mathbf{x}}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\color{red}{\mathbf{x}},\mathbf{m}_k) \}
\end{aligned}\tag{3}\label{3}
$$&lt;/code&gt;
where 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_m \ell_m(\mathbf{m}_k) = \nabla_m \ell_\eta( \underbrace{ \eta(\mathbf{m}_k)}_{=\eta_k} )=  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta \ell_\eta(\eta_k)$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We could also perform  mirror descent in the natural parameter space as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{\eta}_{k+1} \leftarrow \arg \min_{\color{red}{y} \in \Omega_\eta}\{ \langle \nabla_\eta \ell_\eta(\mathbf{\eta}_k), \color{red}{\mathbf{y}}-\mathbf{\eta}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A_\eta}(\color{red}{\mathbf{y}},\mathbf{\eta}_k) \}
\end{aligned}\tag{4}\label{4}
$$&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;Without the Taylor approximation,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; are distinct updates since the KL divergence is not symmetric.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;adaptive-step-size-selection&quot;&gt;Adaptive Step-size Selection&lt;/h2&gt;
&lt;p&gt;Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is an open set in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, the standard natural-gradient descent is still valid when a step-size is small enough.&lt;/p&gt;

&lt;p&gt;One idea is to use an adaptive step-size for natural-gradient descent without a projection.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha_k \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta \ell_\eta(\eta_k)
\end{aligned}\tag{5}\label{5}
$$&lt;/code&gt; where  the step-size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\alpha_k$&lt;/code&gt; is selected  so that
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_{k+1} \in \Omega_\eta$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;However, for a general parameter constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;, this approach could result in a slow progression of the method.
The step-size selection precedure has to  check the constraint at each iteration and could select an extremely small step-size
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\alpha_k$&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;riemannian-gradient-descent&quot;&gt;Riemannian Gradient Descent&lt;/h2&gt;

&lt;p&gt;An alternative approach is to use Riemannian gradient descent as we discussed in 
&lt;a href=&quot;/posts/2021/11/Geomopt04/#riemannian-gradient-descent-and-its-non-linear-invariance&quot;&gt;Part IV&lt;/a&gt;, which is a generalization of natural-gradient descent. 
Note that this approach is completely different from mirror descent.&lt;/p&gt;

&lt;p&gt;To avoid solving the geodeisc ODE, we could use an approximation of the geodesic, which
induces a retraction map.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\eta_{k+1} \leftarrow \mathrm{Ret}_{\eta_k} (- \alpha  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta \ell_\eta(\eta_k) )  
\end{aligned}\tag{6}\label{6}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;As mentioned in 
&lt;a href=&quot;/posts/2021/11/Geomopt04/#natural-gradient-descent-as-inexact-riemannian-gradient-descent&quot;&gt;Part IV&lt;/a&gt;,
we have to carefully select a retraction map to handle the parameter constraint.
Given 
a general parameter constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;, it can be difficult to come out an efficient retraction map to satisfy
the constraint.&lt;/p&gt;

&lt;p&gt;For positive-definite constraints in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;, please see &lt;a class=&quot;citation&quot; href=&quot;#lin2020handling&quot;&gt;[2]&lt;/a&gt; as an example to derive efficient Riemannian gradient updates.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p class=&quot;bibliography&quot;&gt;&lt;p&gt;&lt;span id=&quot;amari2016information&quot;&gt;[1] S.-ichi Amari, &lt;i&gt;Information geometry and its applications&lt;/i&gt; (Springer, 2016).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2020handling&quot;&gt;[2] W. Lin, M. Schmidt, &amp;amp; M. E. Khan, &quot;Handling the positive-definite constraint in the bayesian learning rule,&quot; &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (PMLR, 2020), pp. 6116–6126.&lt;/span&gt;&lt;/p&gt;&lt;/p&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><category term="Exponential Family" /><summary type="html">Warning: working in Progress (incomplete)</summary></entry><entry><title type="html">Part V: Efficient Natural-gradient Methods for Exponential Family</title><link href="/posts/2021/12/Geomopt05/" rel="alternate" type="text/html" title="Part V: Efficient Natural-gradient Methods for Exponential Family" /><published>2021-12-14T00:00:00-08:00</published><updated>2021-12-14T00:00:00-08:00</updated><id>/posts/2021/12/Geomopt05</id><content type="html" xml:base="/posts/2021/12/Geomopt05/">&lt;p&gt;Warning: working in Progress (incomplete)&lt;/p&gt;

&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post should show that we can efficiently implement natural-gradient methods in many cases.&lt;/p&gt;

&lt;p&gt;We will give an informal introduction with a focus on high level of ideas.&lt;/p&gt;

&lt;h1 id=&quot;exponential-family&quot;&gt;Exponential Family&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;An exponential family takes the following (canonical) form as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p(\mathbf{w}|\mathbf{\eta}) = h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta (\mathbf{\eta}) )
\end{aligned}
$$&lt;/code&gt; where   &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$C_\eta(\eta) :=  \int h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}(\mathbf{w}) \rangle ) d \mathbf{w}
$&lt;/code&gt; is the normalization constant.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;name&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\mathbf{\eta}):=\log C_\eta(\eta)$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;log-partition function&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\eta(\mathbf{w})$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;base measure&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{T}_\eta(\mathbf{w})$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sufficient statistics&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;natural parameter&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The parameter space of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is determined so that the normalization constant is well-defined and (strictly and finitely) positive.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Regular&lt;/strong&gt; natural parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;: parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is relatively open.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In this post, we only consider
regular natural parametrizations since commonly used exponential family distributions have a regular natural parametrization.
This natural parametrization is special since the inner product &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$&lt;/code&gt; is &lt;strong&gt;linear&lt;/strong&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;. As we will discuss later,  this linearity is essential.&lt;/p&gt;

&lt;p&gt;Examples of an exmponential family are Gaussian, Bernoulli, Von Mises–Fisher, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions&quot;&gt;more&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Readers should be aware of the following points when using an exponential family.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The support of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt; should not depend on parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The base measure and the log-partition function are only unique up to a constant as illustrated by the following example.
    &lt;div class=&quot;notice--info&quot;&gt;
      &lt;details&gt;
  &lt;summary&gt;Univariate Gaussian as an exponential family (click to expand)&lt;/summary&gt;
  &lt;fieldset class=&quot;field-set&quot;&gt;
          &lt;blockquote&gt;

            &lt;p&gt;Recall that in &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families&quot;&gt;Part I&lt;/a&gt;, we consider this family as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&amp;gt;0 \}$&lt;/code&gt; with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt; and variance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sigma$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $&lt;/code&gt;.&lt;/p&gt;

            &lt;p&gt;We re-express it in an exponential form as&lt;/p&gt;

            &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\eta})  &amp;amp;= \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] \\
&amp;amp;= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2\sigma} \\ \frac{\mu}{\sigma}  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} w^2 \\ w  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ]     )   \\
\end{aligned}
$$&lt;/code&gt;
Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sigma= -\frac{1}{2\eta_1} $&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu = -\frac{\eta_2}{2\eta_1}$&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\mathbf{\eta}) = \frac{1} {2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ] = \frac{1}{2} [ \log ( 2\pi ) + \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $&lt;/code&gt;.&lt;/p&gt;

            &lt;p&gt;It is also valid that $ h_\eta({w}) = \frac{1}{\sqrt{2\pi}} $ and 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\mathbf{\eta}) =  \frac{1}{2} [  \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $&lt;/code&gt;.&lt;/p&gt;

            &lt;p&gt;We easily to verify that parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta= \{ (\eta_1,\eta_2) | \eta_1&amp;lt;0 , \eta_2 \in \mathcal{R} \}$&lt;/code&gt; is open in $\mathcal{R}^2$.&lt;/p&gt;
          &lt;/blockquote&gt;
        &lt;/fieldset&gt;
  &lt;/details&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;The log-partition function can be differentiable w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; &lt;a class=&quot;citation&quot; href=&quot;#johansen1979introduction&quot;&gt;[1]&lt;/a&gt; even when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt; is discrete.
    &lt;div class=&quot;notice--info&quot;&gt;
      &lt;details&gt;
  &lt;summary&gt;Bernoulli as an exponential family (click to expand)&lt;/summary&gt;
  &lt;fieldset class=&quot;field-set&quot;&gt;
          &lt;blockquote&gt;

            &lt;p&gt;Recall that in &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families&quot;&gt;Part I&lt;/a&gt;, we consider this family as
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \Big| 0&amp;lt;\pi&amp;lt;1 \}$&lt;/code&gt;&lt;/p&gt;

            &lt;p&gt;We re-express it in an exponential form as&lt;/p&gt;

            &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;amp;=  \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \\
&amp;amp;=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \log \frac{\pi}{1-\pi}}_{\eta} , \underbrace{ \mathcal{I}(w=0)}_{T_\eta(w) } \rangle - \log \frac{1}{1-\pi} )
\end{aligned}
$$&lt;/code&gt;
Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi = \frac{\exp(\eta)}{1+ \exp(\eta) } $&lt;/code&gt; , we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\mathbf{\eta}) =  \log \frac{1}{1-\pi} = \log(1+\exp(\eta))$&lt;/code&gt;. &lt;br /&gt;
We easily to verify that parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta= \{ \eta | \eta \in \mathcal{R} \}$&lt;/code&gt; is open in $\mathcal{R}$.&lt;/p&gt;
          &lt;/blockquote&gt;
        &lt;/fieldset&gt;
  &lt;/details&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;An invertiable linear reparametrization could also be a natural parametrization. This is one of the reasons using natural-gradient descent since it is linearly invariant.
    &lt;div class=&quot;notice--info&quot;&gt;
      &lt;details&gt;
  &lt;summary&gt;Natural parametrization is not unique (click to expand)&lt;/summary&gt;
  &lt;fieldset class=&quot;field-set&quot;&gt;
          &lt;blockquote&gt;

            &lt;p&gt;For simplicity, let’s assume set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\lambda := \{ \mathbf{U}^{-1} \eta | \eta \in \Omega_\eta \} = \Omega_\eta$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{U}$&lt;/code&gt; is a constant invertiable matrix.&lt;/p&gt;

            &lt;p&gt;Consider a linear reparametrization such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda=\mathbf{U}^{-1} \mathbf{\eta}$&lt;/code&gt;, parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; is also a natural parametrization as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p(\mathbf{w}|\mathbf{\lambda})
&amp;amp;= h_\eta(\mathbf{w}) \exp( \langle \mathbf{U}\mathbf{\lambda} , \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta( \mathbf{U}\mathbf{\lambda}) ) \\
&amp;amp;=  h_\eta(\mathbf{w})\exp( \langle \mathbf{\lambda} , \mathbf{U}^T \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta(\mathbf{U}\mathbf{\lambda}) ) \\
&amp;amp;= h_\lambda(\mathbf{w})  \exp( \langle \mathbf{\lambda} ,  \mathbf{T}_\lambda(\mathbf{w}) \rangle - A_\lambda(\mathbf{\lambda}) ) 
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\lambda(\mathbf{w}):= h_\eta(\mathbf{w})$&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{T}_\lambda(\mathbf{w}):= \mathbf{U}^T\mathbf{T}_\eta(\mathbf{w})$&lt;/code&gt;, and 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\lambda(\mathbf{\lambda}):= A_\eta(\mathbf{U}\mathbf{\lambda})$&lt;/code&gt;.&lt;/p&gt;
          &lt;/blockquote&gt;
        &lt;/fieldset&gt;
  &lt;/details&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;minimal-parametrizations-of-exponential-family&quot;&gt;Minimal Parametrizations of Exponential Family&lt;/h2&gt;

&lt;p&gt;Now, we discuss particular parametrizations of an exponential family. We could efficiently compute natural-gradients when
using this class of parametrizations.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Minimal&lt;/strong&gt; natural parametrization: the corresponding sufficient stattistics &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{T}(\mathbf{w})$&lt;/code&gt; is linearly independent.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;A regular, minimal, and natural parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; has many nice properties &lt;a class=&quot;citation&quot; href=&quot;#johansen1979introduction&quot;&gt;[1]&lt;/a&gt; &lt;a class=&quot;citation&quot; href=&quot;#wainwright2008graphical&quot;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It is an intrinsic parametrization as we discussed in
 &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families&quot;&gt;Part I&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is an open set in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, where
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt; is the number of entires of this parameter array.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The log-partition function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\eta)$&lt;/code&gt; is infinitely differentiable and strictly convex in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The FIM &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_\eta(\eta) = \nabla_\eta^2 A_\eta(\eta)$&lt;/code&gt; is positive-definite in its domain.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will only show the first property in this post. The remaining properties can be found in the literature.
Note that the linearity in the inner product &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$&lt;/code&gt;,  plays a key role in showing these properties.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Claim&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;A regular, minimal, and natural parametrization is intrinsic.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Proof of the claim (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;
        &lt;p&gt;Proof by contradiction:&lt;/p&gt;

        &lt;p&gt;Recall that a parametrization is intrinsic if 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt; partial derivatives 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \partial_{\eta_i} \log p(\mathbf{w}|\eta) \} $&lt;/code&gt;  are linearly independent.
Since the parameter space is open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt; and the log-partition function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\eta)$&lt;/code&gt; is differentiable,  these partial derivatives are well-defined and can be computed as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\eta_i} \log p(\mathbf{w}|\eta) = \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle - \partial_{\eta_i}A_\eta(\eta) 
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{e}_i$&lt;/code&gt; is an one-hot/unit array which has zero in all entries except the $i$-th entry.&lt;/p&gt;

        &lt;p&gt;Suppose a regular, minimal, and natural parametrization is not intrinsic.
These partial derivatives must be linearly dependent.
Thus, there exist a set of non-zero constant &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_i$&lt;/code&gt; such that
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sum_i c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta)= 0 $&lt;/code&gt;, where the value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_i$&lt;/code&gt; does not depent on  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
0 &amp;amp;= \sum_{i=1}^{K} c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta) \\
&amp;amp;= \sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle -   c_i\partial_{\eta_i}A_\eta(\eta) \\
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;Since  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_i$&lt;/code&gt; is a non-zero constant and its value does not depent on  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt;, we must have
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
0 =\sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle ,
\end{aligned}
$$&lt;/code&gt; which implies that
the sufficient stattistics &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{T}(\mathbf{w})$&lt;/code&gt; is linearly dependent. This is a contradiction since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is a minimal natural parametrization.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;p&gt;Now, we give an example of a regular, minimal and natural parametrization.&lt;/p&gt;
&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Minimal parametrization for multivariate Gaussian (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;

        &lt;p&gt;Consider a $d$-dimensional Gaussian family.&lt;/p&gt;

        &lt;p&gt;We specify a parameterization $\mathbf{\tau}$ of the  family as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(\mathbf{w} |\mathbf{\mu},\mathbf{\Sigma}) \Big| \mathbf{\mu} \in \mathcal{R}^d, \mathbf{\Sigma}   \succ \mathbf{0} \}$&lt;/code&gt; with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\mu}$&lt;/code&gt; and covariance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\Sigma}$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;We re-express it in an exponential form as&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\lambda})  &amp;amp;= \frac{1}{\sqrt{ \mathrm{det}( 2\pi \Sigma )} } \exp [- \frac{1}{2} (\mathbf{w}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{w}-\mathbf{\mu})  ] \\
&amp;amp;= \underbrace{ \exp(0) }_{  h_\lambda({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \mathrm{vec}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\lambda} }  ,  \underbrace{\begin{bmatrix} \mathrm{vec}( \mathbf{w} \mathbf{w}^T) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\lambda ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vec}()$&lt;/code&gt; is the vectorization function
and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; is a $(d+d^2)$-dim array.&lt;/p&gt;

        &lt;p&gt;Parametrization $\lambda$ is a natural and regular parametrization. However, it is NOT a minimal
parametrization since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w} \mathbf{w}^T$&lt;/code&gt; is symmetric and therefore the sufficient statistics is linearly dependent.
It can be shown that $\Omega_\lambda$ is relatively open but not open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K=d+d^2$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;Recall that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}()$&lt;/code&gt; is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization&quot;&gt;half-vectorization function&lt;/a&gt;.
We define a new map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vec2h}(\mathrm{S}):=\mathrm{vech}\big(2\mathrm{S} - \mathrm{Diag}( \mathrm{diag}(\mathrm{S}) ) \big)$&lt;/code&gt;, which is like map $\mathrm{vech}()$ except that the off-diagonal entries in the low-triangular part are multiplied by 2.&lt;/p&gt;

        &lt;p&gt;A minimal natural parametrization $\eta$ should be defined as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;amp;= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \color{red}{\mathrm{vech}}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix}  \color{red}{\mathrm{vec2h}}( \mathrm{w}\mathbf{w}^T )   ) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
\end{aligned}
$$&lt;/code&gt; where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is a $(d+\frac{d(d+1)}{2})$-dim array.
It can be shown that $\Omega_\eta$ is open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K=d+\frac{d(d+1)}{2}$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;As we discussed in &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-steepest-direction&quot;&gt;Part II&lt;/a&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}(\Sigma^{-1})$&lt;/code&gt; is an intrinsic parameterization while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vec}(\Sigma^{-1})$&lt;/code&gt; is not.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;p&gt;The following example illustrates
a non-minimal natural parametrization&lt;/p&gt;
&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Non-minimal parametrization for Bernoulli (click to exapnd)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;

        &lt;p&gt;We consider this family in &lt;a href=&quot;#exponential-family&quot;&gt;the previous section&lt;/a&gt; with another
parametrization dicussed in
&lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families&quot;&gt;Part I&lt;/a&gt;.
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \frac{\pi_1}{\pi_1+\pi_2} + \mathcal{I}(w=1) \frac{\pi_2}{\pi_1+\pi_2} \Big| \pi_1&amp;gt;0, \pi_2&amp;gt;0 \}$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;We re-express it in another exponential form as&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;amp;=  \mathcal{I}(w=0) \frac{\pi_1}{\pi_1+\pi_2} + \mathcal{I}(w=1) \frac{\pi_2}{\pi_1+\pi_2} \\
&amp;amp;=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \begin{bmatrix} \log \frac{\pi_1}{\pi_1+\pi_2} \\  \log \frac{\pi_2}{\pi_1+\pi_2} \end{bmatrix} }_{\eta} , \underbrace{ \begin{bmatrix} \mathcal{I}(w=0) \\   \mathcal{I}(w=1)\end{bmatrix} }_{\mathbf{T}_\eta(w) } \rangle -\underbrace{ 0}_{A_\eta(\eta)} )
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;The natural parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta= \{ (\eta_1,\eta_2) | \exp(\eta_1)+\exp(\eta_2) = 1 \}$&lt;/code&gt; is not open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;This is not a minimal natural parametrization since the sufficient statistics $\mathbf{T}_\eta(w)$ is linearly dependent as 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{I}(w=0)+\mathcal{I}(w=1)=1$&lt;/code&gt;.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h1 id=&quot;efficient-natural-gradient-computation&quot;&gt;Efficient Natural-gradient Computation&lt;/h1&gt;

&lt;p&gt;In general, natural-gradient computation can be challenging due to the inverse of the Fisher matrix.
In cases of an exponential family,  natural-gradient computation often can be quite efficient without directly computing the inverse of the Fisher matrix.
We will assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is a reguar, minimal, and natural parametrization.&lt;/p&gt;

&lt;h2 id=&quot;expectation-parametrization&quot;&gt;Expectation Parametrization&lt;/h2&gt;
&lt;p&gt;We introduce a dual parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m} := E_{p(w|\eta)}[ \mathbf{T}_\eta(\mathbf{w}) ] $&lt;/code&gt;, which is known as the expectation parametrization. This new parametrization plays a key role for the efficient natural-gradient computation.&lt;/p&gt;

&lt;p&gt;Recall that in
&lt;a href=&quot;/posts/2021/11/Geomopt04/#the-hessian-is-not-a-valid-manifold-metric&quot;&gt;Part IV&lt;/a&gt;,  we use the identity of the score function.  We  can establish a connection 
between these two parametrizations via this identity.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{0} &amp;amp; = E_{p(w|\eta)} [ \nabla_\eta \log p(\mathbf{w}|\eta) ] &amp;amp;\,\,\,( \text{expectation of the
score is zero} ) \\
&amp;amp;=E_{p(w|\eta)} [ \mathbf{T}_\eta(\mathbf{w}) - \nabla_\eta A_\eta(\eta) ] \\
&amp;amp;= \mathbf{m}  - \nabla_\eta A_\eta(\eta)
\end{aligned}\tag{1}\label{1}
$$&lt;/code&gt;  which is a valid Legendre (dual) transformation&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; since 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\eta^2 A_\eta(\eta)$&lt;/code&gt; is positive-definite in its domain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;expectation-parameter-space&quot;&gt;Expectation Parameter Space&lt;/h2&gt;
&lt;p&gt;We can view
the expectation parameter as an ouput of
a continous map 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{m}(\eta):=\nabla_\eta A_\eta(\eta),
\end{aligned}\tag{2}\label{2}
$$&lt;/code&gt;
 where the input space of this map is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We define the expectation parameter space as the output space of the map 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\Omega_m :=\{\mathbf{m}(\eta) | \eta \in \Omega_\eta \}.
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\eta^2 A_\eta(\eta)$&lt;/code&gt; is positive-definite in open set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;, we can show that there exists an one-to-one relationship between the
natural parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; and the expectation parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt;, which implies that map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}(\cdot)$&lt;/code&gt; is injective.&lt;/p&gt;

&lt;p&gt;Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, we can show that
the expectation parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; is also open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt; due to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Invariance_of_domain&quot;&gt;invariance of domain&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;natural-gradient-computation&quot;&gt;Natural-gradient Computation&lt;/h2&gt;
&lt;p&gt;Note that the FIM of the exponential family under parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}_\eta(\eta)=\nabla_\eta^2 A_\eta(\eta) = \nabla_\eta \mathbf{m}
\end{aligned}\tag{3}\label{3}
$$&lt;/code&gt; which means this FIM is a Jacobian matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{J}=\nabla_\eta \mathbf{m}(\eta)$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As we discussed in
&lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-steepest-direction&quot;&gt;Part II&lt;/a&gt;,
a natural-gradient w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\eta)$&lt;/code&gt; can be computed as below, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\eta=\nabla_\eta f(\eta)$&lt;/code&gt; is a Euclidean gradient w.r.t.  natural parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\hat{\mathbf{g}}_\eta &amp;amp; = \mathbf{F}^{-1}_\eta(\eta) \mathbf{g}_\eta \\
&amp;amp;= (\nabla_\eta \mathbf{m} )^{-1} [ \nabla_\eta f(\eta) ] \\
&amp;amp;= [\nabla_{m} \eta ] [ \nabla_\eta f(\eta) ] \\
&amp;amp;=  \nabla_{m} f(\eta)  
\end{aligned}\tag{4}\label{4}
$$&lt;/code&gt; where 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_{m} f( \eta )$&lt;/code&gt; is a Euclidean gradient w.r.t. expectation parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt; and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta=\eta( \mathbf{m} )$&lt;/code&gt;
can be viewed  as a function of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Therefore, we can  efficinetly compute natural-gradients w.r.t. natural parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; &lt;a class=&quot;citation&quot; href=&quot;#khan2017conjugate&quot;&gt;[3]&lt;/a&gt;  if we  can  easily compute Euclidean gradients w.r.t. its expectation parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;efficient-ngd-for-multivariate-gaussians&quot;&gt;Efficient NGD for multivariate Gaussians&lt;/h2&gt;
&lt;p&gt;Given a $d$-dim multivariate Gaussian with a full covariance structure, 
the naive way to compute natural-gradients in this case has $O( (d^2)^3 )=O(d^6)$ iteration cost since the covariance matrix has $d^2$ entries.
Now, we show how to efficiently compute natural-gradients in this case.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Claim&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;We can efficiently  compute natural-gradients 
in $O( d^3 )$ iteration cost in this case with the number of $O(d^2)$ parameters.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Proof of the claim (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;
        &lt;p&gt;As shown in &lt;a href=&quot;#minimal-parametrizations-of-exponential-family&quot;&gt;the previous section&lt;/a&gt;, a minimal,regular, and natural parameter is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta=(\mathbf{S}\mu,-\frac{1}{2}\mathrm{vech}(\mathbf{S}) )$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Sigma$&lt;/code&gt; is the covariance and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}= ( \Sigma )^{-1}$&lt;/code&gt;.
$\Sigma$ is considered as a d-by-d matrix with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d(d+1)}{2}$&lt;/code&gt; distinct entries (degrees of freedom).&lt;/p&gt;

        &lt;p&gt;The corresponding expectation parameter  is $\mathbf{m}=( E_{p(w)}[ \mathbf{w} ], E_{p(w)}[ \mathrm{vec2h}( \mathbf{w}\mathbf{w}^T ) ]) = (\mu, \mathrm{vec2h}(\mu\mu^T+\Sigma ) ) $, where
$\mathrm{vec2h}(\mathrm{S}):=\mathrm{vech}\big(2\mathrm{S} - \mathrm{Diag}( \mathrm{diag}(\mathrm{S}) ) \big)$ is defined in the previous section.&lt;/p&gt;

        &lt;p&gt;By using the result in this section, we can efficiently  compute each natural gradient w.r.t. $\eta$ by  computing the corresponding Eucldiean gradient w.r.t. $\mathbf{m}$.&lt;/p&gt;

        &lt;p&gt;Denote $\mathbf{m}^{(1)}:=\mu$ and $\mathbf{m}^{(2)}:=\mathrm{vec2h}(\mu\mu^T+\Sigma)$.&lt;/p&gt;

        &lt;p&gt;Notice that we can re-express the mean and the covariance in terms of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}=(\mathbf{m}^{(1)}, \mathbf{m}^{(2)})$&lt;/code&gt; as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mu = \mathbf{m}^{(1)} , \mathrm{vec2h}(\Sigma) = \mathbf{m}^{(2)} -\mathrm{vec2h}( \mathbf{m}^{(1)}(\mathbf{m}^{(1)})^T)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;Given a smooth scalar function denoted by $\ell$,
by the chain rule, we have
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\overbrace{ \frac{\partial \ell}{\partial m^{(1)}_i} }^{\text{scalar}}&amp;amp; = \overbrace{ \frac{\partial \ell}{\partial \mu}}^{\text{row vector  } } \overbrace{ \frac{\partial \mu }{\partial m^{(1)}_i}}^{\text{column vector}} + \frac{\partial \ell}{\partial \mathrm{vec2h}(\Sigma)} \frac{\partial \mathrm{vec2h}(\Sigma) }{\partial m^{(1)}_i} \\
\frac{\partial \ell}{\partial m^{(2)}_i} &amp;amp; = \frac{\partial \ell}{\partial \mu} \frac{\partial \mu }{\partial m^{(2)}_i} + \frac{\partial \ell}{\partial \mathrm{vec2h}(\Sigma)} \frac{\partial \mathrm{vec2h}(\Sigma) }{\partial m^{(2)}_i}
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$m^{(1)}_i$&lt;/code&gt; denotes the $i$-th entry of  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}^{(1)}$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;First, notice that we can efficiently compute the following Euclidean gradients by Auto-Diff
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\frac{\partial \ell}{\partial \mu} &amp;amp;= \mathbf{g}_\mu^T \\
\frac{\partial \ell}{\partial V } &amp;amp; =\mathbf{g}_{V} \,\,\,\, ( \mathbf{V} \text{ is a d-by-d matrix with } d^2  \text{ degrees of freedom} ) \\
\frac{\partial \ell}{\partial \Sigma } &amp;amp; =\mathbf{g}_{\Sigma} \,\,\,\, ( \Sigma \text{ is a d-by-d matrix with } \frac{d(d+1)}{2}  \text{ degrees of freedom} ) \\
\frac{\partial \ell}{\partial \mathrm{vec2h}(\Sigma)} &amp;amp;=  ( \mathrm{vech}(\mathbf{g}_{V}))^T 
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{V}$&lt;/code&gt; takes the same value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Sigma$&lt;/code&gt;.&lt;/p&gt;

        &lt;div class=&quot;notice--danger&quot;&gt;
          &lt;p&gt;Note:&lt;/p&gt;

          &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{vech}(\mathbf{g}_\Sigma) = \mathrm{vec2h}(\mathbf{g}_V)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
        &lt;/div&gt;
        &lt;p&gt;We can verify the following identity for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}^{(1)}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\frac{\partial \ell}{\partial \mathrm{vec2h}(\Sigma)} \frac{\partial \mathrm{vec2h}(\Sigma) }{\partial m^{(1)}_i} &amp;amp;= \mathrm{Trace}\big(\frac{\partial \ell}{\partial \mathbf{V}} \frac{\partial \mathbf{V} }{\partial m^{(1)}_i} \big)
\end{aligned}
$$&lt;/code&gt;  where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{V}= \mathbf{m}^{(2)} - \mathbf{m}^{(1)}(\mathbf{m}^{(1)})^T$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;By the identity, we can easily compute the Euclidean gradients w.r.t. the expectation parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}= (\mathbf{m}^{(1)}, \mathbf{m}^{(2)})$&lt;/code&gt; as&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\frac{\partial \ell}{\partial m^{(1)}} &amp;amp;= \mathbf{g}_\mu^T - 2 (\mathbf{m}^{(1)})^T \mathbf{g}_{V}^T = \mathbf{g}_\mu^T - 2 \mu^T \mathbf{g}_{V}^T \\
\frac{\partial \ell}{\partial m^{(2)}} &amp;amp;= \mathbf{0} + ( \mathrm{vech}(\mathbf{g}_{V}))^T = ( \mathrm{vech}(\mathbf{g}_{V}))^T
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h1 id=&quot;natural-gradient-descent-as-unconstrained-mirror-descent&quot;&gt;Natural-gradient Descent as Unconstrained Mirror Descent&lt;/h1&gt;
&lt;p&gt;We assume natural parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is both regular and minimal.
In exponential family cases, we will show that natural-gradient descent as a mirror descent update when the natural parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is unconstrained.&lt;/p&gt;

&lt;p&gt;Since mirror descent is defined by using a Bregman divergence, we first introduce the Bregman divergence.&lt;/p&gt;

&lt;h2 id=&quot;bregman-divergence&quot;&gt;Bregman Divergence&lt;/h2&gt;
&lt;p&gt;Given a strictly convex function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Phi(\cdot)$&lt;/code&gt; in its domain, a Bregman divergence equipped with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Phi(\cdot)$&lt;/code&gt; is defined as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{B}_\Phi(\mathbf{x},\mathbf{y}):= \Phi(\mathbf{x})- \Phi(\mathbf{y}) - \langle \nabla \Phi(\mathbf{y}), (\mathbf{x}-\mathbf{y}) \rangle
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In particular, the Kullback–Leibler (KL) divergence is a Bregman divergence under natural parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\, &amp;amp; \,\mathrm{KL} [p(\mathbf{w}| \eta_1 ) || p(\mathbf{w}|\color{red}{\eta_2})]\\
=&amp;amp; \,E_{p(w|\eta_1)} [ \log \frac{p(\mathbf{w}|\eta_1)} {p(\mathbf{w}|\eta_2)} ] \\
=&amp;amp; \,E_{p(w|\eta_1)} [  \langle \eta_1-\eta_2 , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta(\eta_1) + A_\eta(\eta_2) ] &amp;amp; ( p(\mathbf{w}|\eta) \text{ is an exponential family}) \\
=&amp;amp; \,A_\eta(\eta_2) - A_\eta(\eta_1) - E_{p(w|\eta_1)} [  \langle \eta_2-\eta_1,  \mathbf{T}_\eta (\mathbf{w}) \rangle ] \\
=&amp;amp; \,A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \underbrace{ E_{p(w|\eta_1)} [ \mathbf{T}_\eta (\mathbf{w}) ] }_{ \nabla_\eta A_\eta(\eta_1) }  \rangle    \\
=&amp;amp; \, \mathrm{B}_{A_\eta}(\color{red} {\mathbf{\eta}_2},  \mathbf{\eta}_1 ) &amp;amp; ( A_\eta(\eta) \text{ is strictly convex})
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We denote the expectation parameter as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt;.
Recall that by the
Legendre transformation, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}=\nabla_\eta A_\eta(\eta)$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; has been defined &lt;a href=&quot;#expectation-parameter-space&quot;&gt;here&lt;/a&gt;.
Note that we assume natural parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is minimal. In other words,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\eta^2 A_\eta(\eta)$&lt;/code&gt; is
positive-definite and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\eta)$&lt;/code&gt; is strictly convex in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We define &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\mathbf{x}):=+\infty$&lt;/code&gt; when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x} \not \in \Omega_\eta$&lt;/code&gt;.
The convex conjugate  of the log-partition function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta$&lt;/code&gt; is defined as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
A^*_\eta( \mathbf{m}) &amp;amp;:= \sup_{x} \{ \langle \mathbf{x},\mathbf{m} \rangle - A_\eta(\mathbf{x}) \} \\
&amp;amp;= \langle \mathbf{\eta},\mathbf{m} \rangle - A_\eta(\mathbf{\eta})  \,\,\,\, (\text{the supremum attains at } \mathbf{x}=\eta \in \Omega_\eta )\\
\end{aligned}\tag{5}\label{5}
$$&lt;/code&gt; where 
the domain of  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A^*_\eta(\mathbf{m})$&lt;/code&gt;  is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt;, and 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta=\eta( \mathbf{m} )$&lt;/code&gt;
should be viewed as a function of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m} \in \Omega_m$&lt;/code&gt;, we have the following identity, which is indeed another Legendre transformation.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\nabla_{\mathbf{m}} A^*_\eta( \mathbf{m})
&amp;amp;= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - \nabla_{\mathbf{m}} A_\eta(\mathbf{\eta}) \\
&amp;amp;= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - [\nabla_{\mathbf{m}} \eta] \underbrace{ [\nabla_\eta A_\eta(\mathbf{\eta})] }_{ = \mathbf{m}}\\
&amp;amp;= \mathbf{\eta} ,
\end{aligned}\tag{6}\label{6}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta \in \Omega_\eta$&lt;/code&gt; due to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The convex conjugate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A^*_\eta( \mathbf{m})$&lt;/code&gt; is strictly convex w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt; since the Hessian &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_m^2 A^*_\eta( \mathbf{m})$&lt;/code&gt;
is positive-definite as shown below.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})
&amp;amp;= \nabla_{\mathbf{m}} \mathbf{\eta}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that due to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt;,
the FIM &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_\eta(\eta)$&lt;/code&gt; under natural parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\eta}$&lt;/code&gt;  is the Jacobian matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{J}= \nabla_{\eta} \mathbf{m}$&lt;/code&gt;
and 
positive-definite.&lt;/p&gt;

&lt;p&gt;Therefore, it is easy to see that
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m}) = 
\mathbf{F}^{-1}_\eta(\eta),
$&lt;/code&gt; which is 
positive-definite and therefore strictly convex.&lt;/p&gt;

&lt;p&gt;By the &lt;a href=&quot;/posts/2021/11/Geomopt03/#transformation-rule-for-the-fisher-information-matrix&quot;&gt;transformation rule&lt;/a&gt; of the FIM,  we have the following relationship.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}_{\eta} (\eta) &amp;amp; = \mathbf{J}^T \mathbf{F}_{m}(\mathbf{m}) \mathbf{J} \\
&amp;amp;= \mathbf{F}^T_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) \\
&amp;amp;= \mathbf{F}_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) &amp;amp; (\text{the FIM is symmetric})
\end{aligned}
$$&lt;/code&gt;  which implies that
the FIM under expectation parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_m(\mathbf{m})=\mathbf{F}^{-1}_\eta(\eta) = \nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})= \nabla_{\mathbf{m}} \mathbf{\eta}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Moreover, we have the following identity since by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\eta)=\langle \mathbf{\eta},\mathbf{m} \rangle- A^*_\eta( \mathbf{m}) $&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{B}_{A_\eta}(\mathbf{\eta}_2, \color{red}{\mathbf{\eta}_1 })
&amp;amp;= A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \overbrace{ \nabla_\eta A_\eta(\eta_1) }^{= \mathbf{m}_1} \rangle    \\
&amp;amp;= [  \langle \mathbf{\eta}_2,\mathbf{m}_2 \rangle- A^*_\eta( \mathbf{m}_2) ]   
-[  \langle \mathbf{\eta}_1,\mathbf{m}_1 \rangle- A^*_\eta( \mathbf{m}_1) ]
-\langle \eta_2-\eta_1, \mathbf{m}_1 \rangle \\
&amp;amp;=  A^*_\eta( \mathbf{m}_1) - A^*_\eta( \mathbf{m}_2) -    
\langle \mathbf{m}_1-\mathbf{m}_2, \underbrace{ \eta_2}_{ = \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_2)} \rangle\\
&amp;amp;= \mathrm{B}_{A^*_\eta}( \color{red}{ \mathbf{m}_1 },\mathbf{m}_2) &amp;amp; (\text{the order is changed})
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;mirror-descent&quot;&gt;Mirror Descent&lt;/h2&gt;

&lt;p&gt;Now, we give the definition of mirror descent.&lt;/p&gt;

&lt;p&gt;Consider the following optimization problem over a convex domain denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\theta$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\theta \in \Omega_\theta} \ell_\theta(\mathbf{\theta})
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Given a strictly convex function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Phi(\mathbf{\theta})$&lt;/code&gt; in the domain , mirror
descent with step-size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\alpha$&lt;/code&gt; is defined as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{\theta}_{k+1} \leftarrow \arg \min_{x \in \Omega_\theta}\{ \langle \nabla_\theta \ell_\theta(\mathbf{\theta}_k), \mathbf{x}-\mathbf{\theta}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{\Phi}(\mathbf{x},\mathbf{\theta}_k) \}
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{B}_\Phi(\cdot,\cdot)$&lt;/code&gt; is a Bregman divergence  equipped  with the strictly convex function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Phi(\cdot)$&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;natural-gradient-descent-as-mirror-descent&quot;&gt;Natural-gradient Descent as Mirror Descent&lt;/h2&gt;

&lt;p&gt;To show natural-gradient descent as a mirror descent update, we have to make the following assumption.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Additional assumption&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;Natural parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt;
is unconstrainted (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta=\mathcal{R}^K$&lt;/code&gt;), where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt; is the number of entries of parameter array &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The following example illustrates that the expectation space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; is constrained even when
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is unconstrained.&lt;/p&gt;
&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Example: $\Omega_m$ is constrained while $\Omega_\eta$ is unconstrained (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;
        &lt;p&gt;Example: Bernoulli family&lt;/p&gt;

        &lt;p&gt;We consider this family as discussed in &lt;a href=&quot;#exponential-family&quot;&gt;the previous section&lt;/a&gt;
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \Big| 0&amp;lt;\pi&amp;lt;1 \}$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;We re-express it in an exponential form as&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;amp;=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \log \frac{\pi}{1-\pi}}_{\eta} , \underbrace{ \mathcal{I}(w=0)}_{T_\eta(w) } \rangle - \log \frac{1}{1-\pi} )
\end{aligned}
$$&lt;/code&gt;
where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi = \frac{\exp(\eta)}{1+ \exp(\eta) } $&lt;/code&gt; and&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\eta(\mathbf{\eta}) =  \log \frac{1}{1-\pi} = \log(1+\exp(\eta))$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;The natural parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta= \{ \eta | \eta \in \mathcal{R} \}=\mathcal{R}^1$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;The corresponding expectation parameter is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$m = E_{q(w|\eta)}[ T_\eta (w) ] = \pi$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;The expectation parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m= \{ m| 0&amp;lt;m&amp;lt;1 \}$&lt;/code&gt; is a constrained open set in $\mathcal{R}^1$.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;p&gt;Now, consider the following mirror descent in the expectation parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{x \in \Omega_m}\{ \langle \nabla_m \ell_m(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k) \}
\end{aligned}\tag{7}\label{7}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}_{k} \in \Omega_m$&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_m \ell_m(\mathbf{m}_k):= \nabla_m \ell_\eta (\eta(\mathbf{m}_k))$&lt;/code&gt; and the Bregman divergence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{B}_{A^*_\eta}(\cdot,\cdot)$&lt;/code&gt; is well-defined
since  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A^*_\eta$&lt;/code&gt; is strictly convex in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt;.
Recall  that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; can still be constrained.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Claim&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta = \mathcal{R}^K$&lt;/code&gt;, the solution of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; is equivalent to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_{k+1}  \leftarrow  \eta_k - \alpha \nabla_m \ell_m(\mathbf{m}_k)$&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Proof of the claim (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;
        &lt;p&gt;Proof :&lt;/p&gt;

        &lt;p&gt;Denote 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
u(\mathbf{x}) &amp;amp;:=\langle \nabla_m \ell_m(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha} [ \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k)] \\
&amp;amp; = \langle \nabla_m \ell_m(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  [A^*_\eta( \mathbf{x}) - A^*_\eta( \mathbf{m}_k) -    
\langle \mathbf{x}-\mathbf{m}_k, \underbrace{ \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_k)}_{ = \eta_k \text{ by } \eqref{6}  } \rangle ],
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}_k \in \Omega_m $&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;A stationary point of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{x}}$&lt;/code&gt;,  must satisfy the following
condition.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{0} &amp;amp;= \nabla_x u(\hat{\mathbf{x}}) &amp;amp; (\mathbf{m}_k \text{ is considered to be a constant}) \\
&amp;amp;= \nabla_m \ell_m(\mathbf{m}_k)+ \frac{1}{\alpha}  [ \nabla_x A^*_\eta( \hat{\mathbf{x}})  -  \eta_k ],
\end{aligned}
$$&lt;/code&gt; which implies that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_x A^*_\eta( \hat{\mathbf{x}}) =  \eta_k - \alpha \nabla_m \ell_m(\mathbf{m}_k)$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;We first show that there exists a stationary point in the domain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(\hat{\mathbf{x}} \in \Omega_m)$&lt;/code&gt;.
Let’s denote &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\eta}_{k+1}:= \eta_k - \alpha \nabla_m \ell_m(\mathbf{m}_k)$&lt;/code&gt;. Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta$&lt;/code&gt; is unconstrained, it
is obvious that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\eta}_{k+1} \in \Omega_\eta$&lt;/code&gt;.
By &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}(\mathbf{\eta}_{k+1}) =\nabla_\eta A_\eta( \mathbf{\eta}_{k+1}) \in
\Omega_m$&lt;/code&gt;. Notice that,  by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt;, we have  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_m A^*_\eta ( \mathbf{m}(\mathbf{\eta}_{k+1}) ) = \mathbf{\eta}_{k+1}$&lt;/code&gt;, which implies that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}(\mathbf{\eta}_{k+1})$&lt;/code&gt; is a stationary point and 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}(\mathbf{\eta}_{k+1}) \in \Omega_m$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;Moreover,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}(\mathbf{\eta}_{k+1})$&lt;/code&gt; is 
the unique  solution of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_x^2 u(\mathbf{x}) =\nabla_x^2 A^*_\eta(
{\mathbf{x}})$&lt;/code&gt; is positive-definite for any  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x} \in \Omega_m$&lt;/code&gt; and therefore strictly convex.
In other words, 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}_{k+1} = \mathbf{m}( {\mathbf{\eta}_{k+1}}) $&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;In summary, when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta = \mathcal{R}^K$&lt;/code&gt;, the unique solution of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; is 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\eta_{k+1} &amp;amp; \leftarrow  \eta_k - \alpha \nabla_m \ell_m(\mathbf{m}_k) \\
\mathbf{m}_{k+1} &amp;amp; \leftarrow  \nabla_\eta A_\eta( {\mathbf{\eta}_{k+1}})
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;p&gt;By the claim,
mirror descent of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; in &lt;strong&gt;expectation&lt;/strong&gt; parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_m$&lt;/code&gt; is equivalent to
the following update
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha \nabla_m \ell_m(\mathbf{m}_k)
= \eta_k - \alpha\nabla_m \ell_\eta( \underbrace{ \eta(\mathbf{m}_k) }_{= \eta_k}),
\end{aligned}\tag{8}\label{8}
$$&lt;/code&gt; which is exactly natural gradient
descent in &lt;strong&gt;natural&lt;/strong&gt; parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\eta=\mathcal{R}^K$&lt;/code&gt; since by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt;, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_m \ell_m(\mathbf{m}_k) = \nabla_m \ell_\eta( \eta_k)= \mathbf{F}
_\eta^{-1} (\eta_k) \nabla_\eta \ell_\eta(\eta_k)$&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p class=&quot;bibliography&quot;&gt;&lt;p&gt;&lt;span id=&quot;johansen1979introduction&quot;&gt;[1] S. Johansen, &lt;i&gt;Introduction to the theory of regular exponential families&lt;/i&gt; (Institute of Mathematical Statistics, University of Copenhagen, 1979).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;wainwright2008graphical&quot;&gt;[2] M. J. Wainwright &amp;amp; M. I. Jordan, &lt;i&gt;Graphical models, exponential families, and variational inference&lt;/i&gt; (Now Publishers Inc, 2008).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;khan2017conjugate&quot;&gt;[3] M. Khan &amp;amp; W. Lin, &quot;Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models,&quot; &lt;i&gt;Artificial Intelligence and Statistics&lt;/i&gt; (PMLR, 2017), pp. 878–887.&lt;/span&gt;&lt;/p&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes:&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;When the natural parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is minimal, this Legendre transformation is diffeomorphic since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\eta^2 A_\eta(\eta)$&lt;/code&gt; is positive-definite in its domain. In other words, the expectation parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{m}$&lt;/code&gt; is also intrinsic when the natural parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is minimal. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><category term="Exponential Family" /><summary type="html">Warning: working in Progress (incomplete)</summary></entry><entry><title type="html">Part IV: Natural and Riemannian Gradient Descent</title><link href="/posts/2021/11/Geomopt04/" rel="alternate" type="text/html" title="Part IV: Natural and Riemannian Gradient Descent" /><published>2021-11-15T00:00:00-08:00</published><updated>2021-11-15T00:00:00-08:00</updated><id>/posts/2021/11/Geomopt04</id><content type="html" xml:base="/posts/2021/11/Geomopt04/">&lt;p&gt;Warning: working in Progress (incomplete)&lt;/p&gt;

&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post should help readers to understand natural-gradient descent and Riemannian gradient descent.
We also discuss some invariance property of natural-gradient descent, Riemannian gradient descent, and Newton’s method.&lt;/p&gt;

&lt;p&gt;We will give an informal introduction with a focus on high level of ideas.&lt;/p&gt;

&lt;h1 id=&quot;two-kinds-of-spaces&quot;&gt;Two kinds of Spaces&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;We will discuss (Riemannian) gradient spaces and parameter spaces for gradient-based updates.&lt;/p&gt;

&lt;p&gt;As we disucssed in &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt;, the parameter space $\Omega_\tau$ and the tangent space denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$T\mathcal{M}_{\tau_0}$&lt;/code&gt; at point $\tau_0$ are two distinct spaces. 
Given  &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;intrinsic parametrization&lt;/a&gt;  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;,  the tangent space is a (complete) vector space and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$T\mathcal{M}_{\tau_0}=\mathcal{R}^K$&lt;/code&gt; while the parameter space $\Omega_\tau$ is like a local vector space in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, where $K$ is the dimension of the manifold. In other words, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; is often an open (proper) subset of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In manifold cases, we have to explicitly distinguish the difference between the representation of a point (parameter) and a vector (Riemannian gradient).  These two spaces are different in many aspects such as
the domain and the distance.
Mathematically
speaking,  a (Riemannian) gradient space is much simpler and nicer than a  parameter space.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sphere.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;natural-gradient-descent-in-an-intrinsic-parameter-space&quot;&gt;Natural-gradient Descent in an Intrinsic Parameter Space&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;Using intrinsic parametrization $\tau$, we can perform a natural-gradient update known as natural-gradient descent (NGD) &lt;a class=&quot;citation&quot; href=&quot;#amari1998natural&quot;&gt;[1]&lt;/a&gt; when we use the Fisher-Rao metric.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{NGD:} &amp;amp;\,\,\,\,\, 
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{1}\label{1}
$$&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\tau_k}$&lt;/code&gt; is a natural gradient evaluated at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_{k}$&lt;/code&gt; and $\alpha&amp;gt;0$ is a step-size.&lt;/p&gt;

&lt;p&gt;This update in Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; is inspired by the standard vector update in the &lt;strong&gt;gradient space&lt;/strong&gt;.
By choosing an intrinsic parametrization, this update is also valid in the &lt;strong&gt;parameter space&lt;/strong&gt; as long as the step-size is small
enough.&lt;/p&gt;

&lt;p&gt;The update in the parameter space is valid since the parameter space $\Omega_\tau$ has a local vector-space structure thanks to the use of an intrinsic parametrization.
However, when $\Omega_\tau$ is a proper subset of $T\mathcal{M}_{\tau_k}$ (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau \neq T\mathcal{M}_{\tau_k} $&lt;/code&gt;), the update in Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; is valid only when the step-size $\alpha$ is small enough so that  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_{k+1} \in \Omega_\tau$&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;notice--danger&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;Using a small step-size could be an issue since it could greatly slow down the progression of natural-gradient
descent in practice.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Example of NGD in a constrained space: (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;

        &lt;p&gt;Consider a univariate Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mu,v) \Big| \mu \in \mathcal{R}, v&amp;gt;0 \}$&lt;/code&gt; with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt;, variance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$v$&lt;/code&gt;, and intrinsic parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\mu,v) $&lt;/code&gt;. &lt;br /&gt;&lt;/p&gt;

        &lt;p&gt;We have to properly select the step-size $\alpha$ for natural-gradient descent in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; due to the positivity constraint in $\sigma$.&lt;/p&gt;

        &lt;p&gt;In multivariate Gaussian cases, we have to handle a positive-definite constraint.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h1 id=&quot;natural-gradient-descent-is-linearly-invariant&quot;&gt;Natural-gradient Descent is Linearly Invariant&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;Recall that in &lt;a href=&quot;/posts/2021/11/Geomopt03/#parameter-transform-and-invariance&quot;&gt;Part III&lt;/a&gt;, we have shown that natural gradients are invariant under any intrinsic parameter transformation.
The parameter transformation can be non-linear.&lt;/p&gt;

&lt;p&gt;It is natural to expect that natural-gradient descent has a similar property. Unfortunately, natural-gradient descent is only invariant under an intrinsic &lt;strong&gt;linear&lt;/strong&gt; transformation. Note that Newton’s method is also linearly invariant while Euclidean gradient descent is not.&lt;/p&gt;

&lt;p&gt;Let’s consider the following (scalar) optimization problem on a smooth manifold $\mathcal{M}$ with the Fisher-Rao metric &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$F$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{x \in \mathcal{M}} h(x)
\end{aligned}\tag{2}\label{2}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that $\mathcal{M}$ in general does not have a vector-space structure. 
We has to &lt;strong&gt;artificially&lt;/strong&gt; choose an intrinsic parameterization $\tau$ so that the parameter space $\Omega_\tau$ at least has a local  vector-space structure.
The problem in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; can be re-expressed as below.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau \in \Omega_\tau} h_\tau(\tau)
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\tau$&lt;/code&gt; is the parameter representation of the smooth function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Natural gradient descent in this parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{3}\label{3}
$$&lt;/code&gt; where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_\lambda$&lt;/code&gt; is the FIM, natural gradient is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\tau_k} := [\mathbf{F}_\tau(\tau_k) ]^{-1} \nabla_\tau h_\tau(\tau_k)$&lt;/code&gt; and the step-size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\alpha$&lt;/code&gt; is small enough so that  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_{k+1} \in \Omega_\tau$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Consider another intrinsic parameterization $\lambda$ so that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda=\mathbf{U} \tau$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{U}$&lt;/code&gt; is a constant (square) invertible matrix. 
For simplicity,  we further assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\lambda=\{ \mathbf{U}\tau |\tau \in\Omega_\tau \} $&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\lambda$&lt;/code&gt; is the parameter space of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Natural gradient descent in this parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\lambda$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} -  \alpha  \hat{\mathbf{g}}_{\lambda_k} 
\end{aligned}\tag{4}\label{4}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\lambda_k} := [\mathbf{F}_\lambda(\lambda_k) ]^{-1} \nabla_\lambda h_\lambda(\lambda_k)$&lt;/code&gt;  and the cost function is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\lambda(\lambda) = h_\tau(\tau(\lambda))$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Recall that we have the &lt;a href=&quot;/posts/2021/11/Geomopt03/#transformation-rules-for-natural-gradients-and-euclidean-gradients&quot;&gt;transformation rule&lt;/a&gt; for natural gradients as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\hat{\mathbf{g}}_\tau= \mathbf{Q}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$&lt;/code&gt; where $Q_{ji}=\frac{\partial \tau^j(\lambda)}{\partial \lambda^i}$.&lt;/p&gt;

&lt;p&gt;We can verify that $\mathbf{Q} = \mathbf{U}^{-1}$. Notice that $\tau_0 = \mathbf{U}^{-1} \lambda_0$ by construction.
The update in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt; at iteration $k=1$ then can be re-expressed as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{1} \leftarrow \tau_{0} -  \alpha  \hat{\mathbf{g}}_{\tau_0} = \mathbf{U}^{-1} \lambda_0 -  \alpha  \mathbf{U}^{-1}  \hat{\mathbf{g}}_{\lambda_0} = \mathbf{U}^{-1} \lambda_1
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\alpha$&lt;/code&gt; is small enough, we have  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_1 \in \Omega_\tau$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_1 \in \Omega_\lambda$&lt;/code&gt;.
It is easy to show that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_k = \mathbf{U}^{-1} \lambda_k$&lt;/code&gt; by induction.
Therefore, updates in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; are equivalent.&lt;/p&gt;

&lt;h1 id=&quot;riemannian-gradient-descent-and-its-non-linear-invariance&quot;&gt;Riemannian Gradient Descent and its (Non-linear) Invariance&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Now we discuss a gradient-based method that is invariant to any intrinsic parameter transformation.&lt;/p&gt;

&lt;p&gt;As mentioned before, a manifold in general does not have a vector-space structure. 
We has to &lt;strong&gt;artificially&lt;/strong&gt; choose an intrinsic parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;, which gives rise to
a parametrization-dependence.
Therefore, it will be ideal if  a 
gradient-based method does not dependent on the choice of intrinsic parametrizations.&lt;/p&gt;

&lt;p&gt;We will first introduce the concept of a (one-dimensional) geodesic $\gamma(t)$, which is the “shortest curve” on a manifold with a Riemannian metric (i.e., the Fisher-Rao metric).
Recall that in  &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt; we only define a distance between two Riemannian gradients evaluated at the same point. We can use the length of a geodesic to define the distance between two points on the manifold. 
In statistics, the distance induced by a geodesic with the Fisher-Rao metric is known as the Rao distance &lt;a class=&quot;citation&quot; href=&quot;#atkinson1981rao&quot;&gt;[2]&lt;/a&gt;. This is
known as the boundary value problem (BVP) of a geodesic. The boundary conditions are  specified by  a starting point and an end
point in the manifold. To solve this problem, we often solve an easier problem, which is known as the initial value
problem (IVP) of a geodesic. The initial conditions are  specified by a starting point and an initial Riemannian gradient/velocity.&lt;/p&gt;

&lt;p&gt;We will only consider the IVP of a geodesic for simplicity. 
Consider an intrinsic parametrization $\tau$, where $\gamma_\tau(t)$ is the parameter representation of the geodesic.
To specify a geodesic, we need to provide a starting point $\tau_0$ on the manifold and a Riemannian gradient $\mathbf{v}_{\tau_0}$ evluated at point $\tau_0$.
The geodesic is the solution of a system of second-order non-linear ordinary differential equations (ODE) with the following initial conditions.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\gamma_\tau(0) = \tau_0; \,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$&lt;/code&gt; where the geodesic is uniquely determined by the initial conditions and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau$&lt;/code&gt; is the domain of the parametric geodesic curve. We assume 0 is contained in the domain for simplicity.&lt;/p&gt;

&lt;p&gt;For a &lt;a href=&quot;https://en.wikipedia.org/wiki/Geodesic_manifold&quot;&gt;geodesically complete&lt;/a&gt; manifold, the domain of the geodesic is the whole space  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_{\tau}
=\mathcal{R}$&lt;/code&gt;. We will only consider this case in this post.&lt;/p&gt;

&lt;p&gt;To avoid writing down the differential equations of a geodesic&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; (i.e., &lt;a href=&quot;https://en.wikipedia.org/wiki/Levi-Civita_connection#Christoffel_symbols&quot;&gt;Christoffel symbols&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Connection_form#Example:_the_Levi-Civita_connection&quot;&gt;connection 1-form in a section&lt;/a&gt;), 
researchers refer to it as the manifold exponential map.
Given an intrinsic parametrization $\tau$,
the  map at point $\tau_0$  is defined as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{Exp}_{\tau_0}\colon  T\mathcal{M}_{\tau_0} &amp;amp; \mapsto \mathcal{M}\\
\mathbf{v}_{\tau_0} &amp;amp; \mapsto \gamma_\tau(1) \,\,\,\, \textrm{s.t.} \,\,\,\,\,\, \gamma_\tau(0) = \tau_0;\,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$&lt;/code&gt;  where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$t$&lt;/code&gt; is fixed to be 1 and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt; denotes the initial point under  parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;notice--danger&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;We use 
a parametric representation of a geodesic curve to define the exponential map.
Thus, the form of the map does depend on the choice of parametrizations.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Now, we can define a Riemannian gradient method.
Under intrinsic parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; of a manifold, (exact) Riemannian gradient descent (RGD) is defined as&lt;/p&gt;
&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{RGD:} &amp;amp;\,\,\,\,\, 
\tau_{k+1} \leftarrow \mathrm{Exp}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} ) 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_{k+1}$&lt;/code&gt; always stays in the manifold thanks to the exponential map since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\tau_k}$&lt;/code&gt; is in the domain of the exponential map.&lt;/p&gt;

&lt;p&gt;The invariance of this update is due to the uniqueness of the geodesic and the invariance of &lt;a href=&quot;https://en.wikipedia.org/wiki/Solving_the_geodesic_equations&quot;&gt;the Euler-Lagrange equation&lt;/a&gt;. We will not discuss this further in this post to avoid complicated derivations.&lt;/p&gt;

&lt;p&gt;Although Riemannian gradient descent is nice, the exponential map or the geodesic often has high computational cost and does not admit a closed-form expression.&lt;/p&gt;

&lt;h1 id=&quot;many-faces-of-natural-gradient-descent&quot;&gt;Many faces of Natural-gradient Descent&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;natural-gradient-descent-as-inexact-riemannian-gradient-descent&quot;&gt;Natural-gradient Descent as Inexact Riemannian Gradient Descent&lt;/h2&gt;

&lt;p&gt;Natural-gradient descent can be derived from a first-order (linear) approximation of the geodesic, which implies that natural-gradient descent is indeed an inexact Riemannian gradient update.
Natural-gradient descent is linearly invariant due to the approximation.&lt;/p&gt;

&lt;p&gt;Consider a first-order Taylor approximation at $t=0$ of the geodesic shown below.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\gamma_\tau(t) \approx  \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (t-0)  
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;notice--danger&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;This approximation does not guarantee that the approximated geodesic stays on the manifold for all $t \neq 0$.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Recall that the exponential map is defined via the geodesic &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma_\tau(1)$&lt;/code&gt;.
We can use this approximation of the geodesic to define a new map (A.K.A. the Euclidean retraction map) as shown below.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{Ret}_{\tau_0}(\mathbf{v}_{\tau_0}) := \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (1-0) =\tau_0 + \mathbf{v}_{\tau_0}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Therefore, an inexact Riemannian gradient update with this new map is defined as 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \mathrm{Ret}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} )  = \tau_k  - \alpha  \hat{\mathbf{g}}_{\tau_k}
\end{aligned}
$$&lt;/code&gt; which recovers natural-gradient descent.&lt;/p&gt;

&lt;h2 id=&quot;natural-gradient-descent-as-unconstrained-proximal-gradient-descent&quot;&gt;Natural-gradient Descent as Unconstrained Proximal-gradient Descent&lt;/h2&gt;

&lt;p&gt;In this section, we will make an additional  assumption:&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Additional assumption&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;The parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau=\mathcal{R}^K$&lt;/code&gt; is unconstrained.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;As we mentioned before, the &lt;strong&gt;distances&lt;/strong&gt; in the gradient space and the parameter space are defined differently. 
In the &lt;a href=&quot;#riemannian-gradient-descent-and-its-non-linear-invariance&quot;&gt;previous section&lt;/a&gt;, we use the geodesic to define the distance between two points in a parameter space.&lt;/p&gt;

&lt;p&gt;We could also use other “distances” denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{D}(.,.)$&lt;/code&gt; (i.e., Kullback–Leibler divergence) &lt;a class=&quot;citation&quot; href=&quot;#khan2016faster&quot;&gt;[3]&lt;/a&gt; to define the length between two points in a parameter space.&lt;/p&gt;

&lt;p&gt;In the following section, we assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$p(w|\mathbf{y})$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$p(w|\tau_k)$&lt;/code&gt; are two members  in a parameteric distribution family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$p(w|\tau)$&lt;/code&gt; indexed by  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{y}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_k$&lt;/code&gt;, respectively.&lt;/p&gt;

&lt;p&gt;We define a class of f-divergence in this case. 
Note that a f-divergence can be defined in a more general setting.&lt;/p&gt;

&lt;h3 id=&quot;csiszar-f-divergence&quot;&gt;Csiszar f-divergence&lt;/h3&gt;
&lt;hr /&gt;

&lt;p&gt;Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f:(0,+\infty)\mapsto \mathcal{R}$&lt;/code&gt; be a  smooth scalar function satisfying all the following conditions.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;convex in its domain&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(1)=0$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\ddot{f}(1)=1$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A f-divergence for a parametric family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$p(w|\tau)$&lt;/code&gt; is defined as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{D}_f(\mathbf{y},\tau_k) := \int f(\frac{p(w|\mathbf{y})}{p(w|\tau_k)}) p(w|\tau_k) dw.
\end{aligned}
$$&lt;/code&gt;
Thanks to Jensen’s inequality,
a f-divergence is always non-negative as&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{D}_f(\mathbf{y},\tau_k) \geq  f( \int \frac{p(w|\mathbf{y})}{p(w|\tau_k)} p(w|\tau_k) dw )  = f(\int p(w|\mathbf{y})dw) =f(1) =0 .
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The KL divergence is a f-divergence where $f(t)=-\log(t)$.&lt;/p&gt;

&lt;h3 id=&quot;proximal-gradient-descent&quot;&gt;Proximal-gradient descent&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;Given such a “distance” &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{D}(\mathbf{y},\tau_k)$&lt;/code&gt;, we could perform an unconstrained proximal-gradient update as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \arg\min_{\mathbf{y} \in  \mathcal{R}^K  } \{ \langle \mathbf{g}_{\tau_k}, \mathbf{y}\rangle   + \frac{1}{\alpha} \mathrm{D}(\mathbf{y},\tau_k) \}
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_{\tau_k}$&lt;/code&gt; is a Eulcidean gradient and the parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; is unconstrained.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Claim&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;The secord-order Taylor approximation of any f-divergence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{D}_f(\mathbf{y},\tau_k)$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{y}=\tau_k$&lt;/code&gt; is  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{1}{2}(\mathbf{y}-\tau_k)^T \mathbf{F}_\tau(\tau_k) (\mathbf{y}-\tau_k)$&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Proof of the claim: (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;p&gt;Proof:&lt;/p&gt;

      &lt;p&gt;We will show that the second-order Talor approximation denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{D}(\mathbf{y},\tau_k)$&lt;/code&gt;  can be expressed as below.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{D}(\mathbf{y},\tau_k) := \underbrace{\mathrm{D}_f(\tau_k,\tau_k)}_{=0} + (\mathbf{y}-\tau_k)^T \underbrace{ [\nabla_y  \mathrm{D}_f(\mathbf{y},\tau_k) \big|_{y=\tau_k}]}_{=0} +\frac{1}{2} (\mathbf{y}-\tau_k)^T \underbrace{ [\nabla_y^2  \mathrm{D}_f(\mathbf{y},\tau_k)\big|_{y=\tau_k}]}_{ =\mathbf{F}_\tau(\tau_k) } (\mathbf{y}-\tau_k)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;For the zero-order term, we have
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{D}_f(\tau_k,\tau_k) &amp;amp; = \int f(\frac{p(w|{\tau_k})}{p(w|\tau_k)}) p(w|\tau_k) dw 
= \int \underbrace{f(1)}_{=0} p(w|\tau_k) dw =0.
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;For the first-order term, we have
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\nabla_y \mathrm{D}_f(\mathbf{y},\tau_k) &amp;amp;  =
\int \nabla_y f(\frac{p(w|\mathbf{y})}{p(w|\tau_k)}) p(w|\tau_k) dw \\
&amp;amp;=\int  \dot{f}\big(\frac{p(w|\mathbf{y})}{p(w|\tau_k)}\big) \frac{\nabla_y p(w|\mathbf{y})}{p(w|\tau_k)} p(w|\tau_k) dw \\
&amp;amp;=\int  \dot{f}\big(\frac{p(w|\mathbf{y})}{p(w|\tau_k)}\big) \nabla_y p(w|\mathbf{y}) dw.
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;Note that when $y=\tau_k$, we can simplify the expression since 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\dot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big)=\dot{f}(1)$&lt;/code&gt; is a constant.
Therefore,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\nabla_y \mathrm{D}_f(\mathbf{y},\tau_k) \big|_{y=\tau_k}  
=\int  \dot{f}(1) \nabla_y p(w|\mathbf{y}) \big|_{y=\tau_k} dw =  
\dot{f}(1)  \nabla_y \underbrace{\Big[ \int   p(w|\mathbf{y}) dw \Big]}_{=1} \Big|_{y=\tau_k} = 0.
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;For the second-order term, we have
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\nabla_{y}^2 \mathrm{D}_f(y,\tau_k) &amp;amp; = 
\int \nabla_{y}^2 f\big(\frac{p(w|y)}{p(w|\tau_k)}\big) p(w|\tau_k) dw  \\
&amp;amp;= \int \nabla_{y} [\dot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big) \frac{\nabla_y p(w|y)}{p(w|\tau_k)} ] p(w|\tau_k) dw \\
&amp;amp;= \int \Big[\ddot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big) \frac{\nabla_y p(w|y)}{p(w|\tau_k)} \frac{\nabla_y^T p(w|y)}{p(w|\tau_k)}+\dot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big) \frac{\nabla_y^2 p(w|y)}{p(w|\tau_k)} \Big] p(w|\tau_k) dw 
\end{aligned}
$$&lt;/code&gt;
Note that when $y=\tau_k$, we can simplify the expression since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\ddot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big)=\ddot{f}(1)=1$&lt;/code&gt; and 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\dot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big)=\dot{f}(1)$&lt;/code&gt; is a constant.&lt;/p&gt;

      &lt;p&gt;Therefore,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\nabla_{y}^2 \mathrm{D}_f(y,\tau_k) \Big|_{y=\tau_k} 
&amp;amp;= \int \Big[\ddot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big) \frac{\nabla_y p(w|y)}{p(w|\tau_k)} \frac{\nabla_y^T p(w|y)}{p(w|\tau_k)} +\dot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big) \frac{\nabla_y^2 p(w|y)}{p(w|\tau_k)} \Big] p(w|\tau_k) dw \Big|_{y=\tau_k} \\
&amp;amp;=\underbrace{\int  \frac{\nabla_y p(w|y)}{p(w|\tau_k)} \frac{\nabla_y^T p(w|y)}{p(w|\tau_k)} p(w|\tau_k) dw  \Big|_{y=\tau_k}}_{\text{Term I}} + \underbrace{\int \dot{f}(1) \frac{\nabla_y^2 p(w|y)}{p(w|\tau_k)}  p(w|\tau_k) dw  \Big|_{y=\tau_k}}_{\text{Term II}}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;Term II is zero since
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\int \dot{f}(1) \frac{\nabla_y^2 p(w|y)}{p(w|\tau_k)}  p(w|\tau_k) dw \Big|_{y=\tau_k}= \dot{f}(1) \int  \nabla_y^2 p(w|y) dw \Big|_{y=\tau_k}= \dot{f}(1) \nabla_y^2 \underbrace{\Big[\int   p(w|y) dw \Big] }_{=1} \Big|_{y=\tau_k}=0
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

      &lt;p&gt;Term I is the FIM since
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\int  \underbrace{ \frac{\nabla_y p(w|y)}{p(w|\tau_k)}}_{= \nabla_y \log p(w|y)}  \frac{\nabla_y^T p(w|y)}{p(w|\tau_k)} p(w|\tau_k) dw  \Big|_{y=\tau_k} &amp;amp;=
E_{p(w|\tau_k)} \Big[ \nabla_y \log p(w|y)   \nabla_y^T \log p(w|y) \Big] \Big|_{y=\tau_k} \\
&amp;amp;=
E_{p(w|\tau_k)} \Big[ \nabla_\tau \log p(w|\tau_k)   \nabla_\tau^T \log p(w|\tau_k) \Big] \\
&amp;amp;= \mathbf{F}_\tau(\tau_k)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;p&gt;By the claim, when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{D}(\mathbf{y},\tau_k)$&lt;/code&gt; is the second-order Taylor approximation of a
f-divergence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{D}_f(\mathbf{y},\tau_k)$&lt;/code&gt;  at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{y}=\tau_k$&lt;/code&gt;,
the unconstrained proximal-gradient update can be re-expressed as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \arg\min_{y \in  \mathcal{R}^K  } \{ \langle \mathbf{g}_{\tau_k}, \mathbf{y} \rangle   + \frac{1}{2\alpha} (\mathbf{y}-\tau_k)^T \mathbf{F}_\tau(\tau_k) (\mathbf{y}-\tau_k) \}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It is easy to see that we can obtain the following natural-gradient update from the above expression.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \underbrace{ \mathbf{F}^{-1}_\tau(\tau_k) \mathbf{g}_{\tau_k}}_{ =\hat{\mathbf{g}}_{\tau_k} }
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;Note&lt;/p&gt;

  &lt;p&gt;By using this Taylor approximation,
we essentially use the distance defined in a (Riemannian) gradient space to approximate the distance in a parameter space.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In &lt;a href=&quot;/posts/2021/12/Geomopt05/#natural-gradient-descent-as-unconstrained-mirror-descent&quot;&gt;Part V&lt;/a&gt;   , we will show that  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{D}(y,\tau_k)$&lt;/code&gt; can also be an exact KL divergence when $p(w)$ is an exponential family.
Under a particular parametrization,  natural-gradient descent also can be viewed as (unconstrained) mirror descent.&lt;/p&gt;

&lt;div class=&quot;notice--danger&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;The connection bewteen natural-gradient descent and proximal-gradient/mirror descent could break down in constrained cases. We will cover more about this point in &lt;a href=&quot;/posts/2021/12/Geomopt06/&quot;&gt;Part VI&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;h1 id=&quot;natural-gradient-descent-in-non-intrinsic-parameter-spaces&quot;&gt;Natural-gradient Descent in Non-intrinsic Parameter Spaces&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;As mentioned in &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;Part I&lt;/a&gt;, an intrinsic parametrization creates a nice parameter space (e.g., a local vector space structure) and guarantees a non-singular FIM.
We now discuss issues when it comes to  natural-gradient descent over non-intrinsic parametrizations including overparameterization.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;We may not have a local vector space structure in a non-intrinsic parameter space. Therefore, natural-gradient descent in this parameter space is pointless since the updated parameter will leave the parameter space. Indeed, the FIM could also be ill-defined in such cases. We will illustrate this by examples.&lt;/p&gt;

    &lt;div class=&quot;notice--info&quot;&gt;
      &lt;details&gt;
 &lt;summary&gt;Bernoulli Example: Invalid NGD (click to expand)&lt;/summary&gt;
 &lt;fieldset class=&quot;field-set&quot;&gt;
          &lt;blockquote&gt;

            &lt;p&gt;Consider Bernoulli family  $ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0, \pi_0+\pi_1=1  \}$ with parameter $\tau = (\pi_0,\pi_1)$.&lt;/p&gt;

            &lt;p&gt;As we shown in &lt;a href=&quot;/posts/2021/09/Geomopt01/#caveats-of-the-fisher-matrix-computation&quot;&gt;Part I&lt;/a&gt;, the FIM is ill-defined due to this eqaulity constraint.&lt;/p&gt;

            &lt;p&gt;Moreover, the NGD update will violate the eqaulity constraint.&lt;/p&gt;
          &lt;/blockquote&gt;
        &lt;/fieldset&gt;
 &lt;/details&gt;
    &lt;/div&gt;

    &lt;div class=&quot;notice--info&quot;&gt;
      &lt;details&gt;
 &lt;summary&gt;Von Mises–Fisher Example: Invalid NGD  (click to expand)&lt;/summary&gt;
 &lt;fieldset class=&quot;field-set&quot;&gt;
          &lt;blockquote&gt;

            &lt;p&gt;Consider $2$-dimensional &lt;a href=&quot;https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution&quot;&gt;Von Mises–Fisher family&lt;/a&gt;  $ \{p(\mathbf{w}|\tau):= C(\kappa) \exp(\kappa (w_1\mu_1+w_2\mu_2) ) \Big| \kappa&amp;gt;0, \mu_1^2+\mu_2^2 =1  \}$ with parameter $\tau = (\kappa,\mu_1,\mu_2)$, where $ C(\kappa)$ is the normalization constant, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\kappa$&lt;/code&gt; is a positive scalar, $\mathbf{w}=(w_1,w_2)$ is a random unit vector defined in a circle,
 and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\mu}=(\mu_1,\mu_2)$&lt;/code&gt; is also a unit vector.&lt;/p&gt;

            &lt;p&gt;We can show that the FIM is ill-defined under this parametrization due to this eqaulity constraint.&lt;/p&gt;
          &lt;/blockquote&gt;
        &lt;/fieldset&gt;
 &lt;/details&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;The FIM is singular in a non-intrinsic space. In theory, Moore–Penrose inverse could be used to compute natural-gradients so that natural-gradient descent is linearly invariant in this case. However, Moore–Penrose inverse often has to use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Singular_value_decomposition&quot;&gt;singular value decomposition&lt;/a&gt; (SVD) and  destroies structures of the FIM.  In practice, the iteration cost of Moore–Penrose inverse is very high as illustrated in the following example.
    &lt;div class=&quot;notice--info&quot;&gt;
      &lt;details&gt;
 &lt;summary&gt;Example: High iteration cost (click to expand)&lt;/summary&gt;
 &lt;fieldset class=&quot;field-set&quot;&gt;
          &lt;blockquote&gt;

            &lt;p&gt;Consider a $d$-dimensional Gaussian mixture family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \frac{1}{C} \sum_{k=1}^{C} \mathcal{N}(w |\mathbf{\mu}_k,  \mathbf{\Sigma}_k)  \Big|  \mathbf{\mu}_k \in \mathcal{R}^d,  \mathbf{\Sigma}_k  \succ \mathbf{0}  \}$&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \{ \mu_k,\Sigma_k\}_{k=1}^{C} $&lt;/code&gt;.&lt;/p&gt;

            &lt;p&gt;If we  use the following initialization such that all $C$ components have the same mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu_0$&lt;/code&gt; and the same covariance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\Sigma}_0$&lt;/code&gt;, this family becomes a Gaussian family.
In this case,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is a non-intrinsic parameterization for a Gaussian family. Note that the FIM is singular under
 parametrization $\tau$.
The iteration cost of natural-gradient descent in this parameter space $\Omega_\tau$ will be $O(C^3 d^6)$ if  Moore–Penrose inverse is employed.&lt;/p&gt;

            &lt;p&gt;Now, consider an equivalent Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mathbf{\mu},  \mathbf{\Sigma})  \Big|  \mathbf{\mu} \in \mathcal{R}^d,  \mathbf{\Sigma}  \succ \mathbf{0}  \}$&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda =( \mu,\mathrm{vech}(\mathbf{\Sigma}) ) $&lt;/code&gt;, where $\lambda$ is an intrinsic parameterization of the Gaussian family and initialized by 
mean $\mu_0$ and  covariance $\mathbf{\Sigma}_0$.&lt;/p&gt;

            &lt;p&gt;As we will show in &lt;a href=&quot;/posts/2021/12/Geomopt05/&quot;&gt;Part V&lt;/a&gt;, the iteration cost of natural-gradient descent in this parameter space $\Omega_\lambda$ will be $O(d^3)$ if we exploit structures of the exact non-singular FIM.&lt;/p&gt;
          &lt;/blockquote&gt;
        &lt;/fieldset&gt;
 &lt;/details&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;It is tempting to approximate the singular FIM by an emprical FIM with a scalar damping term and use Woodbury matrix identity to reduce the iteration cost of computing natural-gradients. However, sample-based emprical approximations could be problematic &lt;a class=&quot;citation&quot; href=&quot;#kunstner2019limitations&quot;&gt;[4]&lt;/a&gt;.
Moreover, damping introduces an additional tuning hyper-parameter and destories the linear invariance property of  natural-gradient descent. Such an approximation should be used with caution.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;euclidean-gradient-descent-is-not-linearly-invariant&quot;&gt;Euclidean Gradient Descent is NOT (Linearly) Invariant&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;For simplicity, consider an unconstrained optimization problem.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau \in \mathcal{R}^K } h_\tau(\tau)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Euclidean gradient descent (GD) in parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha {\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{5}\label{5}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\mathbf{g}}_{\tau_k} := \nabla_\tau h_\tau(\tau_k)$&lt;/code&gt; is a Euclidean gradient.&lt;/p&gt;

&lt;p&gt;Consider a reparametrization  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; so that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda=\mathbf{U} \tau$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{U}$&lt;/code&gt; is a constant (square) invertible matrix. 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\lambda \in \mathcal{R}^K } h_\lambda(\lambda):= h_\tau( \mathbf{U}^{-1} \lambda)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The Euclidean gradient descent (GD) in parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} - \alpha {\mathbf{g}}_{\lambda_k} 
\end{aligned}\tag{6}\label{6}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\mathbf{g}}_{\lambda_k} := \nabla_\lambda h_\lambda(\lambda_k)$&lt;/code&gt; is a Euclidean gradient.&lt;/p&gt;

&lt;p&gt;Note that Euclidean gradients follow the &lt;a href=&quot;/posts/2021/11/Geomopt03/#transformation-rules-for-natural-gradients-and-euclidean-gradients&quot;&gt;transformation rule&lt;/a&gt;  as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\tau^T =  \mathbf{g}_\lambda^T \mathbf{J}
\end{aligned}
$$&lt;/code&gt; where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$J_{ki}:=\frac{\partial \lambda^k(\tau) }{ \partial \tau^i }$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We can verify that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{J}=\mathbf{U}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\tau = \mathbf{U}^T \mathbf{g}_\lambda $&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Notice that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0 = \mathbf{U}^{-1} \lambda_0$&lt;/code&gt; by construction.
The update in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt; at iteration $k=1$ then can be re-expressed as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{1} \leftarrow \tau_{0} -  \alpha  {\mathbf{g}}_{\tau_0} = \mathbf{U}^{-1} \lambda_0 -  \alpha  \mathbf{U}^{T}  {\mathbf{g}}_{\lambda_0} \neq \mathbf{U}^{-1} \lambda_1
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It is easy to see that
updates in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt; are NOT equivalent.
Therefore,  Euclidean gradient descent is not invariant.&lt;/p&gt;

&lt;h1 id=&quot;newtons-method-is-linearly-invariant&quot;&gt;Newton’s Method is Linearly Invariant&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;For simplicity, consider an unconstrained convex optimization problem.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau \in \mathcal{R}^K } h_\tau(\tau)
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\tau(\tau)$&lt;/code&gt; is strongly convex and twice continuously differentiable w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Newton’s method under parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \mathbf{H}^{-1}_\tau(\tau_k) {\mathbf{g}}_{\tau_k} 
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\mathbf{g}}_{\tau_k} := \nabla_\tau h_\tau(\tau_k)$&lt;/code&gt; is a Euclidean gradient and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{H}_\tau(\tau_k):=\nabla_\tau^2 h_\tau(\tau_k)$&lt;/code&gt; is the Hessian.&lt;/p&gt;

&lt;p&gt;Consider a reparametrization  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; so that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda=\mathbf{U} \tau$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{U}$&lt;/code&gt; is a constant (square) invertible matrix. 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\lambda \in \mathcal{R}^K } h_\lambda(\lambda):= h_\tau( \mathbf{U}^{-1} \lambda)
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\lambda(\lambda)$&lt;/code&gt; is also strongly convex w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; due to  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Newton’s method under parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} - \alpha \mathbf{H}^{-1}_\lambda(\lambda_k) {\mathbf{g}}_{\lambda_k} 
\end{aligned} 
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\mathbf{g}}_{\lambda_k} := \nabla_\lambda h_\lambda(\lambda_k)$&lt;/code&gt; is a Euclidean gradient and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{H}_\tau(\lambda_k):=\nabla_\lambda^2 h_\lambda(\lambda_k)$&lt;/code&gt; is the Hessian.&lt;/p&gt;

&lt;p&gt;As we discussed in the previous section, 
Euclidean gradients follow the &lt;a href=&quot;/posts/2021/11/Geomopt03/#transformation-rules-for-natural-gradients-and-euclidean-gradients&quot;&gt;transformation rule&lt;/a&gt;  as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\tau^T =  \mathbf{g}_\lambda^T \mathbf{J}$&lt;/code&gt;, where
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{J}=\mathbf{U}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Surprisingly, for a linear transformation, the Hessian follows the &lt;a href=&quot;/posts/2021/11/Geomopt03/#transformation-rule-for-the-fisher-information-matrix&quot;&gt;transformation rule&lt;/a&gt;  like the FIM as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{H}_{\tau} (\tau_k) &amp;amp;= \nabla_\tau ( \mathbf{g}_{\tau_k} ) \\
&amp;amp;=\nabla_\tau ( \mathbf{J}^T \mathbf{g}_{\lambda_k} ) \\
&amp;amp;=\mathbf{J}^T\nabla_\tau (  \mathbf{g}_{\lambda_k} ) + \underbrace{[\nabla_\tau \mathbf{J}^T ]}_{=0} \mathbf{g}_{\lambda_k}  \,\,\,\,\text{(In linear cases, } \mathbf{J} = \mathbf{U} \text{ is a
constant)}   \\
&amp;amp;=\mathbf{J}^T [ \nabla_\lambda (  \mathbf{g}_{\lambda_k} ) ] \mathbf{J} \\ 
&amp;amp;=\mathbf{J}^T \mathbf{H}_{\lambda} (\lambda_k)\mathbf{J} 
\end{aligned}\tag{7}\label{7}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Therefore, the direction in Newton’s method denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tilde{\mathbf{g}}_{\tau_k} := \mathbf{H}^{-1}_\tau(\tau_k) \mathbf{g}_{\tau_k}$&lt;/code&gt; is transformed like natural-gradients in &lt;strong&gt;linear&lt;/strong&gt; cases as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\tilde{\mathbf{g}}_{\tau_k} &amp;amp;:= \mathbf{H}^{-1}_\tau(\tau_k) \mathbf{g}_{\tau_k} \\
&amp;amp;= [ \mathbf{J}^T \mathbf{H}_{\lambda} (\lambda_k)\mathbf{J} ]^{-1} \mathbf{g}_{\tau_k} \\
&amp;amp;=  \mathbf{J}^{-1} \mathbf{H}^{-1}_{\lambda} (\lambda_k)\mathbf{J}^{-T} [ \mathbf{J}^{T}\mathbf{g}_{\lambda_k} ] \\
&amp;amp;=  \mathbf{J}^{-1} \mathbf{H}^{-1}_{\lambda} (\lambda_k) \mathbf{g}_{\lambda_k}  \\
&amp;amp;=  \mathbf{J}^{-1}  \tilde{\mathbf{g}}_{\lambda_k}  \\
&amp;amp;=  \mathbf{Q}  \tilde{\mathbf{g}}_{\lambda_k}  \\
\end{aligned} 
$$&lt;/code&gt; where by the definition we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{Q}= \mathbf{J}^{-1}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The consequence is that Newton’s method like natural-gradient descent is linearly invariant.&lt;/p&gt;

&lt;h2 id=&quot;the-hessian-is-not-a-valid-manifold-metric&quot;&gt;The Hessian is not a valid manifold metric&lt;/h2&gt;

&lt;p&gt;The Hessian &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{H}_\tau(\tau_k)=\nabla_\tau^2 h_\tau(\tau_k)$&lt;/code&gt;  in general is not a valid manifold metric since it does not follow the transformation
rule of a metric in non-linear cases.&lt;/p&gt;

&lt;p&gt;Contrastingly, the FIM is a valid manifold metric. Recall that the FIM can also be computed
as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_\tau(\tau) = E_{p(w|\tau)}\big[ -\nabla_\tau^2 \log p(w|\tau) \big]$&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Claim&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;The FIM follows the transformation
rule even in non-linear cases.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;
Proof of the claim (click to expand)
&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;
        &lt;p&gt;Proof&lt;/p&gt;

        &lt;p&gt;Given a non-linear intrinsic reparametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt;, recall that the Jacobian matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{J}(\tau_k)$&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; is no longer a constant matrix but a square and non-singular matrix.
In this case, the FIM still follows the &lt;a href=&quot;/posts/2021/11/Geomopt03/#transformation-rule-for-the-fisher-information-matrix&quot;&gt;transformation rule&lt;/a&gt; thanks to &lt;a href=&quot;https://en.wikipedia.org/wiki/Score_(statistics)#Mean&quot;&gt;the expectation of the score function&lt;/a&gt;.&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\require{cancel}
\begin{aligned}
&amp;amp; \mathbf{F}_\tau(\tau_k)\\
=&amp;amp; E_{p(w|\tau_k)}\big[ -\nabla_\tau^2 \log p(w|\tau_k) \big]  \\
=&amp;amp; E_{p(w|\lambda_k)}\big[ -\nabla_\tau^2 \log p(w|\tau_k) \big]  \\
=&amp;amp; E_{p(w|\lambda_k)}\big[ -\nabla_\tau [ \mathbf{J}^T(\tau_k) \nabla_\lambda \log p(w|\lambda_k)]  \big]  \\
=&amp;amp; -    \mathbf{J}^T(\tau_k)   E_{p(w|\lambda_k)}\big[  \nabla_\tau  \nabla_\lambda \log p(w|\lambda_k) \big]   - [\nabla_\tau \mathbf{J}^T(\tau_k)  ]  \underbrace{  \cancelto{=0}{E_{p(w|\lambda_k)}\big[  \nabla_\lambda \log p(w|\lambda_k) \big]}  }_{ \text{ (the expectation of the score is zero)}  }   \\
=&amp;amp; -    \mathbf{J}^T(\tau_k)   E_{p(w|\lambda_k)}\big[  [\nabla_\lambda^2 \log p(w|\lambda_k) ] \mathbf{J }(\tau_k) \big] \\
=&amp;amp; -    \mathbf{J}^T(\tau_k)    E_{p(w|\lambda_k)}\big[  \nabla_\lambda^2 \log p(w|\lambda_k) \big] \mathbf{J}(\tau_k) \\
=&amp;amp;\mathbf{J }^T(\tau_k)  \mathbf{F}_\lambda(\lambda_k) \mathbf{J}(\tau_k)  
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;p&gt;We will discuss in
&lt;a href=&quot;/posts/2021/12/Geomopt05/#minimal-parametrizations-of-exponential-family&quot;&gt;Part V&lt;/a&gt;, for a special parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; (known as a natural parametrization) of &lt;a href=&quot;/posts/2021/12/Geomopt05/#exponential-family&quot;&gt;exponential family&lt;/a&gt;, the FIM under this parametrization can be computed  as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_\tau(\tau) = \nabla_\tau^2 A_\tau(\tau)$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$A_\tau(\tau)$&lt;/code&gt; is a strictly convex function.&lt;/p&gt;

&lt;p&gt;In the literature, the exponential family with a natural parametrization is known as a Hessian manifold &lt;a class=&quot;citation&quot; href=&quot;#shima2007geometry&quot;&gt;[5]&lt;/a&gt;, where the FIM
under this kind of parametrization is called a Hessian metric.
However, a non-linear reparametrization will lead to a non-natural parametrization.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p class=&quot;bibliography&quot;&gt;&lt;p&gt;&lt;span id=&quot;amari1998natural&quot;&gt;[1] S.-I. Amari, &quot;Natural gradient works efficiently in learning,&quot; &lt;i&gt;Neural computation&lt;/i&gt; &lt;b&gt;10&lt;/b&gt;:251–276 (1998).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;atkinson1981rao&quot;&gt;[2] C. Atkinson &amp;amp; A. F. S. Mitchell, &quot;Rao’s distance measure,&quot; &lt;i&gt;Sankhyā: The Indian Journal of Statistics, Series A&lt;/i&gt; 345–365 (1981).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;khan2016faster&quot;&gt;[3] M. E. Khan, R. Babanezhad, W. Lin, M. Schmidt, &amp;amp; M. Sugiyama, &quot;Faster stochastic variational inference using Proximal-Gradient methods with general divergence functions,&quot; &lt;i&gt;Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence&lt;/i&gt; (2016), pp. 319–328.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;kunstner2019limitations&quot;&gt;[4] F. Kunstner, P. Hennig, &amp;amp; L. Balles, &quot;Limitations of the empirical Fisher approximation for natural gradient descent,&quot; &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt; &lt;b&gt;32&lt;/b&gt;:4156–4167 (2019).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;shima2007geometry&quot;&gt;[5] H. Shima, &lt;i&gt;The geometry of Hessian structures&lt;/i&gt; (World Scientific, 2007).&lt;/span&gt;&lt;/p&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes:&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In Riemannian geometry, a geodesic is induced by the Levi-Civita connection. This connection is known as the metric compatiable parallel transport. Christoffel symbols are used to represent the connection in a coordinate/parametrization system. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><summary type="html">Warning: working in Progress (incomplete)</summary></entry><entry><title type="html">Part III: Invariance of Natural-Gradients</title><link href="/posts/2021/11/Geomopt03/" rel="alternate" type="text/html" title="Part III: Invariance of Natural-Gradients" /><published>2021-11-02T00:00:00-07:00</published><updated>2021-11-02T00:00:00-07:00</updated><id>/posts/2021/11/Geomopt03</id><content type="html" xml:base="/posts/2021/11/Geomopt03/">&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post should help readers to understand the invariance of natural-gradients.
We will also discuss why the Euclidean steepest direction is NOT invariant.&lt;/p&gt;

&lt;p&gt;We will give an informal introduction with a focus on high level of ideas.&lt;/p&gt;

&lt;h1 id=&quot;parameter-transformation-and-invariance&quot;&gt;Parameter Transformation and Invariance&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;In &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt;, we have shown that a Riemannian gradient is a parametric representation of the tangent direction of a curve in a  manifold.
Since a curve and its tangent direction are geometric obejects, they should be invariant to the choice of parametrization.
In other words, geometric properties should be persevered in any valid coordinate system. This is a coordinate-free argument.&lt;/p&gt;

&lt;p&gt;The argument could be abstract for beginners. To be more concrete, we consider the corresponding coordinate-dependent argument: geometric properties should remain unchanged if we perform a (valid) parameter transformation. This coordinate-dependent argument also gives us a rule to compute natrual-gradients under a parameter transformation.&lt;/p&gt;

&lt;p&gt;The transformation rules are summarized in the following table&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Type of  directions&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;transformation rules&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Euclidean gradient&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Euclidean steepest descent direction (normalized Euclidean gradient)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;N/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Riemannian gradient&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Riemannian steepest descent direction (normalized Riemannian gradient)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The following example illustrates these transformation rules.&lt;/p&gt;
&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Univariate Gaussian example: (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;
        &lt;p&gt;Consider the following scalar function
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
h_\tau(\tau):= E_{q(w|\tau)} [ w^2 + \log q(w|\tau) ]
= \mu^2 + \frac{1}{s} + \frac{1}{2} \log(s)- \frac{1}{2}(1+\log(2\pi))
\end{aligned}
$$&lt;/code&gt;
where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)= \mathcal{N}(w|\mu,s^{-1})$&lt;/code&gt; is a Gaussian family with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt;, variance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$s^{-1}$&lt;/code&gt;, 
  intrinsic parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=(\mu,s)$&lt;/code&gt;, and parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau=\{(\mu,s)|\mu \in \mathcal{R},s&amp;gt;0 \}$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;The FIM of Gaussian $q(w|\tau)$ under parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}_\tau (\tau) := -E_{q(w|\tau)} [ \nabla_\tau^2 \log q(w|\tau) ] 
=
\begin{bmatrix}
s &amp;amp; 0 \\
0 &amp;amp; \frac{1}{2s^2}
\end{bmatrix}
\end{aligned}
$$&lt;/code&gt;
We consider a member $\tau_0=(0.5,0.5)$ in the Gaussian family.
The Euclidean gradient is 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\tau (\tau_0) :=
\nabla_\tau h_\tau(\tau_0) =
\begin{bmatrix}
2 \mu \\
-\frac{1}{s^2} +\frac{1}{2s}
\end{bmatrix}_{\tau=\tau_0}
=\begin{bmatrix}
1 \\ -3
\end{bmatrix}
\end{aligned}
$$&lt;/code&gt;
The natural/Riemannian gradient is 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\hat{\mathbf{g}}_\tau (\tau_0) :=
\mathbf{F}_\tau^{-1} (\tau_0) \nabla_\tau h_\tau(\tau_0) =
\begin{bmatrix}
2 \mu s^{-1}  \\
( -\frac{1}{s^2} +\frac{1}{2s} ) (2s^2)
\end{bmatrix}_{\tau=\tau_0}
=\begin{bmatrix}
2 \\ -\frac{3}{2}
\end{bmatrix}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;Now, consider the following re-parametrization of the function&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
h_\lambda(\lambda)= E_{q(w|\lambda)} [ w^2 + \log q(w|\lambda)]
= \mu^2 + v - \frac{1}{2} \log(v) - \frac{1}{2}(1+\log(2\pi))
\end{aligned}
$$&lt;/code&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\lambda)= \mathcal{N}(w|\mu,v)$&lt;/code&gt; with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt; and variance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$v=s^{-1}$&lt;/code&gt;,
  intrinsic parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda=(\mu,v)$&lt;/code&gt;, and parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\lambda=\{(\mu,v)|\mu \in \mathcal{R},v&amp;gt;0 \}$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;The FIM of Gaussian $q(w|\lambda)$ under parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}_\lambda (\lambda):= -E_{q(w|\lambda)} [ \nabla_\lambda^2 \log q(w|\lambda) ] 
=
\begin{bmatrix}
\frac{1}{v} &amp;amp; 0 \\
0 &amp;amp; \frac{1}{2v^2}
\end{bmatrix}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;The Jacobian matrix is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{J} :=\frac{\partial \lambda(\tau)}{\partial \tau} = 
\begin{bmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; -\frac{1}{s^2}
\end{bmatrix}
\end{aligned}
$$&lt;/code&gt; where $\lambda(\tau)=(\mu,v)=(\mu,\frac{1}{s})$ and $\tau=(\mu,s)$.&lt;/p&gt;

        &lt;p&gt;We can verify that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{8}$&lt;/code&gt; holds for the FIM.&lt;/p&gt;

        &lt;p&gt;Consider the same member $\lambda_0=(0.5,2)$ in the Gaussian family.
The Euclidean gradient is 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\lambda (\lambda_0) :=
\nabla_\lambda h_\lambda(\lambda_0) =
\begin{bmatrix}
2 \mu \\
1 - \frac{1}{2v}
\end{bmatrix}_{\lambda=\lambda_0}
=\begin{bmatrix}
1 \\ \frac{3}{4}
\end{bmatrix} 
\end{aligned}
$$&lt;/code&gt;
We can verify that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt; holds for the Euclidean gradient.&lt;/p&gt;

        &lt;p&gt;The natural/Riemannian gradient is 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\hat{\mathbf{g}}_\lambda (\lambda_0) :=
\mathbf{F}_\lambda^{-1} (\lambda_0) \nabla_\lambda h_\lambda(\lambda_0) =
\begin{bmatrix}
2 \mu  v \\
( 1 - \frac{1}{2v} ) (2v^2)
\end{bmatrix}_{\lambda=\lambda_0}
=\begin{bmatrix}
2 \\ 6
\end{bmatrix}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;We can verify that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; holds for the Riemannian gradient&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;We will show that two key &lt;strong&gt;geometric properties&lt;/strong&gt; remains the same under any &lt;strong&gt;intrinsic&lt;/strong&gt; parameter transformation.&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;Directional derivative&lt;/li&gt;
    &lt;li&gt;Length of a Riemannian vector/gradient induced by the Fisher-Rao metric&lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;

&lt;p&gt;Thanks to these properties, we will show that the optimal solution of the &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-steepest-direction&quot;&gt;Riemannian steepest direction&lt;/a&gt; considered in Part II is equivalent under an intrinsic parameter transformation. This is in contrast with the Euclidean steepest direction which is not invaraint under an intrinsic parameter transformation.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt;, we consider a point $\mathbf{x}_0$ in a manifold $\mathcal{M}$, a (1-dimensional) curve $\gamma(t)$, and a smooth scalar function $h: \mathcal{M} \to \mathcal{R}$.
Given an intrinsic parametrization $\tau$ containing the point, we consider the following parametric representations.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;geometric objects&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;parametric representations&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;curve  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma_\tau(t) $&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;function  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h(x_0)$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\tau(\tau_0) $&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;transformation-rules-for-natural-gradients-and-euclidean-gradients&quot;&gt;Transformation Rules for Natural Gradients and Euclidean Gradients&lt;/h2&gt;

&lt;p&gt;Intuitively, the following identity should hold for any two (intrinsic) parametrizations $\tau$ and $\lambda$.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
h(\gamma(t)) = h_\tau(\gamma_\tau(t)) = h_\lambda(\gamma_\lambda(t))
\end{aligned}
$$&lt;/code&gt; where we consider $t$ to be fixed.&lt;/p&gt;

&lt;p&gt;Technically speaking,  domain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau$&lt;/code&gt; of curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma_\tau(t)$&lt;/code&gt; and  domain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\lambda$&lt;/code&gt; of curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma_\lambda(t)$&lt;/code&gt; may be different.
For simplicity, we assume both domains are open intervals containing 0.
In other words, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma_\tau(0)=\tau_0$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma_\lambda(0)=\lambda_0$&lt;/code&gt;  are parametric representations of the same point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;From the above expression, we can see that directional derivatives should be the same at $t=0$
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} = \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} 
\end{aligned}\tag{1}\label{1}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt;, we have shown that 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &amp;amp;=  [\nabla h_\tau(\mathbf{\tau}_0)  ]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}   \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} &amp;amp; = [\nabla h_\lambda(\mathbf{\lambda}_0) ]^T  \frac{d {\gamma}_\lambda(t) }{d t} \Big|_{t=0}  
\end{aligned}
$$&lt;/code&gt; where $\nabla$ is the standard (coordinate) derivative.&lt;/p&gt;

&lt;p&gt;Recall that in &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt;,  we have shown that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$&lt;/code&gt; is a  parametric representation of a tangent vector, which is a Riemannian gradient.
Notice that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla h_\lambda(\mathbf{\lambda}_0)$&lt;/code&gt; is a Euclidean gradient&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;We will use the following notations to simplify expressions.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Notations&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Meanings&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Euclidean gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(g_\tau)_i$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$i$-th entry  under parametrization $\tau$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Riemannian gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(\hat{g}_\tau)^j$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$j$-th entry under parametrization $\tau$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau^j$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$j$-th parameter under parametrization   $\tau$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Using these notations, the derivational derivatives then can be re-expressed as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &amp;amp;= \sum_{i}  (g_\tau)_i  (\hat{g}_\tau)^i  = \mathbf{g}_\tau^T \hat{\mathbf{g}}_\tau \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} &amp;amp; =\sum_{i}  (g_\lambda)_i (\hat{g}_\lambda)^i =  {\mathbf{g}}_\lambda^T \hat{\mathbf{g}}_\lambda
\end{aligned}\tag{2}\label{2}
$$&lt;/code&gt;
where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\lambda$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\tau$&lt;/code&gt; are Euclidean gradients (e.g.,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\tau=\nabla h_\tau(\tau_0) $&lt;/code&gt;)  while  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\lambda$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\tau$&lt;/code&gt;  are  Riemannian gradients (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\tau=\frac{d \gamma_\tau(0) }{d t}$&lt;/code&gt;) .&lt;/p&gt;

&lt;p&gt;By &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt;, we have the following identity obtained from the &lt;strong&gt;geometric property&lt;/strong&gt; of directional derivatives.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \mathbf{g}_\tau^T \hat{\mathbf{g}}_\tau  =  \mathbf{g}_\lambda^T \hat{\mathbf{g}}_\lambda 
\end{aligned}\tag{3}\label{3}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now, we discuss the parameter transformation between $\tau$ and $\lambda$, where we can express $\lambda$ in terms of $\tau$ denoted by $\lambda(\tau)$.&lt;/p&gt;

&lt;p&gt;By the (standard) chain rule for a Euclidean gradient&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, we has
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
(g_\tau)_i = \sum_{k}  \color{red}{ (g_\lambda)_k} \frac{\color{red}{ \partial \lambda^k(\tau) }}{ \partial \tau^i } 
\end{aligned}
\tag{4}\label{4}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$J_{ki}:=\frac{\partial \lambda^k(\tau) }{ \partial \tau^i }$&lt;/code&gt; denotes the $(k,i)$ entry of the Jacobian matrix. We illustrate our matrix notation in a 2D case as below.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\begin{matrix}
&amp;amp; \\
\mathbf{J} = 
    \left ( \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    i=1 &amp;amp; i=2 \\ \hline
    J_{11} &amp;amp; J_{12}  \\
   J_{21} &amp;amp; J_{22} \\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
&amp;amp; \\
\left . \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right )
    \begin{matrix}
    k=1 \\ k=2 
    \end{matrix}
\end{matrix}
\end{aligned}\tag{5}\label{5}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; gives us the transformation rule for Eulcidean gradients (denoted by a row vector)  as below in a vector form.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\tau^T =  \mathbf{g}_\lambda^T \mathbf{J}
\end{aligned}\tag{6}\label{6},
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;Note:
&lt;span style=&quot;color:blue&quot;&gt;&lt;strong&gt;row&lt;/strong&gt;&lt;/span&gt; vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\mathbf{g}}_\tau^T$&lt;/code&gt; can be computed via a vector-Jacobian product in any standard Auto-Diff toolbox given that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\mathbf{g}}_\lambda$&lt;/code&gt; is pre-computed.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;By Eq &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt;, we obtain the transformation rule for Riemannian gradients  (denoted by a column vector) as below,  where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{Q}:=\mathbf{J}^{-1}$&lt;/code&gt; is also a Jacobian matrix and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$Q_{ki}=\frac{\partial \tau^k(\lambda)}{\partial \lambda^i}$&lt;/code&gt; is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(k,i)$&lt;/code&gt; entry of matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{Q}$&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\hat{\mathbf{g}}_\tau= \mathbf{J}^{-1}  \hat{\mathbf{g}}_\lambda 
\end{aligned}\tag{7}\label{7}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;Note:
&lt;span style=&quot;color:blue&quot;&gt;&lt;strong&gt;column&lt;/strong&gt;&lt;/span&gt; vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\tau$&lt;/code&gt; can be computed via a Jacobian-vector product used in forward-mode differentiation  &lt;a class=&quot;citation&quot; href=&quot;#lin2021tractable&quot;&gt;[1]&lt;/a&gt; &lt;a class=&quot;citation&quot; href=&quot;#salimbeni2018natural&quot;&gt;[2]&lt;/a&gt; given that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\lambda$&lt;/code&gt; is pre-computed.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The elementwise expression of the transformation rule for Riemannian gradients is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
(g_\tau)^k
 = \sum_{i}  \frac{ \partial \tau^k(\lambda)  }{\color{red} {\partial  \lambda^i} }  \color{red} {(g_\lambda)^i}
\end{aligned},
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that these transformation rules are valid  when the Jacobian matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{J}$&lt;/code&gt; is square and non-singular.
As we discussed in Part I about &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;intrinsic parameterizations&lt;/a&gt;, the transformation map between $\tau$ and $\lambda$ must be bi-jective, which implies the Jacoabian matrix is square. 
Moreover, the map and its inverse map should be smooth, which implies that the Jacobian matrix is well-defined and non-singular.&lt;/p&gt;

&lt;h2 id=&quot;transformation-rule-for-the-fisher-information-matrix&quot;&gt;Transformation Rule for the Fisher Information Matrix&lt;/h2&gt;

&lt;p&gt;Now, we discuss a transformation rule for the Fisher information matrix (FIM) as defined at &lt;a href=&quot;/posts/2021/09/Geomopt01/#fisher-rao-metric&quot;&gt;Part I&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log p(w|\tau) \Big) ]
\end{aligned}
$$&lt;/code&gt;
Since $ \log p(w|\tau )$ can be considered as a scalar function $h$ defined on the manifold for any valid $w$, we have 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \log p(w|\tau_0  ) = h_\tau(\tau_0) = h_\lambda(\lambda_0) =  \log p(w|\lambda_0  )
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Thus, the FIM can be computed as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 F_{ij}(\tau_0) &amp;amp;= E_{p(w|\tau_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log p(w|\tau_0) \Big) ] \\
&amp;amp;=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log p(w|\tau_0) \Big) ]\\
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Recall that by the standard chain rule, we have 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\tau_i} \log p(w|\tau_0 ) = \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \Big( \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Moreover, the Jacobian matrix does not depent on $w$. Therefore, we have
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 F_{ij}(\tau_0) 
&amp;amp;=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log p (w|\tau_0) \Big) ]\\
&amp;amp;=  E_{p(w|\lambda_0) }  [ \Big( \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \sum_l \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&amp;amp;= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  E_{p(w|\lambda_0) }  [ \Big(  \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&amp;amp;= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } F_{kl}(\lambda_0)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We can re-express the above expression in a matrix form as below. This is the transformation rule for the FIM.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}_{\tau} (\tau_0) = \underbrace{\mathbf{J}^T}_{  \frac{ \partial \lambda^i(\tau_0) }{ \partial \tau^k }  } \mathbf{F}_{\lambda} (\lambda_0) \underbrace{\mathbf{J}}_{  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  } 
\end{aligned}\tag{8}\label{8}
$$&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;By using this transformation rule, we can show that another &lt;strong&gt;geometric property&lt;/strong&gt;: the length of a Riemannian vector is preserved.&lt;/p&gt;

&lt;p&gt;We can see that the length of a Riemannian vector is also invariant.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\| \hat{\mathbf{g}}_\tau \|^2_{F_{\tau_0}} &amp;amp;= [\hat{\mathbf{g}}_\tau]^T \mathbf{F}_{\tau} (\tau_0) \hat{\mathbf{g}}_\tau \\
&amp;amp;= [\mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda]^T \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda \\
&amp;amp;= [\hat{\mathbf{g}}_\lambda]^T [ \mathbf{J}^{-T}  \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} ] \hat{\mathbf{g}}_\lambda \\
&amp;amp;= [\hat{\mathbf{g}}_\lambda]^T  \mathbf{F}_{\lambda} (\lambda_0)  \hat{\mathbf{g}}_\lambda = \| \hat{\mathbf{g}}_\lambda \|^2_{F_{\lambda_0}}
\end{aligned}\tag{9}\label{9}
$$&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&quot;riemannian-steepest-direction-is-invariant&quot;&gt;Riemannian Steepest Direction is Invariant&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Now, we can show that the optimal solution of &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-steepest-direction&quot;&gt;Riemannian steepest direction&lt;/a&gt; considered in Part II  under parametrization $\tau$ and $\lambda$ are equivalent since both the length and the directional derivative remain the same.&lt;/p&gt;

&lt;p&gt;Denote Euclidean gradients as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\lambda:= \nabla  f_\lambda(\mathbf{\lambda}_0) $&lt;/code&gt; and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\tau:= \nabla  f_\tau(\mathbf{\tau}_0)  = \nabla  f_\lambda(\mathbf{\lambda}(\tau_0)) $&lt;/code&gt;, which follows the parameter transformation rule in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now, consider natural/Riemannian gradients as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\lambda:= \mathbf{F}_{\lambda}^{-1}(\mathbf{\lambda}_0)  \mathbf{g}_\lambda $&lt;/code&gt; and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_\tau:= \mathbf{F}_{\tau}^{-1}(\mathbf{\tau}_0) \mathbf{g}_\tau $&lt;/code&gt;. These Riemannian gradients follow the parameter transformation rule in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; as shown below.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\hat{\mathbf{g}}_\tau &amp;amp;= \mathbf{F}_{\tau}^{-1}(\mathbf{\tau}_0) \mathbf{g}_\tau \\
&amp;amp;= \big( \mathbf{J}^T  \mathbf{F}_{\lambda} (\mathbf{\lambda}_0) \mathbf{J} \big)^{-1} ( \mathbf{g}^T_\tau )^T &amp;amp; ( \text{by } \eqref{8} )\\
&amp;amp;=  \mathbf{J}^{-1} \mathbf{F}_{\lambda}^{-1} (\mathbf{\lambda}_0) \mathbf{J}^{-T}   ( \mathbf{g}^T_\lambda \mathbf{J} )^T  &amp;amp; ( \text{by } \eqref{6} ) \\
&amp;amp;=  \mathbf{J}^{-1} \mathbf{F}_{\lambda}^{-1} (\mathbf{\lambda}_0) \mathbf{J}^{-T}  (  \mathbf{J}^T  \mathbf{g}_\lambda  ) \\
&amp;amp;=  \mathbf{J}^{-1} \mathbf{F}_{\lambda}^{-1} (\mathbf{\lambda}_0)   \mathbf{g}_\lambda   \\
&amp;amp;=  \mathbf{J}^{-1}   \hat{\mathbf{g}}_\lambda
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Recall that the optimal solution of the Riemannian steepest direction is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{v}_{\lambda}^{(opt)}= -\frac{ \mathbf{F_\lambda}^{-1}(\mathbf{\lambda}_0) \nabla_\lambda f(\mathbf{\lambda}_0) }{\| \mathbf{F_\lambda}^{-1}(\mathbf{\lambda}_0)\nabla_\lambda f(\mathbf{\lambda}_0) \|_{F_{\lambda_0}}} = -\frac{\hat{\mathbf{g}}_\lambda}{\|\hat{\mathbf{g}}_\lambda\|_{ F_{\lambda_0} } } \\
\mathbf{v}_{\tau}^{(opt)}= -\frac{ \mathbf{F_\tau}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0) }{\| \mathbf{F_\tau}^{-1}(\mathbf{\tau}_0)\nabla_\tau f(\mathbf{\tau}_0) \|_{F_{\tau_0}}} = -\frac{\hat{\mathbf{g}}_\tau}{\|\hat{\mathbf{g}}_\tau\|_{ F_{\tau_0} } } 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We can easily verify the following identities since
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \|\hat{\mathbf{g}}_\lambda\|_{ F_{\lambda_0} } =  \|\hat{\mathbf{g}}_\tau\|_{ F_{\tau_0} }  $&lt;/code&gt; as shown in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{9}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\tau^T   \mathbf{v}^{(opt)}_{\tau}   &amp;amp; =   \mathbf{g}_\lambda^T  \mathbf{v}^{(opt)}_{\lambda} &amp;amp; \,\,\, \text{(invariance of a directional derivative)}  \\
\|  \mathbf{v}^{(opt)}_{\tau} \|^2_{\color{red}{F_{\tau_0}}} &amp;amp; = \|  \mathbf{v}^{(opt)}_{\lambda} \|^2_{\color{red}{F_{\lambda_0}} } &amp;amp; \,\,\, \text{(invariance of the length of the Riemannian steepest direction)}  \\
\mathbf{v}^{(opt)}_{\tau} &amp;amp; = \mathbf{J}^{-1}  \mathbf{v}^{(opt)}_{\lambda} &amp;amp; \,\,\, \text{(transformation rule for the Riemannian steepest direction)}  
\end{aligned}.
$$&lt;/code&gt; 
In other words, the Riemannian steepest direction (which is indeed  a normalized Riemannian gradient) is also transformed according to the transformation rule for Riemannian gradients in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As we will discuss in &lt;a href=&quot;/posts/2021/11/Geomopt04/#natural-gradient-descent-is-linearly-invariant&quot;&gt;Part IV&lt;/a&gt;, this invariance property implies that natural-gradient descent is linearly invariant.&lt;/p&gt;

&lt;h1 id=&quot;euclidean-steepest-direction-is-not-invariant&quot;&gt;Euclidean Steepest Direction is NOT Invariant&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Recall that we have shown that a Euclidean gradient is the optimal solution of  &lt;a href=&quot;/posts/2021/10/Geomopt02/#euclidean-steepest-direction-and directional-derivative&quot;&gt;Euclidean steepest direction&lt;/a&gt; in Part II.&lt;/p&gt;

&lt;p&gt;We can show that the (standard) length of a Euclidean gradient is NOT invariant under a parameter transformation due to the Jacobian matrix (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\| \mathbf{g}_\tau \| \neq \| \mathbf{g}_\lambda \|$&lt;/code&gt;). This is a reason why we use the &lt;a href=&quot;/posts/2021/10/Geomopt02/#distance-induced-by-the-fisher-rao-metric&quot;&gt;weighted inner product&lt;/a&gt; to define the length of a gradient vector.&lt;/p&gt;

&lt;p&gt;Note that we denote the Euclidean gradients as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\lambda:= \nabla  f_\lambda(\mathbf{\lambda}_0) $&lt;/code&gt; and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\tau:= \nabla  f_\tau(\mathbf{\tau}_0)  = \nabla  f_\lambda(\mathbf{\lambda}(\tau_0)) $&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Recall that the optimal solution of the &lt;a href=&quot;/posts/2021/10/Geomopt02/#euclidean-steepest-direction-and-directional-derivative&quot;&gt;Euclidean steepest direction&lt;/a&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{v}_{\lambda}^{(opt)}= -\frac{\nabla_\lambda f_\lambda(\mathbf{\lambda}_0) }{\|\nabla_\lambda f_\lambda(\mathbf{\lambda}_0)  \|} = -\frac{\mathbf{g}_\lambda}{\|\mathbf{g}_\lambda\|} \\
\mathbf{v}_{\tau}^{(opt)}= -\frac{\nabla_\tau f_\tau(\mathbf{\tau}_0) }{\|\nabla_\tau f_{\tau}(\mathbf{\tau}_0) \|} = -\frac{\mathbf{g}_\tau}{\|\mathbf{g}_\tau\|} 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, the Euclidean steepest direction  does NOT obey the parameter transformation rule for Euclidean gradients in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
(\mathbf{v}_{\tau}^{(opt)})^T \neq (\mathbf{v}_{\lambda}^{(opt)})^T \mathbf{J} 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Moreover, the optimal value of the Euclidean steepest direction is NOT invariant under a parameter transformation as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\lambda^T \mathbf{v}^{(opt)}_{\lambda}  = - \|\mathbf{g}_\lambda\| \neq - \|\mathbf{g}_\tau\| = \mathbf{g}_\tau^T \mathbf{v}^{(opt)}_{\tau} 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In summary, the Euclidean steepest direction (which is a normalized Euclidean gradient) is NOT transformed according to the transformation rule for a Euclidean gradient.
Moreover, Euclidean gradient descent is not invariant under a parameter transformation. We will cover more about this in &lt;a href=&quot;/posts/2021/11/Geomopt04/&quot;&gt;Part IV&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p class=&quot;bibliography&quot;&gt;&lt;p&gt;&lt;span id=&quot;lin2021tractable&quot;&gt;[1] W. Lin, F. Nielsen, M. E. Khan, &amp;amp; M. Schmidt, &quot;Tractable structured natural gradient descent using local parameterizations,&quot; &lt;i&gt;International Conference on Machine Learning (ICML)&lt;/i&gt; (2021).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;salimbeni2018natural&quot;&gt;[2] H. Salimbeni, S. Eleftheriadis, &amp;amp; J. Hensman, &quot;Natural gradients in practice: Non-conjugate variational inference in Gaussian process models,&quot; &lt;i&gt;International Conference on Artificial Intelligence and Statistics&lt;/i&gt; (PMLR, 2018), pp. 689–697.&lt;/span&gt;&lt;/p&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes:&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In differential geometry, a Euclidean gradient is also known as a coordinate representation of a cotangent vector. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We assume readers are familar with the transformation rule for Euclidean gradients. In differential geometry, this transformation rule can also be shown by using an abstract coordinate map. We avoid defining this map for simplicity. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><summary type="html">Goal This blog post should help readers to understand the invariance of natural-gradients. We will also discuss why the Euclidean steepest direction is NOT invariant.</summary></entry><entry><title type="html">Part II: Natural-Gradients Evaluated at one Point</title><link href="/posts/2021/10/Geomopt02/" rel="alternate" type="text/html" title="Part II: Natural-Gradients Evaluated at one Point" /><published>2021-10-04T00:00:00-07:00</published><updated>2021-10-04T00:00:00-07:00</updated><id>/posts/2021/10/Geomopt02</id><content type="html" xml:base="/posts/2021/10/Geomopt02/">&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post should help readers to understand natural-gradients, which are known as Riemannian gradients with the Fisher-Rao metric.
The main propose of this post is to show how to define and compute natural-gradients.
The space of natural-gradients evaluated at the same point is called a tangent space at that point.&lt;/p&gt;

&lt;p&gt;We will give an informal introduction with a focus on high level of ideas.&lt;/p&gt;

&lt;!--&lt;img src=&quot;/img/gd_vs_ngd.png&quot;  width=&quot;1000&quot;/&gt;--&gt;

&lt;h1 id=&quot;euclidean-steepest-direction-and-directional-derivative&quot;&gt;Euclidean steepest direction and directional derivative&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Before we discuss natural-gradients, we first revisit Euclidean gradients.&lt;/p&gt;

&lt;p&gt;We will show a (normalized) Euclidean gradient can be viewed as the Euclidean steepest direction. Later, we extend the steepest direction in Riemannian cases and show that the Riemannian steepest direction w.r.t. the Fisher-Rao metric is indeed a (normalized) natural-gradient.&lt;/p&gt;

&lt;p&gt;Given a smooth scalar function $\min_{\tau \in \mathcal{R}^K } \,\,f(\mathbf{\tau})$ in a &lt;strong&gt;vector space&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, we can define the (Euclidean) steepest direction at current &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; as the optimal solution to the following optimization problem,
where we assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\tau f(\mathbf{\tau}_0)  \neq \mathbf{0}$&lt;/code&gt;.
We can express the optimization problem in terms of a &lt;strong&gt;directional derivative&lt;/strong&gt; along vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt;.
We want to find the optimal directional derivative, which is the steepest direction.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\|v\|^2=1} \lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t} = ( \nabla_\tau f(\mathbf{\tau}_0) )^T \mathbf{v} 
\end{aligned}\tag{1}\label{1}
$$&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;Note:&lt;/p&gt;

  &lt;p&gt;Each possible vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; lives in the same (vector) space at current point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;It is easy to see that the optimal solution of Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{opt}}= -\frac{\nabla_\tau f(\mathbf{\tau}_0) }{\|\nabla_\tau f(\mathbf{\tau}_0) \|}$&lt;/code&gt;, which is the (Euclidean) steepest direction at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;distance-induced-by-the-fisher-rao-metric&quot;&gt;Distance induced by the Fisher-Rao metric&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;To generalize  the steepest direction at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; in a Riemannian manifold, we want to formulate a similar optimization problem like Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; in the manifold case.
To do so, we have to define the length of a vector in manifold cases. In &lt;a href=&quot;/posts/2021/11/Geomopt03/#standard-euclidean-gradients-are-not-invariant&quot;&gt;Part III&lt;/a&gt;, we will show that the (standard) length does not perseve under a parameter transformation while the length induced by the Fisher-Rao metric does.&lt;/p&gt;

&lt;p&gt;As mentioned at &lt;a href=&quot;/posts/2021/09/Geomopt01/#fisher-rao-metric&quot;&gt;Part I&lt;/a&gt;, the FIM &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}$&lt;/code&gt; is positive definite everywhere in an intrinsic parameter space. We can use the FIM to define the length/norm of a vector (e.g., a Riemannian gradient) $\mathbf{v}$ at a point in a manifold via a weighted inner product. We use an intrinsic parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt; to represent this point.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\|\mathbf{v}\|_F := \sqrt{\mathbf{v}^T \mathbf{F} \mathbf{v}}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The positive-definiteness of the FIM is essential since we do not want a non-zero vector has a zero length.&lt;/p&gt;

&lt;p&gt;The distance (and orthogonality) between two &lt;span style=&quot;color:red&quot;&gt;vectors&lt;/span&gt; at  &lt;span style=&quot;color:red&quot;&gt;point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;&lt;/span&gt;  is also induced by the FIM since we can define them by the inner product as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
d(\mathbf{v},\mathbf{w}) := \|\mathbf{v}-\mathbf{w}\|_F
\end{aligned}
$$&lt;/code&gt;
where vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt; live in the same (vector) space at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/tmanifold.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;In the figure,
the vector space at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;  is just a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt; space. We do not care about whether it is embedded in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^3$&lt;/code&gt; space or not.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In manifold cases, we have to distinguish the difference between a point (e.g., parameter array $\tau_0$) and a vector (e.g., Riemannian gradient under a parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;).
This point is crucial to (natural) gradient-based methods in &lt;a href=&quot;/posts/2021/11/Geomopt04/#two-kinds-of-spaces&quot;&gt;Part IV&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;notice--danger&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;We do NOT define how to compute the distance between two points in the manifold, which will be discussed &lt;a href=&quot;#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;We also do NOT define how to compute the distance between a vector at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt; and another vector at a distinct point
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_1$&lt;/code&gt;, which involves the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Parallel_transport&quot;&gt;parallel transport&lt;/a&gt; in a curved space. For simplicity, we do not define how to parallelly transport a
vector in this post.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;h1 id=&quot;directional-derivatives-in-a-manifold&quot;&gt;Directional derivatives in a manifold&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;As we shown before, the objective function in Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; is a directional derivative in Euclidean cases.
The next step is to generalize the concept of directional derivatives in a manifold.&lt;/p&gt;

&lt;p&gt;Recall that a manifold should be locally like a vector space under &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;&lt;strong&gt;intrinsic&lt;/strong&gt; parameterization&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}$&lt;/code&gt;.
Using this parameterization, consider an optimization problem $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$, where the parameter space $\Omega_\tau$ is determined by the parameterization and the manifold. Recall that we have a local vector space structure in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; if we parametrize the manifold with an intrinsic parameterization.&lt;/p&gt;

&lt;p&gt;Therefore, we can similarly define a directional derivative&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; along Riemannian vector&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; $\mathbf{v}$ as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t}$&lt;/code&gt;, where $t$ is a scalar real number. The main point is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v}$&lt;/code&gt; stays in the parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; thanks to the &lt;strong&gt;local vector space&lt;/strong&gt; structure.&lt;/p&gt;

&lt;p&gt;Recall that we allow a &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;small perturbation&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$E$&lt;/code&gt; around &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt; contained in  parameter space  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$E \subset \Omega_\tau$&lt;/code&gt;) since  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}$&lt;/code&gt; is an intrinsic parameterization.
Therefore, when $|t|$ is small enough, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v} $&lt;/code&gt; stays in the parameter space and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\mathbf{\tau}_0+t\mathbf{v})$&lt;/code&gt; is well-defined.
Note that we only require &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v} \in \Omega_\tau$&lt;/code&gt; when $|t|$ is small enough. When $|t|$ is small enough, this is possible since a line segment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{\tau}_0+t\mathbf{v} \in E$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$E \subset \Omega_\tau$&lt;/code&gt;.
 Technically, this is because  $\Omega_\tau$ is an open set in $\mathcal{R}^K$, where $K$ is the number of entires of parameter array &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Under &lt;strong&gt;intrinsic&lt;/strong&gt; parameterization $\mathbf{\tau}$, the directional derivative remains the same as in the Euclidean case thanks to the &lt;strong&gt;local vector space&lt;/strong&gt; structure in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$\begin{aligned} \lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t} = ( \nabla_\tau f(\mathbf{\tau}_0))^T \mathbf{v}. \end{aligned}$$&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;Note:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v}$&lt;/code&gt; lives in the parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; when scalar &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$|t|$&lt;/code&gt; is small enough&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; lives in a distinct space. This space is called the tangent vector space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^k$&lt;/code&gt; at current point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;

&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/img/sphere.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following example illustrates directional derivatives in manifold cases.&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Valid case: (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is a &lt;strong&gt;local intrinsic&lt;/strong&gt; parameterization for the unit sphere.&lt;/p&gt;

        &lt;p&gt;The line segment from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v} $&lt;/code&gt;  is shown in blue, which is the parameter representation of the yellow curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt; in the manifold.
We will show later that Riemannian gradient vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; under this parametrization at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; is the &lt;strong&gt;parameter representation&lt;/strong&gt; of the tangent vector of curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt; at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/img/sphere_simple.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

        &lt;div class=&quot;notice--danger&quot;&gt;
          &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;:
Curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt; often is NOT the shortest curve in the manifold from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt; to  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_1$&lt;/code&gt;.&lt;/p&gt;
        &lt;/div&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Invalid case: (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;

        &lt;p&gt;A directional derivative can be ill-defined under a &lt;strong&gt;non-intrinsic&lt;/strong&gt; parameterization.&lt;/p&gt;

        &lt;p&gt;We use &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;parameterization 3&lt;/a&gt; for unit circle &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{S}^1$&lt;/code&gt;, where the red line segment passes through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0=(0,1) \in \mathcal{S}^1 $&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/img/tangent_non.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;Any  point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0 + t\mathbf{v}$&lt;/code&gt; in the line segment leaves the manifold when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$t\neq 0$&lt;/code&gt;.  Thus, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\mathbf{\tau}_0+t\mathbf{v})$&lt;/code&gt; is not well-defined.
The main reason is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is not an intrinsic parameterization.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h1 id=&quot;riemannian-steepest-direction&quot;&gt;Riemannian steepest direction&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Recall that we have defined the length of a Riemannian vector and directional derivatives in a manifold.
Now, we can introduce the Riemannian steepest direction &lt;a class=&quot;citation&quot; href=&quot;#absil2009optimization&quot;&gt;[1]&lt;/a&gt; . We will use this to define/compute natrual-gradients.&lt;/p&gt;

&lt;p&gt;Given  a smooth scalar funcion defined in a manifold $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$ under an intrinsic parameterization $\mathbf{\tau}$. We can define the Riemannian steepest direction as the optimal solution to the following optimization problem.  The optimization problem is expressed in terms of a directional derivative along Riemannian vector $\mathbf{v}$, where we assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\tau f(\mathbf{\tau}_0)  \neq \mathbf{0}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{ \color{red} {\|v\|_{F}^2=1} } ( \nabla_\tau f(\mathbf{\tau}_0) )^T  \mathbf{v} 
\end{aligned} \tag{2}\label{2}
$$&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;Note:&lt;/p&gt;

  &lt;p&gt;Each possible (Riemannian) vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; lives in the same (tangent) vector space at current point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The Lagrangian function of this problem is given below, where $\lambda$ is a Lagrange multiplier. 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
L(\mathbf{v},\lambda) =  ( \nabla_\tau f(\mathbf{\tau}_0))^T \mathbf{v} + \lambda (\|v\|_{F}^2 - 1) = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0) + \lambda (\mathbf{v}^T \mathbf{F}(\mathbf{\tau}_0) \mathbf{v}  - 1) 
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}(\mathbf{\tau}_0)$&lt;/code&gt; is the FIM evaluated at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;One of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions&quot;&gt;Karush–Kuhn–Tucker&lt;/a&gt; (KKT) necessary conditions implies that
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{0} = \nabla_{v} L(\mathbf{v}_{\text{opt}},\lambda) = \nabla_\tau f(\mathbf{\tau}_0) + 2 \lambda \mathbf{F}(\mathbf{\tau}_0) \mathbf{v}_{\text{opt}}
\end{aligned}
$$&lt;/code&gt;
When $\lambda \neq 0$, vector 	&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{opt}}$&lt;/code&gt; should be proportional to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0)$&lt;/code&gt;, where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}^{-1}(\mathbf{\tau}_0)$&lt;/code&gt; is well-defined since the FIM &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}(\mathbf{\tau}_0)$&lt;/code&gt; is positive definite.&lt;/p&gt;

&lt;p&gt;We can show that the optimal solution of Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{opt}}= -\frac{ \mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0) }{\| \mathbf{F}^{-1}(\mathbf{\tau}_0)\nabla_\tau f(\mathbf{\tau}_0) \|_F}$&lt;/code&gt;, which gives us the Riemannian steepest direction at current &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Euclidean&lt;/strong&gt; steepest direction &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{euclid}}= -\frac{ \nabla_\tau f(\mathbf{\tau}_0) }{\| \nabla_\tau f(\mathbf{\tau}_0) \|_F}$&lt;/code&gt; is &lt;strong&gt;not&lt;/strong&gt; the optimal solution of  Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}(\tau_0) \neq \mathbf{I}$&lt;/code&gt;.
We will illustrate this by using an example.&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Euclidean steepest direction is not the optimal solution of  Eq. $\eqref{2}$ (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;

        &lt;p&gt;Consider &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}(\tau_0)=\begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; \frac{1}{2} \end{bmatrix}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\tau f(\mathbf{\tau}_0)=\begin{bmatrix} 1\\1 \end{bmatrix}$&lt;/code&gt;.
We have the following results
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\| F^{-1} \nabla_\tau f(\mathbf{\tau}_0) \|_F^2  =  \nabla_\tau^T f(\mathbf{\tau}_0) \mathbf{F}^{-1}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) = 3; \,\,\,
\| \nabla_\tau f(\mathbf{\tau}_0) \|_F^2  =  \nabla_\tau^T f(\mathbf{\tau}_0) \mathbf{F}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) = \frac{3}{2}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{v}_{\text{opt}} = -\begin{bmatrix} \frac{1}{\sqrt{3}} \\ \frac{2}{\sqrt{3}} \end{bmatrix}; \,\,\,
\mathbf{v}_{\text{euclid}}=
-\begin{bmatrix} \sqrt{\frac{2}{3}} \\ \sqrt{\frac{2}{3}} \end{bmatrix}\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{v}_{\text{opt}}^T \nabla_\tau f(\mathbf{\tau}_0)= -\sqrt{3}  &amp;lt;  -\frac{2\sqrt{2}}{\sqrt{3}} = \mathbf{v}_{\text{euclid}}^T \nabla_\tau f(\mathbf{\tau}_0) 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;Therefore, the Euclidean steepest direction &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{euclid}}$&lt;/code&gt; is not the optimal solution of  Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt;.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;p&gt;Given a scalar function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\mathbf{\tau})$&lt;/code&gt; with an intrinsic parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;, we define its (un-normalized) &lt;strong&gt;Riemannian&lt;/strong&gt;  gradient as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{F}_\tau^{-1}(\mathbf{\tau}) \nabla_\tau f(\mathbf{\tau})$&lt;/code&gt; if its (un-normalized) &lt;strong&gt;Euclidean&lt;/strong&gt; gradient is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\tau f(\mathbf{\tau})$&lt;/code&gt;.
We use a learning-rate to control the length of a gradient instead of normalizing its length. 
Since we use the Fisher-Rao metric &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}$&lt;/code&gt;, the Riemannian gradient is also known as the &lt;strong&gt;natural&lt;/strong&gt; gradient.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example: Univariate Gaussian&lt;/p&gt;

  &lt;p&gt;Consider the following scalar function
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
f(\tau):= E_{q(w|\tau)} [ w^2 + \log q(w|\tau) ]
= \mu^2 + \frac{1}{s} + \frac{1}{2} \log(s) - \frac{1}{2}(1+\log(2\pi))
\end{aligned}
$$&lt;/code&gt;
where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)= \mathcal{N}(w|\mu,s^{-1})$&lt;/code&gt; is a Gaussian family with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt;, variance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$s^{-1}$&lt;/code&gt;, 
  intrinsic parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=(\mu,s)$&lt;/code&gt;, and parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau=\{(\mu,s)|\mu \in \mathcal{R},s&amp;gt;0 \}$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;The Fisher information matrix of Gaussian $q(w|\tau)$ under this parametrization is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}_\tau (\tau)  = -E_{q(w|\tau)} [ \nabla_\tau^2 \log q(w|\tau) ] 
=
\begin{bmatrix}
s &amp;amp; 0 \\
0 &amp;amp; \frac{1}{2s^2}
\end{bmatrix}
\end{aligned}
$$&lt;/code&gt;
Now, we consider a member $\tau_0=(0.5,1)$ in the Gaussian family.
The Euclidean gradient is 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\nabla_\tau f(\tau_0) =
\begin{bmatrix}
2 \mu \\
-\frac{1}{s^2} +\frac{1}{2s}
\end{bmatrix}_{\tau=\tau_0}
=\begin{bmatrix}
1 \\ -\frac{1}{2}
\end{bmatrix}
\end{aligned}
$$&lt;/code&gt;
The natural/Riemannian gradient is 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}_\tau^{-1} (\tau_0) \nabla_\tau f(\tau_0) =
\begin{bmatrix}
2 \mu s^{-1}  \\
( -\frac{1}{s^2} +\frac{1}{2s} ) (2s^2)
\end{bmatrix}_{\tau=\tau_0}
=\begin{bmatrix}
1 \\ -1
\end{bmatrix}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Example: Multivariate Gaussian (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;

        &lt;p&gt;Consider a $d$-dimensional Gaussian family $\mathbf{\tau}$ of the family as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(\mathbf{w} |\mathbf{0},\mathbf{S}^{-1}) \Big| \mathbf{S}   \succ \mathbf{0} \}$&lt;/code&gt; with zero mean and precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt; discussed in &lt;a href=&quot;/posts/2021/09/Geomopt01/#dimensionality-of-a-manifold&quot;&gt;Part I&lt;/a&gt;.&lt;/p&gt;

        &lt;p&gt;Parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \mathrm{vech}(\mathbf{S})$&lt;/code&gt; is intrinsic while
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta = \mathrm{vec}(\mathbf{S})$&lt;/code&gt; is not, where
map $\mathrm{vech}()$ is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization&quot;&gt;half-vectorization map&lt;/a&gt; and map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vec}()$&lt;/code&gt; is the standard vectorization map.
Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d(d+1)}{2}$&lt;/code&gt;-dim parameter array while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$d^2$&lt;/code&gt;-dim parameter array,&lt;/p&gt;

        &lt;p&gt;In other words, the FIM w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;  is singular if  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt; is considered as a matrix parameter with $d^2$ degrees of freedom.
Strictly speaking, a natural gradient/vector w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt; is not well-defined.&lt;/p&gt;

        &lt;p&gt;In the literature, a natural gradient w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt; is  defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{MatH}(\mathbf{v})$&lt;/code&gt;, where
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; is a valid natural gradient w.r.t. intrinsic parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}(\mathbf{S})$&lt;/code&gt;
(see  &lt;a href=&quot;/posts/2021/12/Geomopt05/#efficient-ngd-for-multivariate-gaussian&quot;&gt;Part V&lt;/a&gt; for the
details.)&lt;/p&gt;
      &lt;/blockquote&gt;

    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h1 id=&quot;riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Riemannian gradients as tangent vectors (optional)&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;In the previous section, we only consider Riemannian vectors/gradients under a parametrization $\tau$.
Now, we will disucss abstract Riemannian vectors without a parametrization &lt;a class=&quot;citation&quot; href=&quot;#tu2011introduction&quot;&gt;[2]&lt;/a&gt;. This concept is often used to show the invariance of Riemannian gradients, which will be discussed in &lt;a href=&quot;/posts/2021/11/Geomopt03/#parameter-transform-and-invariance&quot;&gt;Part III&lt;/a&gt;.  In physics, this invariance means that a law of physics should be independent of the choice of (reference) coordinate systems.&lt;/p&gt;

&lt;p&gt;A Riemannian gradient denoted by $\mathbf{v}(\tau)$ is indeed a tangent vector $\mathbf{v}$ of a smooth curve in the manifold under the parametrization $\tau$. 
The set of tangent vectors evaluated at $\mathbf{\tau}_0$ is called the tangent space at the corresponding point. 
We will illustrate this by an example.&lt;/p&gt;

&lt;p&gt;Let’s denote the unit sphere by $\mathcal{M}$, where we set the origin to be the center of the sphere. Point $\mathbf{x_0}=(0,0,1)$ is the north pole.
We use the following parameterization, where the top half of the sphere can be locally expressed as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{(\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2})|  \tau_x^2 + \tau_y^2 &amp;lt;1 \}$&lt;/code&gt; with parameter $\mathbf{\tau}=(\tau_x,\tau_y)$. 
Under parametrization $\mathbf{\tau}$, we have the following parametric representations.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;     &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Parametric representation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;North pole  $\mathbf{x_0}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$\mathbf{\tau}_0=(0,0)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Intrinsic parameter space&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;red space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau:=\{ (\tau_x,\tau_y)| \tau_x^2 + \tau_y^2 &amp;lt;1 \}$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Tangent space at $\mathbf{x_0}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;green space  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Yellow curve from $\mathbf{x_0}$ to $\mathbf{x_1}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;blue line segment from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v}(\tau_0)$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/img/sphere.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt; is a parameter array, which is a representation of a point $\mathbf{x}_0$ while $\mathbf{v}(\tau_0)$ is  a Riemannian gradient, which is a representation of the tangent vector of curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma$&lt;/code&gt; at point $\mathbf{x}_0$.&lt;/p&gt;

&lt;div class=&quot;notice--danger&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;:
Be aware of the differences shown in the table.&lt;/p&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;     &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;parametric representation of&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;supported operations&lt;/th&gt;
      &lt;th&gt;distance  discussed in this post&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt; (vector/natural-gradient) space&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;tangent vector space at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;real scalar product, vector addition&lt;/td&gt;
      &lt;td&gt;defined&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; (point/parameter) space&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;top half of the manifold&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;span style=&quot;color:red&quot;&amp;gt; **local** &amp;lt;/span&amp;gt; scalar product, &amp;lt;span style=&quot;color:red&quot;&amp;gt;**local** &amp;lt;/span&amp;gt; vector addition&lt;/td&gt;
      &lt;td&gt;undefined&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Under &lt;strong&gt;intrinsic&lt;/strong&gt; parametrization $\tau$, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau \subset \mathcal{R}^2$&lt;/code&gt;. Thus, we can perform this operation in $\Omega_\tau$ space: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0 +t\mathbf{v}(\tau_0) \in \Omega_\tau$&lt;/code&gt; when scalar &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$|t|$&lt;/code&gt; is small enough. Note that we only define the &lt;a href=&quot;#distance-induced-by-the-fisher-rao-metric&quot;&gt;distance&lt;/a&gt; between two (Riemannian gradient) vectors in the tangent space. The distance between two points in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; space is undefined in this post.&lt;/p&gt;

&lt;h2 id=&quot;parameterization-free-representation-of--vector-mathbfv&quot;&gt;Parameterization-free representation of  vector $\mathbf{v}$&lt;/h2&gt;

&lt;p&gt;The tangent vector $\mathbf{v}$ at point $\mathbf{x_0}$  can be viewed as the &lt;strong&gt;tangent direction&lt;/strong&gt; of a (1-dimensional) smooth curve $\gamma(t) \in \mathcal{M}$, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(0)=\mathbf{x_0}$&lt;/code&gt; and   &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d {\gamma}(t) }{d t} \Big|_{t=0}=\mathbf{v}$&lt;/code&gt; and the support of $\gamma(t)$ denoted by $\mathbf{I}$ is an open interval in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^1$&lt;/code&gt; containing 0. 
Since a curve $\gamma(t)$ is a geometric object,  its tangent direction is also a geometric object. The tangent direction is a parameterization-free repesentation of vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;parameterization-dependent-representation-of-vector-mathbfv&quot;&gt;Parameterization-dependent representation of vector $\mathbf{v}$&lt;/h2&gt;

&lt;p&gt;Given intrinsic parametrization $\tau$, we can define the parametric representation of the curve denoted by ${\gamma}_\tau(t)$, where the domain is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau \subset \mathcal{R}^1$&lt;/code&gt;.
The parametric representation of vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; is defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0):= \frac{d {\gamma}_{\tau}(t) }{d t} \Big|_{t=0}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\gamma}_{\tau}(0)=\tau_0$&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example&lt;/p&gt;

  &lt;p&gt;Consider the yellow curve $\gamma(t) = (t v_{x}, t v_{y}, \sqrt{1 - t^2(v_{x}^2 + v_{y}^2) } ) \in \mathcal{M} $ 
and the blue line segment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\gamma}_{\tau}(t)= (t v_{x} , t v_y  ) \in \Omega_\tau $&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$|t|$&lt;/code&gt; must be small enough.&lt;/p&gt;

  &lt;p&gt;The parametric  representation of the vector is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0):= \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}=(v_x,v_y)$&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A Riemannian gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0)$&lt;/code&gt; can be viewed as a parametric representation of tangent vector  $\mathbf{v}$ as shown below.&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;Consider a smooth scalar function defined in the manifold $h: \mathcal{M} \to \mathcal{R}$. In the unit sphere case, consider &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h(\mathbf{z})$&lt;/code&gt; subject to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{z}^T \mathbf{z}=1$&lt;/code&gt;.
Under parameterization $\mathbf{\tau}$, we can locally re-expressed the function as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\tau(\mathbf{\tau}):=h( (\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2}) )$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau \in \Omega_\tau$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;By the definition of a directional derivative, the following identity holds for any smooth scalar function $h$: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$[\nabla h_\tau(\mathbf{\tau}_0)]^T \mathbf{v}(\mathbf{\tau}_0) =\frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0}$&lt;/code&gt;, where $h_\tau$ is the parametric representation of  $h$ . Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(h_\tau \circ {\gamma}_\tau) (t)$&lt;/code&gt; is a function defined from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau $&lt;/code&gt; to $\mathcal{R}^1$, where domain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau \subset \mathcal{R}^1$&lt;/code&gt;.&lt;/p&gt;

  &lt;div class=&quot;notice--success&quot;&gt;
    &lt;p&gt;The &lt;strong&gt;key&lt;/strong&gt; observation:&lt;/p&gt;

    &lt;p&gt;Function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(h_\tau \circ {\gamma}_\tau) (t)$&lt;/code&gt; becomes a standard real-scalar function thanks to parametrization $\tau$. Thus, we can safely use the standard chain rule.&lt;/p&gt;
  &lt;/div&gt;

  &lt;p&gt;By the chain rule, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0}=[\nabla h_\tau(\mathbf{\tau}_0)]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\gamma}_\tau(0)=\tau_0$&lt;/code&gt;. Thus,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0) =  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$&lt;/code&gt; since (Euclidean gradient) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla h_\tau(\mathbf{\tau}_0)$&lt;/code&gt; is an arbitrary vector in $\mathcal{R}^2$ and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is a 2-dim parameter array.&lt;/p&gt;

  &lt;p&gt;In summary, a Riemannian gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0)$&lt;/code&gt; can be viewed as a parametric representation of the tangent vector 
 of curve $\gamma(t)$ at $\mathbf{x}_0$ since  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\gamma}_\tau(t)$&lt;/code&gt; is the parametric representation of $\gamma(t)$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;riemannian-gradient-space-has-a-vector-space-structure&quot;&gt;(Riemannian) gradient space has a vector-space structure&lt;/h2&gt;

&lt;p&gt;We can also define vector additions and real scalar products in a tangent vector space by using tangent directions of curves in the manifold with/without a parameterization.&lt;/p&gt;

&lt;p&gt;The key takeway is that a vector space structure is an integral part of a tangent &lt;strong&gt;vector&lt;/strong&gt; space. On the other hand, we have to use an intrinsic parametrization $\tau$ to artificially create a local vector space structure in parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt;. Recall that a parameter space is a parametric representation of a  set of &lt;strong&gt;points&lt;/strong&gt; in a manifold.
We will discuss more about this in &lt;a href=&quot;/posts/2021/11/Geomopt04/#two-kinds-of-spaces&quot;&gt;Part IV&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p class=&quot;bibliography&quot;&gt;&lt;p&gt;&lt;span id=&quot;absil2009optimization&quot;&gt;[1] P.-A. Absil, R. Mahony, &amp;amp; R. Sepulchre, &lt;i&gt;Optimization algorithms on matrix manifolds&lt;/i&gt; (Princeton University Press, 2009).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;tu2011introduction&quot;&gt;[2] L. W. Tu, &quot;An introduction to manifolds. Second,&quot; &lt;i&gt;New York, US: Springer&lt;/i&gt; (2011).&lt;/span&gt;&lt;/p&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes:&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For simplicity, we avoid defining a (coordinate-free) &lt;a href=&quot;https://en.wikipedia.org/wikiCovariant_derivative&quot;&gt;covariant derivative&lt;/a&gt;, which induces parallel transport. Given a smooth scalar field/function on a manifold, a coordinate representation of the covariant derivative remains the same as the Euclidean case. Note that the standard coordinate derivative  is identical to  the coordinate representation of the covariant derivative when it comes to a scalar field. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A Riemannian gradient is a coordinate representation of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors&quot;&gt;contravariant vector&lt;/a&gt; (A.K.A. a Riemannian vector) while a Euclidean gradient is a coordinate representation of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors&quot;&gt;covariant vector&lt;/a&gt; (A.K.A. a Riemannian covector). We will discuss their transformation rules in &lt;a href=&quot;/posts/2021/11/Geomopt03/&quot;&gt;Part III&lt;/a&gt;. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><summary type="html">Goal This blog post should help readers to understand natural-gradients, which are known as Riemannian gradients with the Fisher-Rao metric. The main propose of this post is to show how to define and compute natural-gradients. The space of natural-gradients evaluated at the same point is called a tangent space at that point.</summary></entry><entry><title type="html">Part I: Smooth Manifolds with the Fisher-Rao Metric</title><link href="/posts/2021/09/Geomopt01/" rel="alternate" type="text/html" title="Part I: Smooth Manifolds with the Fisher-Rao Metric" /><published>2021-09-06T00:00:00-07:00</published><updated>2021-09-06T00:00:00-07:00</updated><id>/posts/2021/09/Geomopt01</id><content type="html" xml:base="/posts/2021/09/Geomopt01/">&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post focuses on the Fisher-Rao metric, which gives rise to the Fisher information matrix (FIM). We will introduce the following concepts useful to ensure non-singular FIMs&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regularity conditions and intrinsic parameterization of a distribution&lt;/li&gt;
  &lt;li&gt;Dimensionality of a smooth manifold&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The discussion here is informal and focuses on more on intuitions, rather than rigor.&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Click to see how to cite this blog post&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;

      &lt;div class=&quot;language-latex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;lin2021blog01,
  title = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Smooth Manifolds with the Fisher-Rao Metric&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;,
  author = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Lin, Wu and Nielsen, Frank and Khan, Mohammad Emtiyaz and Schmidt, Mark&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;,
  url = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;https://informationgeometryml.github.io/posts/2021/09/Geomopt01/&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;, 
  howpublished = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;https://informationgeometryml.github.io/posts/2021/09/Geomopt01/&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;,
  year = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;2021&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;,
  note = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Accessed: 2021-09-06&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;      &lt;/div&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;The goal of this blog is introduce the geometric structures associated with probability distribution. Let’s start with some motivation: why should we care about such geometric structures?&lt;/p&gt;

&lt;p&gt;The answer is that by exploiting geometric structures, we can&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;design efficient and simple algorithms &lt;a class=&quot;citation&quot; href=&quot;#amari1998natural&quot;&gt;[1]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;design robust methods that are less sensitive to re-parametrization &lt;a class=&quot;citation&quot; href=&quot;#lin2021tractable&quot;&gt;[2]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;understand the behavior of models/algorithms using tools from differential geometry, information geometry, and invariant theory &lt;a class=&quot;citation&quot; href=&quot;#liang2019fisher&quot;&gt;[3]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is true for a majority of machine learning methods, all of which make use of probability distributions of various kinds.&lt;/p&gt;

&lt;p&gt;Below, we give some common examples from the literature. A reader familiar with such examples can skip this part.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Least Squares&lt;/strong&gt; (frequestist estimation):&lt;/p&gt;

  &lt;p&gt;Given N input-output pairs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(x_i,y_i)$&lt;/code&gt;,  the least-square loss can be viewed as a finite-sample approximation of the expectation w.r.t. a probability distribution (data generating distribution),
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau}  \frac{1}{2n} \sum_{i=1}^{n}   (y_i-x_i^T\tau)^2 
 &amp;amp;= - \frac{1}{n}  \sum_{i=1}^{n}  \log \mathcal{N}(y_i | x_i^T\tau,1) + \text{constant}\\
&amp;amp; \approx  E_{ \color{red}  { p(x,y | \tau) } } [ - \log  p(x,y | \tau) ]
\end{aligned} \tag{1}\label{1}
$$&lt;/code&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(x,y | \tau) = \mathcal{N}(y | x^T\tau,1) p(x) $&lt;/code&gt; is assumed to be the data-generating distribution. (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathcal{N} (y | m, v) $&lt;/code&gt; denotes a normal distribution over &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ y $&lt;/code&gt; with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ m $&lt;/code&gt; and variance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ v $&lt;/code&gt;).&lt;/p&gt;

  &lt;p&gt;Well-known algorithms such as  &lt;a href=&quot;https://en.wikipedia.org/wiki/Scoring_algorithm#Fisher_scoring&quot;&gt;&lt;strong&gt;Fisher scoring&lt;/strong&gt;&lt;/a&gt;  and &lt;strong&gt;(emprical) natural-gradient descent&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#martens2020new&quot;&gt;[4]&lt;/a&gt; are commonly used methods that exploit the geometric structure of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$p(x,y | \tau)$&lt;/code&gt;. These are examples of algorithms derived from a frequentist perspective, which can also be generalized to neural network &lt;a class=&quot;citation&quot; href=&quot;#martens2020new&quot;&gt;[4]&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Variational Inference&lt;/strong&gt; (Bayesian estimation):&lt;/p&gt;

  &lt;p&gt;Given a prior &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(z) $&lt;/code&gt; and a likelihood &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(\mathcal{D} | z ) $&lt;/code&gt; over a latent vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$z$&lt;/code&gt; and known data &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathcal{D} $&lt;/code&gt;, we can approximate the exact posterior &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p( z | \mathcal{D} ) =\frac{p(z,\mathcal{D})}{p(\mathcal{D})} $&lt;/code&gt; by optimizing a variational objective with respect to  an approximated distribution &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ q(z | \tau) $&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau} \mathrm{KL} [ { q(z | \tau) || p( z | \mathcal{D} ) }   ] = E_{ \color{red}  {q(z | \tau)} } [ \log q(z | \tau) - \log p( z , \mathcal{D} )    ]  + \text{constant} 
\end{aligned} \tag{2}\label{2}
$$&lt;/code&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathrm{KL} [ q(z) ||  p(z) ] :=  E_{ {q(z)} } [ \log \big(\frac{q(z)}{p(z)}\big) ]$&lt;/code&gt; is the Kullback–Leibler divergence.&lt;/p&gt;

  &lt;p&gt;The &lt;strong&gt;natural-gradient variational inference&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#khan2017conjugate&quot;&gt;[5]&lt;/a&gt; is an algorithm that speeds up the inference by exploiting the geometry of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(z|\tau)$&lt;/code&gt; induced by the Fisher-Rao metric.
This approach is derived from a Bayesian  perspective, and can also be generalized to neural network cases &lt;a class=&quot;citation&quot; href=&quot;#lin2021tractable&quot;&gt;[2]&lt;/a&gt; &lt;a class=&quot;citation&quot; href=&quot;#osawa2019practical&quot;&gt;[6]&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Evolution Strategies&lt;/strong&gt; (Global optimization):&lt;/p&gt;

  &lt;p&gt;Global optimization methods often use a search distribution, denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi(a | \tau ) $&lt;/code&gt;, to find the global maximum of an objective &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h(a)$&lt;/code&gt; by solving a problem of the following form:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau}  E_{ \color{red}  {\pi(a | \tau)} } [ h(a)  ]
\end{aligned} \tag{3}\label{3}
$$&lt;/code&gt;
Samples from the search distribution are evaluated through a “fitness” function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ h(a) $&lt;/code&gt;, and guide the optimization towards better optima.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;The &lt;strong&gt;natural evolution strategies&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#wierstra2014natural&quot;&gt;[7]&lt;/a&gt; is an algorithm that speeds up the search process by exploiting the geometry of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi(a|\tau)$&lt;/code&gt;.
In the context of reinforcement learning,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi(a | \tau ) $&lt;/code&gt; is known as the policy distribution to generate actions and the natural evolution strategies is known as the &lt;strong&gt;natural policy gradient&lt;/strong&gt; method &lt;a class=&quot;citation&quot; href=&quot;#kakade2001natural&quot;&gt;[8]&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In all of the examples above, the objective function is expressed in terms of an expectation w.r.t. a distribution in red, parameterized with the parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \tau $&lt;/code&gt;. 
The geometric structure of a distribution &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w|\tau) $&lt;/code&gt; for the quantity &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ w $&lt;/code&gt; can be exploited to improve the learning algorithms. The table below summarizes the three examples. 
More applications of similar nature are discussed in &lt;a class=&quot;citation&quot; href=&quot;#le2007topmoumoute&quot;&gt;[9]&lt;/a&gt; and &lt;a class=&quot;citation&quot; href=&quot;#duan2020ngboost&quot;&gt;[10]&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Example       &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$w$&lt;/code&gt;        &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w|\tau) $&lt;/code&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Least Square&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;observation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$y$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$p(y|x,\tau)$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Variational Inference&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;latent variable $z$&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(z|\tau)$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Evolution Strategies&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;decision variable $a$&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi(a|\tau)$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;In general, we may have to compute or estimate the inverse of the FIM. However, in many useful machine learning applications, algorithms such as &lt;a class=&quot;citation&quot; href=&quot;#lin2021tractable&quot;&gt;[2]&lt;/a&gt;  &lt;a class=&quot;citation&quot; href=&quot;#martens2020new&quot;&gt;[4]&lt;/a&gt; &lt;a class=&quot;citation&quot; href=&quot;#khan2017conjugate&quot;&gt;[5]&lt;/a&gt; &lt;a class=&quot;citation&quot; href=&quot;#osawa2019practical&quot;&gt;[6]&lt;/a&gt;  &lt;a class=&quot;citation&quot; href=&quot;#wierstra2014natural&quot;&gt;[7]&lt;/a&gt;  &lt;a class=&quot;citation&quot; href=&quot;#kakade2001natural&quot;&gt;[8]&lt;/a&gt;  &lt;a class=&quot;citation&quot; href=&quot;#le2007topmoumoute&quot;&gt;[9]&lt;/a&gt; can be efficiently implemented without
explicitly computing the inverse of the FIM.&lt;/p&gt;

  &lt;p&gt;We discuss this in other posts; see&lt;br /&gt;
&lt;a href=&quot;/posts/2021/12/Geomopt05/#efficient-ngd-for-multivariate-gaussians&quot;&gt;Part V&lt;/a&gt; and 
&lt;a href=&quot;/posts/2021/07/ICML/&quot;&gt;our ICML work&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In the rest of the post, we will mainly focus on the geometric structure of (finite-dimensional) parametric families, for example, a 1-dimensional Gaussian family.
The following figure illustrates four distributions in the Gaussian family denoted by
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&amp;gt;0 \}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w | \tau ): = \mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $&lt;/code&gt;  and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau :=(\mu,\sigma) $&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/gauss1d.png&quot; alt=&quot;Figure 2&quot; title=&quot;Source:Wikipedia&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;intrinsic-parameterizations&quot;&gt;Intrinsic Parameterizations&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;We start by discussing parameterizations for a distribution family. As we will see, a proper parameterization plays an important role in defining the FIM.
We can not use an arbitrary differentiable parameterization to represent a geometric structure.
This structure is also known as a manifold structure &lt;a class=&quot;citation&quot; href=&quot;#tu2011introduction&quot;&gt;[11]&lt;/a&gt;. A (smooth) manifold should be locally like a “flat” vector space. For example, consider the (curved) Earth surface. It looks like a (locally) flat space to us.&lt;/p&gt;

&lt;p&gt;We require that a manifold should be locally like a vector space denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ E $&lt;/code&gt; under a proper  parameterization. Informally, we refer to 
such structure as a local vector-space structure.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;Local &lt;strong&gt;vector-space structure&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;It supports local &lt;strong&gt;vector additions&lt;/strong&gt;,  local &lt;strong&gt;real scalar products&lt;/strong&gt;, and their algebraic laws (i.e., the distributive law). (see &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Part II&lt;/a&gt; for the details.)&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Intuitively, this vector-space structure means that a local (small) perturbation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ E $&lt;/code&gt; at each point should stay in the parameter space.
Technically, the parameter space denoted by $\Omega_\tau$ should be an &lt;strong&gt;open&lt;/strong&gt; set in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt; is the number of entries of a parameter array.
As we will see soon, the FIM is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt;-by-&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt; matrix.&lt;/p&gt;

&lt;p&gt;We refer to this kind of parametrizations as an intrinsic parameterization&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.
The main reason of using an intrinsic parameterization is (1) the topology of a parameter space is nice. (2) The (exact) FIM is non-singular and well-defined (finite).
These properties will play a key role in &lt;a href=&quot;/posts/2021/11/Geomopt04/#natural-gradient-descent-in-an-intrinsic-parameter-space&quot;&gt;Part IV&lt;/a&gt; for natural-gradient descent.
As we will see later that every intrinsic parametrization of a manifold has the same degrees of freedom.&lt;/p&gt;

&lt;p&gt;To illustrate this, let’s consider a unit circle in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt; as shown in the Figure.
Clearly, a point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ (0,1) $&lt;/code&gt; highlighted in green is in the circle, where we consider its center as the origin.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/circle.png&quot; title=&quot;Source:Wikipedia&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Parametrization 1 (Intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;A (local) parametrization at $ (0,1) $ highlighted in red for the circle is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ (t,\sqrt{1-t^2}) | -h&amp;lt;t&amp;lt;h \} $&lt;/code&gt;, where $h=0.1$. We use &lt;strong&gt;one&lt;/strong&gt; (scalar) parameter in this parametrization.&lt;/p&gt;

  &lt;p&gt;The manifold is (locally) “flat” since we can always find a small &lt;strong&gt;1-dimensional&lt;/strong&gt; perturbation $E$ in the &lt;strong&gt;1-dimensional&lt;/strong&gt; parameter space  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_t=\{t|-h&amp;lt;t&amp;lt;h \} $&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/img/1d-perturbation.png&quot; title=&quot;Fig&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

  &lt;p&gt;This parametrization is called an &lt;strong&gt;intrinsic&lt;/strong&gt; parameterization.&lt;/p&gt;

  &lt;p&gt;We can similarly define a (local) parametrization at each point of the circle. In fact, we can use  finite (local) parameterizations to represent the whole circle as shown below.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/img/charts.png&quot; title=&quot;Source:Wikipedia&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Parametrization 2 (Non-intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;Let’s define a map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f : [0,2\pi) \rightarrow \mathcal{S}^1 $&lt;/code&gt; such that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\theta) = (\sin \theta, \cos \theta ) $&lt;/code&gt;, where we use $\mathcal{S}^1$ to denote the circle.&lt;/p&gt;

  &lt;p&gt;A (global) parametrization of the circle is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ f(\theta) | \theta \in [0,2\pi)  \}$&lt;/code&gt;, where we use one (scalar) parameter.&lt;/p&gt;

  &lt;p&gt;This map $f$ is bijective and smooth. However, the parameter space is not open in $\mathcal{R}$. Moreover, its inverse map $f^{-1}$ is &lt;strong&gt;not&lt;/strong&gt; continous at point $(0,1) \in  \mathcal{S}^1$.
This parametrization is not intrinsic.
In fact, there does not exist a (single) &lt;strong&gt;global&lt;/strong&gt; and intrinsic parametrization to represent the circle.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As shown in Parametrization 2, this smoothness requirement of the inverse map is essential when it comes to reparametrization (A.K.A. parameter transformation). The smoothness&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; of a map and the inverse map togeher gives us a way to generate new intrinsic parameterizations. In other words, the Jacobian matrix is non-singular if the requirement is satisfied. Thus, we can safely use the chain rule and inverse function theorem.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Parametrization 3 (Non-intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;The circle does &lt;strong&gt;not&lt;/strong&gt; look like a flat space under the following parametrization
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ (x,y) | x^2+y^2=1, x,y \in \mathcal{R} \} $&lt;/code&gt;. The number of entries in this parameter array is 2.&lt;/p&gt;

  &lt;p&gt;The reason is that we cannot find a small &lt;strong&gt;2-dimensional&lt;/strong&gt; perturbation $E$ in the &lt;strong&gt;2-dimensional&lt;/strong&gt; parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_{\tau}=\{(x,y)|x^2+y^2=1 \} $&lt;/code&gt; due to the constraint $x^2+y^2=1$. In other words, $\Omega_\tau$ is not open in $\mathcal{R}^2$.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/img/2d-perturbation.png&quot; title=&quot;Fig&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;intrinsic-parameterizations-for-parametric-families&quot;&gt;Intrinsic Parameterizations for Parametric families&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Now, we discuss how to choose a parameterization given a parametric family so that we can exploit the geometric structure &lt;a class=&quot;citation&quot; href=&quot;#amari2016information&quot;&gt;[12]&lt;/a&gt; induced by the Fisher-Rao metric.&lt;/p&gt;

&lt;p&gt;Given a parametric distribution family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w|\tau) $&lt;/code&gt; indexed by its parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w|\tau) $&lt;/code&gt; should be smooth w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \tau $&lt;/code&gt; by considering &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ w $&lt;/code&gt; to be fixed.
We say a parametrization is &lt;strong&gt;intrinsic&lt;/strong&gt; if the following condition for parameter (array) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau $&lt;/code&gt; holds:&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Regularity Condition&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;The following partial derivatives 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \partial_{\tau_i} \log p(w|\tau) \} $&lt;/code&gt;  should be linearly independent.&lt;/p&gt;

  &lt;p&gt;In other words, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sum_i c_i \partial_{\tau_i} \log p(w|\tau)= 0 $&lt;/code&gt; holds only when constant &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_i$&lt;/code&gt; is zero and the value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_i$&lt;/code&gt; does not depent on  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$w$&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Note that this regularity condition implicitly assumes that the parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; is an open set in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt; due to the definition of the partial derivatives, where K is the number of entries in parameter array &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;.
We will discuss more about this at &lt;a href=&quot;#caveats-of-the-fisher-matrix-computation&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We will use the following examples to illustrate this condition.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 1 (Intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;We will show that the regularity condition holds. Consider a 1-dimensional Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&amp;gt;0 \}$&lt;/code&gt; with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt;, variance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sigma$&lt;/code&gt;, and parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\mu,\sigma) $&lt;/code&gt;.
The partial derivatives are
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\mu} \log \mathcal{N}(w |\mu,\sigma) = \frac{w-\mu}{\sigma}, \,\,\, \partial_{\sigma} \log \mathcal{N}(w |\mu,\sigma) = \frac{ (w-\mu)^2 }{2 \sigma^2} - \frac{1}{2\sigma} 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\mu} \log \mathcal{N}(w |\mu,\sigma) \Big|_{\mu=0,\sigma=1}= w, \,\,\, \partial_{\sigma} \log \mathcal{N}(w |\mu,\sigma) \Big|_{\mu=0,\sigma=1} = \frac{ w^2 -1 }{2}  
\end{aligned}
$$&lt;/code&gt;
If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ c_1 w + c_2  (\frac{ w^2 -1 }{2}) = 0$&lt;/code&gt; holds for any $w$, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_1=c_2=0$&lt;/code&gt;, which implies  linear independence.&lt;/p&gt;

  &lt;p&gt;Similarly, we can show that for any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu \in \mathcal{R}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sigma &amp;gt;0$&lt;/code&gt;, 
the partial derivatives are linearly independent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 2 (Non-intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;We will show that the regularity condition fails. Consider a Bernoulli family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0  \}$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1) $&lt;/code&gt;, where function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathcal{I}(\cdot) $&lt;/code&gt; is the indicator function.
The partial derivatives are&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\pi_0} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{\pi_1}{(\pi_0+\pi_1)^2}   
\end{aligned}
$$&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\pi_1} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} 
\end{aligned}
$$&lt;/code&gt;
Note that when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_0 = \pi_0 \neq 0 $&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_1= \pi_1 \neq 0$&lt;/code&gt;, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_0 \frac{\pi_1}{(\pi_0+\pi_1)^2}  + c_1 \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} = 0$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;Therefore, we can show that 
the partial derivatives are linearly dependent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 3 (Non-intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;We will soon show that the  condition fails for Bernoulli family  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0, \pi_0+\pi_1=1  \}$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1)$&lt;/code&gt;. 
The main reason is that the parameter space is not open in $\mathcal{R}^2$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 4  (Intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;We can show that the condition holds for Bernoulli family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) (1-\pi_0)  \Big| 0&amp;lt;\pi_0&amp;lt;1  \}$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \pi_0$&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;fisher-rao-metric&quot;&gt;Fisher-Rao Metric&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;As we can see from the &lt;a href=&quot;#motivation&quot;&gt;previous section&lt;/a&gt;, we can use the Fisher-Rao metric to design fast and efficient algorithms.
In statistics, this metric is closely related to  maximum likelihood estimation, central limit theorem, and principle of maximum entropy.
These statistical principles  &lt;a class=&quot;citation&quot; href=&quot;#casella2002statistical&quot;&gt;[13]&lt;/a&gt; &lt;a class=&quot;citation&quot; href=&quot;#jaynes1957information&quot;&gt;[14]&lt;/a&gt; play essential roles in training and estimating machine learning models.
Moreover, the Fisher-Rao metric has been theorically and emprically used in machine learning and statistics.&lt;/p&gt;

&lt;p&gt;Given an intrinsic parameterization, we can define the Fisher-Rao metric under this parameterization as:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log p(w|\tau) \Big) ].
\end{aligned}
$$&lt;/code&gt;
Note that the metric could be ill-defined since the expectation may not exist.&lt;/p&gt;

&lt;p&gt;Given a parameterization,  we can express the metric in a matrix form as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}(\tau) := E_{ p(w|\tau) }  [ \Big( \nabla_{\tau} \log p(w|\tau ) \Big)  \Big(\nabla_{\tau} \log p(w|\tau) \Big)^T ],
\end{aligned}
$$&lt;/code&gt;
where $K$ is the number of entries of parameter array $\tau$ and 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \nabla_{\tau} \log p(w|\tau ) := [ \partial_{\tau_1} \log p(w|\tau ), \cdots, \partial_{\tau_K} \log p(w|\tau ) ]^T  $&lt;/code&gt; is a column vector.&lt;/p&gt;

&lt;p&gt;The matrix form is also known as the &lt;strong&gt;Fisher information matrix&lt;/strong&gt; (FIM). Obviously, the form of the FIM depends on the choice of parameterizations. In many cases,  we could also compute the FIM as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{F}(\tau) := - E_{p(w|\tau) }  [  \nabla_{\tau}^2 \log p(w|\tau )  ]$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The regularity condition guarantees that the FIM is non-singular if the matrix exists.
The condition is also related to the model identification in maximum likelihood estimation.&lt;/p&gt;

&lt;p&gt;In the following discussion, we will assume the metric is well-defined.
In such cases, the Fisher-Rao metric is a valid Riemannian metric &lt;a class=&quot;citation&quot; href=&quot;#lee2018introduction&quot;&gt;[15]&lt;/a&gt; since the corresponding FIM is positive definite everywhere in an &lt;strong&gt;intrinsic&lt;/strong&gt; parameter space.&lt;/p&gt;

&lt;div class=&quot;notice--danger&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;:
An arbitrary Riemannian metric often is NOT useful for applications in machine learning and statistics.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Recall that when we perform a parameter transformation, we require both the transformation map and its inverse map to be smooth.
If this smoothness requirement is satisfied, we can obtain a new intrinsic parameterization via the transformation.
In &lt;a href=&quot;/posts/2021/11/Geomopt03/#Pparameter-transform-and-invariance&quot;&gt;Part III&lt;/a&gt;,
we will show the FIM is also positive definite under this new intrinsic parameterization.&lt;/p&gt;

&lt;h1 id=&quot;caveats-of-the-fisher-matrix-computation&quot;&gt;Caveats of the Fisher matrix computation&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;There are some caveats when it comes to the Fisher matrix computation. In particular, the regularity condition should be satisfied.
It is possible to define the FIM under a non-intrinsic parameterization. However, the FIM often is singular or ill-defined under a non-intrinsic  parameterization as shown below.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 1 (Ill-defined FIM):&lt;/p&gt;

  &lt;p&gt;Consider Bernoulli family  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0, \pi_0+\pi_1=1  \}$&lt;/code&gt; with non-intrinsic parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1)$&lt;/code&gt;.
The following computation is not correct. Do you make similar mistakes like this?&lt;/p&gt;

  &lt;p&gt;Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  p(w|\tau ) =  \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1)$&lt;/code&gt;. The derivative is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$  \begin{aligned} \nabla_{\tau} \log p(w|\tau ) = \frac{1}{p(w|\tau )} [\mathcal{I}(w=0) ,  \mathcal{I}(w=1)]^T  \end{aligned} \tag{4}\label{4}$$&lt;/code&gt;
Thus, by Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt;, the FIM under this  parameterization is&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$  \begin{aligned} F(\tau) = E_{p(w|\tau) } [ \frac{1}{p^2(w|\tau)} \begin{bmatrix} \mathcal{I}^2(w=0) &amp;amp;  \mathcal{I}(w=1)  \mathcal{I}(w=0) \\  \mathcal{I}(w=0)  \mathcal{I}(w=1) &amp;amp;  \mathcal{I}^2(w=1) \end{bmatrix} ]   = \begin{bmatrix} \frac{1}{\pi_0} &amp;amp;  0 \\ 0 &amp;amp;  \frac{1}{\pi_1} \end{bmatrix} \end{aligned}$$&lt;/code&gt;&lt;/p&gt;
  &lt;div class=&quot;notice--danger&quot;&gt;
    &lt;p&gt;This computation is not correct. Do you know why?&lt;/p&gt;
  &lt;/div&gt;

  &lt;div class=&quot;notice--info&quot;&gt;
    &lt;details&gt;
&lt;summary&gt;Reason: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
        &lt;p&gt;The key reason is the equality constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi_0+\pi_1=1 $&lt;/code&gt;. Thus, Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; is &lt;strong&gt;incorrect&lt;/strong&gt;.&lt;/p&gt;

        &lt;p&gt;By definition, a Bernoulli distribution is valid only when the constraint holds. 
Thus, the constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi_0+\pi_1=1 $&lt;/code&gt; must be satisifed when we compute the Fisher matrix since the computation involves computing the expectation w.r.t. this distribution.&lt;/p&gt;

        &lt;p&gt;Note that the gradient is defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \nabla_{\tau} \log p(w|\tau ) := [ \partial_{\pi_0} \log p(w|\tau ), \partial_{\pi_1} \log p(w|\tau ) ]^T $&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;Unfortunately, these partial derivatives do not exist. By the definition of  partial derivative &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \partial_{\pi_0} \log p(w|\tau )$&lt;/code&gt;, we fix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi_1$&lt;/code&gt; and allow &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi_0$&lt;/code&gt; to change.
However, given that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi_1$&lt;/code&gt; is fixed and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi_0 $&lt;/code&gt; is fully determined by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi_1$&lt;/code&gt; due to the equality constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi_0+\pi_1=1 $&lt;/code&gt;. 
Therefore, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  \partial_{\pi_0} \log p(w|\tau ) $&lt;/code&gt; is not well-defined.
In other words, the above Fisher matrix computation is not correct since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \nabla_{\tau} \log p(w|\tau ) $&lt;/code&gt; does not exist.&lt;/p&gt;
      &lt;/fieldset&gt;
&lt;/details&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 2  (Singular FIM):&lt;/p&gt;

  &lt;p&gt;Consider Bernoulli family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0  \}$&lt;/code&gt; with  non-intrinsic parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1) $&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;The FIM under this  parameterization is singular as shown below.&lt;/p&gt;

  &lt;p&gt;Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  p(w|\tau ) =  \mathcal{I}(w=0)  \frac{\pi_0}{\pi_0+\pi_1}+ \mathcal{I}(w=1)  \frac{\pi_1}{\pi_0+\pi_1}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1)$&lt;/code&gt;. The derivative is&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$ 
\begin{aligned}
\nabla_{\tau} \log p(w|\tau ) = \frac{ \mathcal{I}(w=0) - \mathcal{I}(w=1)  }{p(w|\tau )} [\frac{\pi_1}{(\pi_0+\pi_1)^2} , \frac{-\pi_0}{(\pi_0+\pi_1)^2}]^T 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;Thus, the FIM under this  parameterization is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned} 
F(\tau) = E_{p(w|\tau) } [ \frac{( \mathcal{I}(w=0) - \mathcal{I}(w=1))^2 }{p^2(w|\tau)} \begin{bmatrix} \frac{\pi_1^2}{(\pi_0+\pi_1)^4}  &amp;amp; \frac{-\pi_0 \pi_1}{(\pi_0+\pi_1)^4} \\ \frac{-\pi_0 \pi_1}{(\pi_0+\pi_1)^4}  &amp;amp;  \frac{\pi_0^2}{(\pi_0+\pi_1)^4} \end{bmatrix} ]   =\frac{1}{(\pi_0+\pi_1)^2} \begin{bmatrix} \frac{\pi_1}{\pi_0} &amp;amp;  -1 \\ -1 &amp;amp;  \frac{\pi_0}{\pi_1} \end{bmatrix}
\end{aligned}
$$&lt;/code&gt;
where this FIM is singular since the matrix determinant is zero as shown below. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{det}\big( \begin{bmatrix} \frac{\pi_1}{\pi_0} &amp;amp;  -1 \\ -1 &amp;amp;  \frac{\pi_0}{\pi_1} \end{bmatrix}\big) = 0.
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, we give an example to show that the FIM of a Bernoulli family can be non-singular when we use an intrinsic parameterization.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 3  (Non-singular FIM):&lt;/p&gt;

  &lt;p&gt;Consider Bernoulli family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi)  \Big| 0&amp;lt;\pi&amp;lt;1 \}$&lt;/code&gt; with  &lt;strong&gt;intrinsic&lt;/strong&gt; parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \pi $&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;The FIM under this parameterization is non-singular as shown below.&lt;/p&gt;

  &lt;p&gt;Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  p(w|\tau ) =  \mathcal{I}(w=0) \pi+ \mathcal{I}(w=1) (1-\pi)$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \pi$&lt;/code&gt;. The derivative is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned} \nabla_{\tau} \log p(w|\tau ) = \frac{\mathcal{I}(w=0) - \mathcal{I}(w=1)}{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) }  \end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;Thus, the FIM under this  parameterization is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
F(\tau) &amp;amp;= E_{p(w|\tau) } [ \frac{ (\mathcal{I}(w=0) - \mathcal{I}(w=1) )^2 }{ (\mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) )^2 }  ]\\
&amp;amp; = \pi \frac{1^2}{ \pi^2} + (1-\pi) \frac{(-1)^2}{(1-\pi)^2} \\
&amp;amp;= \frac{1}{\pi } + \frac{1}{1-\pi} = \frac{1}{\pi(1-\pi)}&amp;gt; 0
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;/blockquote&gt;

&lt;h1 id=&quot;dimension-of-a-manifold&quot;&gt;Dimension of a manifold&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;We can define the dimension of a manifold by using the  degrees of freedom of an intrinsic parametrization.
Mathematically speaking, we can show that any intrinsic parametrization of a manifold has the same degrees of freedom &lt;a class=&quot;citation&quot; href=&quot;#tu2011introduction&quot;&gt;[11]&lt;/a&gt;.
This also gives us a tool to  identify non-manifold cases.
We now illustrate this by examples.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;unit circle&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;open unit ball&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;closed unit ball&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/circle-org.png&quot; alt=&quot;Source:Wikipedia&quot; width=&quot;200&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/open-ball.png&quot; alt=&quot;Source:Wikipedia&quot; width=&quot;200&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/closed-ball.png&quot; alt=&quot;Source:Wikipedia&quot; width=&quot;200&quot;/&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1-dim manifold&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2-dim manifold&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;non-manifold, which is indeed a manifold with (closed) boundary&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As we shown in &lt;a href=&quot;#intrinsic-parameterizations&quot;&gt;the previous section&lt;/a&gt;, a unit circle is a 1-dimensional manifold.  We can similarly show that an open unit ball is a 2-dimensional manifold.&lt;/p&gt;

&lt;p&gt;However, a closed
unit ball is NOT a manifold since its interior is an open unit ball and its boundary is a unit circle.
The  circle and the open unit ball do not have the same dimensionality.&lt;/p&gt;

&lt;p&gt;For statistical  manifolds, 
 consider the following examples. We will discuss more about them in &lt;a href=&quot;/posts/2021/10/Geomopt02/#riemannian-steepest-direction&quot;&gt;Part II&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;1-dim Gaussian with zero mean&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$d$-dim Gaussian with zero mean&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |0,s^{-1}) \Big|  s&amp;gt;0 \}$&lt;/code&gt; with precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$s$&lt;/code&gt; &lt;br /&gt; under intrinsic parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = s $&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(\mathbf{w} |\mathbf{0},\mathbf{S}^{-1}) \Big| \mathrm{MatH}(\tau) = \mathbf{S}   \succ \mathbf{0} \}$&lt;/code&gt; with precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;  &lt;br /&gt; under intrinsic parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \mathrm{vech}(\mathbf{S})$&lt;/code&gt;.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1-dim statistical manifold&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d(d+1)}{2}$&lt;/code&gt;-dim statistical  manifold&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We  use 
the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization&quot;&gt;half-vectorization map&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}()$&lt;/code&gt;.
The map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}(\mathbf{S})$&lt;/code&gt;  returns  a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d(d + 1)}{2}$&lt;/code&gt;-dim array obtained by vectorizing only the lower triangular part of a (symmetric) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$d$&lt;/code&gt;-by-&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$d$&lt;/code&gt; matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;.
The map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{MatH}()$&lt;/code&gt; is the inverse map of  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}()$&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Illustration of these two maps (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;
        &lt;p&gt;Consider the following symmetric 2-by-2 matrix&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\mathbf{S} = 
\begin{aligned}
\begin{bmatrix} 2 &amp;amp;  -1 \\  -1  &amp;amp; 3  \end{bmatrix}
\end{aligned}
$$&lt;/code&gt;
The output of map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}(\mathbf{S})$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\mathbf{v}:=\mathrm{vech}(\mathbf{S}) = 
\begin{aligned}
\begin{bmatrix} 2  \\  -1  \\ 3  \end{bmatrix}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;The output of  map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{MatH}(\mathbf{v})$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\mathrm{MatH}(\mathbf{v}) = 
\begin{aligned}
\begin{bmatrix} 2 &amp;amp;  -1 \\  -1  &amp;amp; 3  \end{bmatrix}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p class=&quot;bibliography&quot;&gt;&lt;p&gt;&lt;span id=&quot;amari1998natural&quot;&gt;[1] S.-I. Amari, &quot;Natural gradient works efficiently in learning,&quot; &lt;i&gt;Neural computation&lt;/i&gt; &lt;b&gt;10&lt;/b&gt;:251–276 (1998).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2021tractable&quot;&gt;[2] W. Lin, F. Nielsen, M. E. Khan, &amp;amp; M. Schmidt, &quot;Tractable structured natural gradient descent using local parameterizations,&quot; &lt;i&gt;International Conference on Machine Learning (ICML)&lt;/i&gt; (2021).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;liang2019fisher&quot;&gt;[3] T. Liang, T. Poggio, A. Rakhlin, &amp;amp; J. Stokes, &quot;Fisher-rao metric, geometry, and complexity of neural networks,&quot; &lt;i&gt;The 22nd International Conference on Artificial Intelligence and Statistics&lt;/i&gt; (PMLR, 2019), pp. 888–896.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;martens2020new&quot;&gt;[4] J. Martens, &quot;New Insights and Perspectives on the Natural Gradient Method,&quot; &lt;i&gt;Journal of Machine Learning Research&lt;/i&gt; &lt;b&gt;21&lt;/b&gt;:1–76 (2020).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;khan2017conjugate&quot;&gt;[5] M. Khan &amp;amp; W. Lin, &quot;Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models,&quot; &lt;i&gt;Artificial Intelligence and Statistics&lt;/i&gt; (PMLR, 2017), pp. 878–887.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;osawa2019practical&quot;&gt;[6] K. Osawa, S. Swaroop, A. Jain, R. Eschenhagen, R. E. Turner, R. Yokota, &amp;amp; M. E. Khan, &quot;Practical deep learning with Bayesian principles,&quot; &lt;i&gt;Proceedings of the 33rd International Conference on Neural Information Processing Systems&lt;/i&gt; (2019), pp. 4287–4299.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;wierstra2014natural&quot;&gt;[7] D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters, &amp;amp; J. Schmidhuber, &quot;Natural evolution strategies,&quot; &lt;i&gt;The Journal of Machine Learning Research&lt;/i&gt; &lt;b&gt;15&lt;/b&gt;:949–980 (2014).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;kakade2001natural&quot;&gt;[8] S. M. Kakade, &quot;A natural policy gradient,&quot; &lt;i&gt;Advances in neural information processing systems&lt;/i&gt; &lt;b&gt;14&lt;/b&gt; (2001).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;le2007topmoumoute&quot;&gt;[9] N. Le Roux, P.-A. Manzagol, &amp;amp; Y. Bengio, &quot;Topmoumoute Online Natural Gradient Algorithm.,&quot; &lt;i&gt;NIPS&lt;/i&gt; (Citeseer, 2007), pp. 849–856.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;duan2020ngboost&quot;&gt;[10] T. Duan, A. Anand, D. Y. Ding, K. K. Thai, S. Basu, A. Ng, &amp;amp; A. Schuler, &quot;Ngboost: Natural gradient boosting for probabilistic prediction,&quot; &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (PMLR, 2020), pp. 2690–2700.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;tu2011introduction&quot;&gt;[11] L. W. Tu, &quot;An introduction to manifolds. Second,&quot; &lt;i&gt;New York, US: Springer&lt;/i&gt; (2011).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;amari2016information&quot;&gt;[12] S.-ichi Amari, &lt;i&gt;Information geometry and its applications&lt;/i&gt; (Springer, 2016).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;casella2002statistical&quot;&gt;[13] G. Casella &amp;amp; R. L. Berger, &lt;i&gt;Statistical inference&lt;/i&gt; (2002).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;jaynes1957information&quot;&gt;[14] E. T. Jaynes, &quot;Information theory and statistical mechanics,&quot; &lt;i&gt;Physical review&lt;/i&gt; &lt;b&gt;106&lt;/b&gt;:620 (1957).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lee2018introduction&quot;&gt;[15] J. M. Lee, &lt;i&gt;Introduction to Riemannian manifolds&lt;/i&gt; (Springer, 2018).&lt;/span&gt;&lt;/p&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes:&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In differential geometry, an intrinsic parametrization is known as a coordinate chart. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In differential geometry, the smoothness requirement is known as a diffeomorphism. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><summary type="html">Goal This blog post focuses on the Fisher-Rao metric, which gives rise to the Fisher information matrix (FIM). We will introduce the following concepts useful to ensure non-singular FIMs Regularity conditions and intrinsic parameterization of a distribution Dimensionality of a smooth manifold</summary></entry><entry><title type="html">Structured Natural Gradient Descent (ICML 2021)</title><link href="/posts/2021/07/ICML/" rel="alternate" type="text/html" title="Structured Natural Gradient Descent (ICML 2021)" /><published>2021-07-05T00:00:00-07:00</published><updated>2021-07-05T00:00:00-07:00</updated><id>/posts/2021/07/GeomProj01</id><content type="html" xml:base="/posts/2021/07/ICML/">&lt;p&gt;More about this work &lt;a class=&quot;citation&quot; href=&quot;#lin2021tractable&quot;&gt;[1]&lt;/a&gt;: &lt;a href=&quot;https://www.youtube.com/watch?v=vEY1ZxDJX8o&amp;amp;t=11s&quot;&gt;(Youtube) talk&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2102.07405&quot;&gt;extended paper&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2107.10884&quot;&gt;short paper&lt;/a&gt;,
&lt;a href=&quot;/img/poster.pdf&quot;&gt;poster&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Many problems in optimization, search, and inference can be solved via natural-gradient descent (NGD)&lt;/p&gt;

&lt;p&gt;Structures play an essential role in&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Preconditioners of first-order and second-order optimization, gradient-free search.&lt;/li&gt;
  &lt;li&gt;Covariance matrices of variational Gaussian inference &lt;a class=&quot;citation&quot; href=&quot;#opper2009variational&quot;&gt;[2]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Natural-gradient descent on structured parameter spaces is computationally challenging.&lt;/p&gt;

&lt;p&gt;Limitations of existing NGD methods:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Limited structures due to the complicated Fisher information matrix (FIM)&lt;/li&gt;
  &lt;li&gt;Ad-hoc approximations for handling the singular FIM and cost reductions&lt;/li&gt;
  &lt;li&gt;Inefficient and complicated natural-gradient computation&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Existing approach for rank-one covariance&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Our NGD for rank-one covariance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-fig02.png&quot;  width=&quot;465&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-fig04.png&quot;  width=&quot;495&quot;/&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;our-contributions&quot;&gt;Our Contributions&lt;/h2&gt;

&lt;p&gt;We propose a flexible and efficient NGD method to incorporate structures via matrix Lie groups.&lt;/p&gt;

&lt;p&gt;Our NGD method&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;generalizes the exponential natural evolutionary strategy &lt;a class=&quot;citation&quot; href=&quot;#glasmachers2010exponential&quot;&gt;[3]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;recovers existing  Newton-like algorithms&lt;/li&gt;
  &lt;li&gt;yields new structured 2nd-order methods and adaptive-gradient methods with group-structural invariance &lt;a class=&quot;citation&quot; href=&quot;#lin2021structured&quot;&gt;[4]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;gives new NGD updates to learn structured covariances of Gaussian, Wishart and their mixtures&lt;/li&gt;
  &lt;li&gt;is a systemic approach to incorporate a range of structures&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;#applications&quot;&gt;Applications&lt;/a&gt; of our method:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;deep learning (structured adaptive-gradient),&lt;/li&gt;
  &lt;li&gt;non-convex optimization (structured 2nd-order),&lt;/li&gt;
  &lt;li&gt;evolution strategies (structured gradient-free),&lt;/li&gt;
  &lt;li&gt;variational mixture of Gaussians (Monte Carlo gradients for structured covariance).&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;ngd-for-optimization-inference-and-search&quot;&gt;NGD for Optimization, Inference, and Search&lt;/h1&gt;

&lt;p&gt;A unified  view for problems in optimization, inference, and search
as optimization over  (variational) parametric family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)$&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
   \min_{ \tau \in \Omega_\tau } \mathcal{L}(\tau):= \mathrm{E}_{q(\text{w}| \tau )} \big[ \ell(\mathbf{w}) \big] + \gamma \mathrm{E}_{q(\text{w} |\tau )} \big[ \log q(w|\tau) \big] 
\end{aligned} \tag{1}\label{1}
$$&lt;/code&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt; is the decision variable,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\ell(\mathbf{w})$&lt;/code&gt; is a loss function, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; is the parameter space of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q$&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma\ge 0$&lt;/code&gt; is a constant.&lt;/p&gt;

&lt;p&gt;Using gradient descent and natural-gradient descent to solve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\textrm{GD: } &amp;amp;\tau_{t+1} \leftarrow \tau_t - \alpha \nabla_{\tau_t} \mathcal{L}(\tau) \\
\textrm{Standard NGD: } &amp;amp; \tau_{t+1} \leftarrow \tau_t - \beta\,\, \big[ \mathbf{F}_{\tau} (\tau_t) \big]^{-1} \nabla_{\tau_t} \mathcal{L}(\tau)
\end{aligned} 
$$&lt;/code&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_{\tau} (\tau_t)$&lt;/code&gt; is the FIM of distribution &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=\tau_t$&lt;/code&gt;.
For an introduction to natural-gradient methods, see this &lt;a href=&quot;/posts/2021/09/Geomopt01/&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Advantages of NGD:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;recovers a Newton-like update for Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(\mathbf{w}|\mu,\mathbf{S})$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=(\mu,\mathbf{S})$&lt;/code&gt;, mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt;, and precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mu_{t+1}  &amp;amp; \leftarrow \mu_t - \beta \mathbf{S}_{t}^{-1} E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w \ell( \mathbf{w}) \big] } \\
\mathbf{S}_{t+1}  &amp;amp; \leftarrow (1-\beta \gamma)\mathbf{S}_t + \beta  E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) \big] }
\end{aligned} \tag{2}\label{2}
$$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;is less sensitive to parameter transformations  than GD&lt;/li&gt;
  &lt;li&gt;converges faster than GD&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/icml2021-fig01.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Challenges of standard NGD:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;NGD could violate parameterization constraints (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}_{t+1}$&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; may not be positive-definite)&lt;/li&gt;
  &lt;li&gt;Singular Fisher information matrix (FIM) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_{\tau}(\tau)$&lt;/code&gt; of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Limited precision/covariance structures&lt;/li&gt;
  &lt;li&gt;Ad-hoc approximations for cost reductions&lt;/li&gt;
  &lt;li&gt;Complicated and inefficient natural-gradient computation&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;ngd-using-local-parameterizations&quot;&gt;NGD using Local Parameterizations&lt;/h1&gt;

&lt;p&gt;Our method performs NGD updates in local parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; while maintaining structures via matrix groups in auxiliary parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt;. This decoupling enables a &lt;span style=&quot;color:red&quot;&gt;tractable&lt;/span&gt; update that exploits the structures in auxiliary parameter spaces.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&amp;lt;img src=&quot;/img/icml2021-fig03.png&quot;  width=&quot;500&quot;/&amp;gt;&lt;/td&gt;
      &lt;td&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; space has a local vector-space structure, &lt;br /&gt; standard NGD in $\tau$ space is a speical case of our NGD,  &lt;br /&gt; where we choose &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\psi$&lt;/code&gt; to be the identity map and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\phi_{\lambda_t}$&lt;/code&gt; to be a linear map.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We consider the following three kinds of parameterizations.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Global (original) parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;New auxiliary parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt;  with a surjective map: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau= \psi(\lambda)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Local parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; at a current value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t$&lt;/code&gt; with a local map:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda = \phi_{\lambda_t} (\eta)$&lt;/code&gt;,&lt;br /&gt; where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\phi_{\lambda_t}$&lt;/code&gt; is &lt;span style=&quot;color:red&quot;&gt; tight &lt;/span&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t$&lt;/code&gt;: $\lambda_t \equiv \phi_{\lambda_t} (\eta_0)$, and  we assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0 =\mathbf{0}$&lt;/code&gt; to be a relative origin.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;legend class=&quot;leg-title&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;Our NGD:&lt;/span&gt;&lt;/legend&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$ 
\begin{aligned} 
\lambda_{t+1} &amp;amp; \leftarrow  \phi_{\lambda_t} \big( \overbrace{\eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} }^{ \text{NGD in local space} } \big) \,\,\,\, \textrm{(structure-preserving update in auxiliary space)}\\
\tau_{t+1} &amp;amp; \leftarrow \psi\big( \lambda_{t+1} \big) \,\,\,\, \textrm{(constraint-satisfaction update in global space)}
\end{aligned} 
$$&lt;/code&gt;&lt;/p&gt;
  &lt;/fieldset&gt;
&lt;/div&gt;
&lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\eta_0}^{(t)}$&lt;/code&gt; is
 the natural-gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\eta_0}^{(t)}$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0$&lt;/code&gt; tied to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t$&lt;/code&gt;, which is computed by the chain rule,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned} 
        \hat{\mathbf{g}}_{\eta_0}^{(t)} &amp;amp;=  \color{green}{\mathbf{F}_{\eta}(\eta_0)^{-1} } 
         \,\, \big[ \nabla_{\eta_0} \big[ \psi \circ \phi_{\lambda_t} (\eta) \big]
        \nabla_{\tau_t}\mathcal{L}(\tau) \big]  
\end{aligned} 
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_{\eta}(\eta_0)$&lt;/code&gt;  is the (exact) FIM for  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0$&lt;/code&gt; tied to  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t$&lt;/code&gt;. 
Our method allows us to choose map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\psi \circ \phi_{\lambda_t}$&lt;/code&gt; so that
the FIM &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_{\eta}(\eta_0)$&lt;/code&gt; is easy to inverse at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0$&lt;/code&gt;, which enables tractable natural-gradient
computation.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;gaussian-example-with-full-precision&quot;&gt;Gaussian Example with Full Precision&lt;/h1&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;Notations:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{GL}^{p\times p}$&lt;/code&gt;: Invertible Matrices (General Linear Group),&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{D}^{p\times p}$&lt;/code&gt;: Diagonal Matrices,&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{D}_{++}^{p\times p}$&lt;/code&gt;: Diagonal and invertible Matrices (Diagonal Matrix Group),&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{S}_{++}^{p\times p}$&lt;/code&gt;: (Symmetric) positive-definite Matrices,&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{S}^{p\times p}$&lt;/code&gt;: Symmetric Matrices.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Consider a Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\mu,\mathbf{S})$&lt;/code&gt; with mean $\mu$ and precision $\mathbf{S}=\Sigma^{-1}$.&lt;/p&gt;

&lt;p&gt;The global, auxiliary, and local parameterizations are:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
        \tau &amp;amp;= \Big\{\mu \in \mathcal{R}^p, \mathbf{S} \in \mathcal{S}_{++}^{p\times p} \Big\},  &amp;amp; \mathbf{S}: \text{positive-definite matrix} \\
        \lambda &amp;amp; = \Big\{ \mu \in  \mathcal{R}^p , \mathbf{B} \in\mathrm{GL}^{p\times p} \Big\}, &amp;amp;\mathbf{B}: \text{ (closed) matrix Lie group member}\\
        \eta &amp;amp;= \Big\{ \delta\in  \mathcal{R}^p, \mathbf{M} \in\mathcal{S}^{p\times p}  \Big\}, &amp;amp; \mathbf{M}: \text{ Lie sub-algebra member}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Define &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{h}(\mathbf{M}):=\mathbf{I}+\mathbf{M}+\frac{1}{2} \mathbf{M}^2$&lt;/code&gt;. 
Maps &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\psi$&lt;/code&gt; and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\phi_{\lambda_t}$&lt;/code&gt; are :
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
        \Big\{ \begin{array}{c} \mu \\ \mathbf{S} \end{array} \Big\} = \psi(\lambda) &amp;amp; := \Big \{ \begin{array}{c} \mu \\ \mathbf{B}\mathbf{B}^\top \end{array} \Big \}, \\
       \Big \{ \begin{array}{c} \mu \\ \mathbf{B} \end{array} \Big \} = \phi_{\lambda_t}(\eta) &amp;amp; := \Big \{ \begin{array}{c} \mu_t + \mathbf{B}_t^{-T} \delta \\ \mathbf{B}_t \mathbf{h} (\mathbf{M}) \end{array} \Big \}.
\end{aligned} \tag{3}\label{3}
$$&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;

  &lt;p&gt;We propose using Lie-group retraction map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{h}()$&lt;/code&gt; to&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;keep natural-gradient computation tractable&lt;/li&gt;
    &lt;li&gt;maintain numerical stability&lt;/li&gt;
    &lt;li&gt;enable lower iteration cost compared to the matrix exponential map suggested in &lt;a class=&quot;citation&quot; href=&quot;#glasmachers2010exponential&quot;&gt;[3]&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Our NGD update in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  \lambda $&lt;/code&gt; space is shown below, where we assume $\eta_0=\mathbf{0}$.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
  \Big\{ \begin{array}{c} \mu_{t+1} \\ \mathbf{B}_{t+1} \end{array} \Big\} = \lambda_{t+1} = 
  \phi_{\lambda_t} \big( \eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} \big)
  =\Big\{ \begin{array}{c}  \mu_t - \beta \mathbf{B}_{t}^{-T} \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ \mathbf{B}_t  \mathbf{h}\big(\beta \mathbf{B}_t^{-1}\mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \end{array} \Big\} 
 \end{aligned}
$$&lt;/code&gt;
where &lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;tractable&lt;/strong&gt;&lt;/span&gt; natural-gradient  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\eta_0}^{(t)}$&lt;/code&gt;  at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0=\{\delta_0, \mathbf{M}_0\}$&lt;/code&gt; tied to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t=\{\mu_t,\mathbf{B}_t\}$&lt;/code&gt; is&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
    \hat{\mathbf{g}}_{\eta_0}^{(t)} =
 \Big(  \begin{array}{c} \hat{\mathbf{g}}_{\delta_0}^{(t)}\\ \mathrm{vec}(  \hat{\mathbf{g}}_{M_0}^{(t)})\end{array}   \Big)
= \underbrace{  \color{green}{\Big(\begin{array}{cc} \mathbf{I}_p &amp;amp; 0 \\ 0 &amp;amp; 2 \mathbf{I}_{p^2} \end{array} \Big)^{-1}}  }_{ \text{inverse of the exact FIM } } \Big[\begin{array}{c}  \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ \mathrm{vec}( -2\mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T}) \end{array} \Big] \,\,\,\,&amp;amp; (\text{tractable: easy to inverse FIM at  } \eta_0)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\mu$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_{\Sigma}$&lt;/code&gt; are Euclidean gradients of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; computed via Stein’s lemma &lt;a class=&quot;citation&quot; href=&quot;#opper2009variational&quot;&gt;[2]&lt;/a&gt; &lt;a class=&quot;citation&quot; href=&quot;#lin2019stein&quot;&gt;[5]&lt;/a&gt; :
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\mu = \nabla_{\mu}\mathcal{L}(\tau) = E_{q}{ \big[ \nabla_w \ell( \mathbf{w} ) \big] }, \,\,\,\,\,
\mathbf{g}_{\Sigma}  = \nabla_{S^{-1}}\mathcal{L}(\tau)
 = \frac{1}{2}  E_{q}{ \big[ \nabla_w^2 \ell( \mathbf{w}) \big] } - \frac{\gamma}{2} \mathbf{S} 
 \end{aligned} \tag{4}\label{4}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Our update on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}_{t+1}=\mathbf{B}_{t+1}\mathbf{B}_{t+1}^T$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu_{t+1}$&lt;/code&gt; is like update of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
&amp;amp; \mu_{t+1}   = \mu_t - \beta \mathbf{S}_{t}^{-1} E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w \ell( \mathbf{w} ) \big] } \\
&amp;amp;\mathbf{S}_{t+1} =   \underbrace{ \overbrace{(1-\beta \gamma)\mathbf{S}_t + \beta  E_{q(w|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) \big] }}^{\text{standard NGD on $\mathbf{S}$ }} + \color{red}{ \frac{\beta^2}{2} \mathbf{G}_t \mathbf{S}_t^{-1}\mathbf{G}_t}
}_{\color{red}{\text{ RGD with retraction}}}+ O(\beta^3)
 \end{aligned}
$$&lt;/code&gt; where $\mathbf{B}$ is a &lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;dense&lt;/strong&gt;&lt;/span&gt; matrix in matrix group &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{GL}^{p\times p}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{G}_t := E_{q(w|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) ] } -\gamma \mathbf{S}_t$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The second-order term shown in red is used for the positive-definite constraint &lt;a class=&quot;citation&quot; href=&quot;#lin2020handling&quot;&gt;[6]&lt;/a&gt; known as a retraction in Riemannian gradient descent (RGD).  The higher-order term &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(\beta^3)$&lt;/code&gt; will be used for structured precision matrices in the next section.&lt;/p&gt;

&lt;p&gt;Well-known (group) structures in matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}$&lt;/code&gt; are illustrated in the following figure.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Dense (invertible)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Triangular (Cholesky)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Diagonal (invertible)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-full.png&quot;  width=&quot;250&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-tri.png&quot;  width=&quot;250&quot;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-diag.png&quot;  width=&quot;250&quot;/&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;structured-gaussian-with-flexiable-precision&quot;&gt;Structured Gaussian with Flexiable Precision&lt;/h1&gt;

&lt;p&gt;Structures in precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}=\mathbf{B}\mathbf{B}^T$&lt;/code&gt; and matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}$&lt;/code&gt;
is a sparse (group) member as below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Block lower&lt;br /&gt; triangular&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Block upper&lt;br /&gt; triangular&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Hierarchical&lt;br /&gt; (lower Heisenberg)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Kronecker product&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Triangular-Toeplitz&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Sparse Cholesky&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-low.png&quot;  width=&quot;220&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-up.png&quot;  width=&quot;220&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-hie.png&quot;  width=&quot;220&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-kro.png&quot;  width=&quot;220&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-tri-Toep.png&quot;  width=&quot;220&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-sparse.png&quot;  width=&quot;220&quot;/&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} \mathbf{B}_A &amp;amp;  \mathbf{0}  \\ \mathbf{B}_C &amp;amp;  \mathbf{B}_D  \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} \mathbf{B}_A &amp;amp;  \mathbf{B}_B  \\ \mathbf{0} &amp;amp;  \mathbf{B}_D  \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} \mathbf{B}_A &amp;amp;  \mathbf{0}  \\ \mathbf{B}_C &amp;amp;  \begin{bmatrix} \mathbf{B}_{D_1} &amp;amp; \mathbf{0} \\ \mathbf{B}_{3} &amp;amp; \mathbf{B}_{4} \end{bmatrix} \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} d &amp;amp;  0  \\ s &amp;amp;  t  \end{bmatrix} \otimes \begin{bmatrix} r &amp;amp;  0 &amp;amp; 0 \\ {b}_1 &amp;amp; {o}_1 &amp;amp; 0 \\ {b}_2 &amp;amp; 0 &amp;amp; {o}_2     \end{bmatrix} $&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} r &amp;amp; 0 &amp;amp; 0 &amp;amp;0 \\ y &amp;amp;  r &amp;amp; 0 &amp;amp; 0  \\ g &amp;amp; y &amp;amp; r &amp;amp; 0 \\ b &amp;amp; g &amp;amp; y &amp;amp; r \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} \mathbf{B}_{D_1} &amp;amp; \mathbf{0} &amp;amp; \mathbf{0} \\ \mathbf{B}_{A} &amp;amp; \mathbf{B}_{B} &amp;amp; \mathbf{0} \\ \mathbf{B}_{D_2} &amp;amp; \mathbf{0} &amp;amp; \mathbf{B}_{D_3} \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;a-structured-gaussian-example&quot;&gt;A Structured Gaussian Example:&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Auxiliary  parameter $\mathbf{B}$ lives in  a structured space (matrix Lie group):  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{B}_{\text{up}}}(k)$&lt;/code&gt;,
a &lt;span style=&quot;color:red&quot;&gt;block upper-triangular&lt;/span&gt; sub-group of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{GL}^{p \times p}$&lt;/code&gt;;&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
{\cal{B}_{\text{up}}}(k)  := \Big\{ 
\begin{bmatrix}
\mathbf{B}_A &amp;amp;  \mathbf{B}_B  \\
\mathbf{0} &amp;amp; \mathbf{B}_D
     \end{bmatrix} \Big| &amp;amp; \mathbf{B}_A \in \mathrm{GL}^{k \times k},\,
\mathbf{B}_D  \in{\cal D}^{(p-k) \times (p-k)}_{++}  \Big\},\,\,
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$k=0$&lt;/code&gt;, the space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{B}_{\text{up}}}(0) = {\cal D}^{p \times p}_{++}$&lt;/code&gt; becomes  the diagonal case.
When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$k=p$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{B}_{\text{up}}}(p) = \mathrm{GL}^{p\times p}$&lt;/code&gt; becomes the dense case.&lt;/p&gt;

  &lt;p&gt;Consider a local parameter space (Lie sub-algebra): &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{M}_{\text{up}}}(k)$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
{\cal{M}_{\text{up}}}(k):  = \Big\{ 
\begin{bmatrix}
\mathbf{M}_A &amp;amp;  \mathbf{M}_B  \\
\mathbf{0} &amp;amp; \mathbf{M}_D
     \end{bmatrix} \Big| &amp;amp;  \mathbf{M}_A \in{\cal S}^{k \times k}, \,
\mathbf{M}_D  \in{\cal D}^{(p-k) \times (p-k)} \Big\}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;The global, auxiliary, and local parameterizations :
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
      \tau &amp;amp;= \Big\{\mu \in \mathcal{R}^p,  \mathbf{S}=\mathbf{B} \mathbf{B}^T \in \mathcal{S}_{++}^{p\times p} | \mathbf{B} \in {\cal{B}_{\text{up}}}(k) \Big\}, \\
       \lambda &amp;amp; = \Big\{ \mu \in \mathcal{R}^p,  \mathbf{B} \in {\cal{B}_{\text{up}}}(k) \Big\},\\
       \eta &amp;amp;= \Big\{ \delta\in \mathcal{R}^p,  \mathbf{M} \in {\cal{M}_{\text{up}}}(k)   \Big\}.
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;Maps &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\psi$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\phi_{\lambda_t}$&lt;/code&gt; are defined in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt;.
Our NGD update in the auxiliary space is shown below, where we assume $\eta_0=\mathbf{0}$.&lt;/p&gt;
  &lt;div class=&quot;notice--success&quot;&gt;
    &lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;legend class=&quot;leg-title&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;Structure-preserving update in $\lambda$ space&lt;/span&gt;&lt;/legend&gt;
      &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \Big\{ \begin{array}{c} \mu_{t+1} \\ \mathbf{B}_{t+1} \end{array} \Big\} = \lambda_{t+1} = 
 \phi_{\lambda_t} \big( \eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} \big)
 =\Big\{ \begin{array}{c}  \mu_t - \beta \mathbf{B}_{t}^{-T} \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ 
\mathbf{B}_t  \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \kappa_{\text{up}}\big( 2 \mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \Big)  \end{array} \Big\}   
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
    &lt;/fieldset&gt;
  &lt;/div&gt;
  &lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\odot$&lt;/code&gt; is the elementwise product ,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\kappa_{\text{up}}(\mathbf{X}) \in {\cal{M}_{\text{up}}}(k)$&lt;/code&gt; extracts non-zero entries of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{M}_{\text{up}}}(k)$&lt;/code&gt; from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{X}$&lt;/code&gt;, 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{C}_{\text{up}} = 
\begin{bmatrix}
\frac{1}{2} \mathbf{J}_A &amp;amp;  \mathbf{J}_B  \\
\mathbf{0} &amp;amp; \frac{1}{2} \mathbf{I}_D
     \end{bmatrix}  \in {\cal{M}_{\text{up}}}(k)$&lt;/code&gt;, and $\mathbf{J}$ is a matrix of ones.&lt;/p&gt;

  &lt;p&gt;Note that (see &lt;a class=&quot;citation&quot; href=&quot;#lin2021tractable&quot;&gt;[1]&lt;/a&gt;  for the detail)&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{B}_{t+1} \in$&lt;/code&gt; matrix Lie group &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{B}_{\text{up}}}(k)$&lt;/code&gt; since 
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
      \begin{aligned}
           &amp;amp;\mathbf{h}\big(\mathbf{M}\big) \in {\cal{B}_{\text{up}}}(k) \text{ for }  \mathbf{M} \in \text{ Lie sub-algebra } \cal{M}_{\text{up}}(k) \,\,\,\,  &amp;amp;(\text{by design, } \mathbf{h}(\cdot) \text{ is a Lie-group retraction}) \\
          &amp;amp;\mathbf{B}_{t} \in {\cal{B}_{\text{up}}}(k)  \,\,\,\, &amp;amp; (\text{by construction}) \\
          &amp;amp;\mathbf{B}_{t+1} =  \mathbf{B}_{t}\mathbf{h}\big(\mathbf{M}\big)  \,\,\,\, &amp;amp; (\text{closed under the group product}) 
      \end{aligned}
  $$&lt;/code&gt;&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}$&lt;/code&gt; also induces a low-rank-plus-diagonal structure in covariance
matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Sigma=\mathbf{S}^{-1}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}=\mathbf{B}\mathbf{B}^T$&lt;/code&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;In summary, our NGD method:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;is a systemic approach to incorporate structures&lt;/li&gt;
    &lt;li&gt;induces exact and non-singular FIMs&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;applications&quot;&gt;Applications&lt;/h1&gt;

&lt;h2 id=&quot;structured-2nd-order-methods-for-non-convex-optimization&quot;&gt;Structured 2nd-order Methods for Non-convex Optimization&lt;/h2&gt;

&lt;p&gt;Given an optimization problem
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\mu \in \mathcal{R}^p} \ell(\mu),
 \end{aligned}\tag{5}\label{5}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;we formulate a new problem over Gaussian &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(\mathbf{w}|\tau)$&lt;/code&gt; with structured precision, which is a speical case of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma=1$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
   \min_{\tau \in \Omega_\tau} E_{q(w|\tau)} \big[ \ell(\mathbf{w}) \big] +  E_{q(w|\tau)} \big[ \log q(\mathbf{w}|\tau)\big],
 \end{aligned}\tag{6}\label{6}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B} \in {\cal{B}_{\text{up}}}(k)$&lt;/code&gt; is a block upper-triangular group member,  $\tau=(\mu,\mathbf{S})$ with mean $\mu$ and precision matrix $\mathbf{S}=\mathbf{B}\mathbf{B}^T$.&lt;/p&gt;

&lt;p&gt;Using our NGD to solve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;gives the following update
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mu_{t+1}  &amp;amp; \leftarrow \mu_{t} - \beta \mathbf{S}_t^{-1} \mathbf{g}_{\mu_t},\\
\mathbf{B}_{t+1}  &amp;amp; \leftarrow   \mathbf{B}_t \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \kappa_{\text{up}}\big( 2 \mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \Big)
 \end{aligned}
$$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;obtains an update to solve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt; with group-structural invariance &lt;a class=&quot;citation&quot; href=&quot;#lin2021structured&quot;&gt;[4]&lt;/a&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \mu_{t+1} &amp;amp;  \leftarrow  \mu_t - \beta \mathbf{S}_{t}^{-1} \nabla_{\mu_t} \ell( \mu), \\ 
\mathbf{B}_{t+1} &amp;amp; \leftarrow 
   \mathbf{B}_t \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \color{red}{\kappa_{\text{up}}\big(  \mathbf{B}_t^{-1} \nabla_{\mu_t}^2 \ell( \mu)  \mathbf{B}_t^{-T} - \mathbf{I} \big)} \Big)
 \end{aligned}\tag{7}\label{7}
$$&lt;/code&gt; by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; evaluated at the mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu_t$&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_{\mu_t} \approx \nabla_{\mu_t} \ell( \mu),\,\,\,\,
 \mathbf{g}_{\Sigma_t} \approx \frac{1}{2} \big[ \nabla_{\mu_t}^2 \ell( \mu) - \mathbf{S}_t\big].
 \end{aligned}\tag{8}\label{8}
$$&lt;/code&gt; where $\Sigma=\mathbf{S}^{-1}$ is the covariance.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Group-structural invariance: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;

      &lt;p&gt;Recall that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B} \in {\cal{B}_{\text{up}}}(k)$&lt;/code&gt;. 
The update in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; is invariant under any (group) transform &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{R}^T \in  {\cal{B}_{\text{up}}}(k)$&lt;/code&gt; of  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt;  such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\min_{y \in \mathcal{R}^p}  \ell(\mathbf{R} \, \mathbf{y})$&lt;/code&gt;.&lt;/p&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Time complexity: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(k^2 p)$&lt;/code&gt; for triangular structure,&lt;/li&gt;
        &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O( (k_1^2+k_2^2) p)$&lt;/code&gt; for hierarchical structure.&lt;/li&gt;
      &lt;/ul&gt;

      &lt;p&gt;Implementation using Hessian-vector products (HVPs);&lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;Off-diagonal: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$k$&lt;/code&gt; HVPs (triangular), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(k_1+k_2)$&lt;/code&gt; HVPs (hierarchical),&lt;/li&gt;
        &lt;li&gt;Diagonal: compute/approximate diagonal entries of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_{\mu_t}^2 \ell( \mu)$&lt;/code&gt;.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Classical non-convex optimization: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;

      &lt;table&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th style=&quot;text-align: center&quot;&gt;: &amp;lt;font size=&quot;4&quot;&amp;gt; 200-dim non-separable, non-convex functions :|&lt;/th&gt;
            &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-rbfun.png&quot;  width=&quot;90%&quot;/&amp;gt;&lt;/th&gt;
            &lt;th style=&quot;text-align: left&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-dpfun.png&quot;  width=&quot;90%&quot;/&amp;gt;&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td style=&quot;text-align: center&quot;&gt;: &amp;lt;font size=&quot;3&quot;&amp;gt; Performance of our method with group structures (lower-triangular, upper-triangular, upper Heisenberg, lower Heisenberg), Adam, and BFGS &amp;lt;/font&amp;gt; :|&lt;/td&gt;
            &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;

    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h2 id=&quot;structured-adaptive-gradient-methods-for-deep-learning&quot;&gt;Structured Adaptive-gradient Methods for Deep Learning&lt;/h2&gt;
&lt;p&gt;At each NN layer,
consider a  Gaussian family
       &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(\mathbf{w}|\mu,\mathbf{S})$&lt;/code&gt; with a Kronecker product structure, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=(\mu,\mathbf{S})$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Our method gives adaptive-gradient updates with group-structural invariance  by
 approximating &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_{\mu_t}^2 \ell( \mu)$&lt;/code&gt;  in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{8}$&lt;/code&gt; using the Gauss-Newton.&lt;/p&gt;

&lt;p&gt;The Kronecker product (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}=\mathbf{B}_1 \otimes \mathbf{B}_2$&lt;/code&gt;) of two sparse structured groups (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}
_1$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}_2$&lt;/code&gt;) further reduces the time complexity, where precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}=\mathbf{B}\mathbf{B}^T= (\mathbf{B}_1 \mathbf{B}_1^T) \otimes (\mathbf{B}_2 \mathbf{B}_2^T)$&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Time complexity: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(k p)$&lt;/code&gt; for our Kronecker product with triangular groups, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$0&amp;lt;k&amp;lt;p^{1/2}$&lt;/code&gt;&lt;/li&gt;
        &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O( (k_1+k_2) p)$&lt;/code&gt; for our  Kronecker product with hierarchical groups, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$0&amp;lt;k_1+k_2&amp;lt;p^{1/2}$&lt;/code&gt;&lt;/li&gt;
        &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(p)$&lt;/code&gt; for Adam and our diagonal groups&lt;/li&gt;
        &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(p^{3/2})$&lt;/code&gt; for KFAC and our Kronecker product with dense groups&lt;/li&gt;
      &lt;/ul&gt;

      &lt;p&gt;Implementation:&lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;Automatically parallelized by Auto-Differentiation&lt;/li&gt;
        &lt;li&gt;No sequential conjugate-gradient (CG) steps&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Image classification problems: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;

      &lt;table&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th style=&quot;text-align: center&quot;&gt;: &amp;lt;font size=&quot;4&quot;&amp;gt; Kronecker product of lower-triangular groups for CNN &amp;lt;/font&amp;gt; :|&lt;/th&gt;
            &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-stl10.png&quot;  width=&quot;90%&quot;/&amp;gt;&lt;/th&gt;
            &lt;th style=&quot;text-align: left&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-cifar10.png&quot;  width=&quot;90%&quot;/&amp;gt;&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td style=&quot;text-align: center&quot;&gt;: &amp;lt;font size=&quot;4&quot;&amp;gt; Comparison between our method with Kronecker product groups and Adam &amp;lt;/font&amp;gt; :|&lt;/td&gt;
            &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;

    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h2 id=&quot;variational-inference-with-gaussian-mixtures&quot;&gt;Variational Inference with Gaussian Mixtures&lt;/h2&gt;

&lt;p&gt;Our NGD&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;can use structured Gaussian mixtures as flexiable variational distributions: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(\mathbf{w}|\tau)=\frac{1}{C}\sum_{c=1}^{C}q(\mathbf{w}|\mu_c,\mathbf{S}_c)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;gives efficient stochastic natural-gradient variational methods beyond mean-field/diagonal covariance&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Approximating 80-dim multimodal distributions: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;

      &lt;table&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th style=&quot;text-align: center&quot;&gt;: &amp;lt;font size=&quot;4&quot;&amp;gt; First 8 marginal distributions of Gaussian mixture approximation with upper-triangular structure &amp;lt;/font&amp;gt; :|&lt;/th&gt;
            &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-tmm80d-01.png&quot;  width=&quot;90%&quot;/&amp;gt;&lt;/th&gt;
            &lt;th style=&quot;text-align: left&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-tmm80d-02.png&quot;  width=&quot;90%&quot;/&amp;gt;&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td style=&quot;text-align: center&quot;&gt;:  &amp;lt;font size=&quot;4&quot;&amp;gt; Comparison between our approximation (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$k=5$&lt;/code&gt;) and the ground-truth (mixture of t distributions) &amp;lt;/font&amp;gt; :|&lt;/td&gt;
            &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;

    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p class=&quot;bibliography&quot;&gt;&lt;p&gt;&lt;span id=&quot;lin2021tractable&quot;&gt;[1] W. Lin, F. Nielsen, M. E. Khan, &amp;amp; M. Schmidt, &quot;Tractable structured natural gradient descent using local parameterizations,&quot; &lt;i&gt;International Conference on Machine Learning (ICML)&lt;/i&gt; (2021).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;opper2009variational&quot;&gt;[2] M. Opper &amp;amp; C. Archambeau, &quot;The variational Gaussian approximation revisited,&quot; &lt;i&gt;Neural computation&lt;/i&gt; &lt;b&gt;21&lt;/b&gt;:786–792 (2009).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;glasmachers2010exponential&quot;&gt;[3] T. Glasmachers, T. Schaul, S. Yi, D. Wierstra, &amp;amp; J. Schmidhuber, &quot;Exponential natural evolution strategies,&quot; &lt;i&gt;Proceedings of the 12th annual conference on Genetic and evolutionary computation&lt;/i&gt; (2010), pp. 393–400.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2021structured&quot;&gt;[4] W. Lin, F. Nielsen, M. E. Khan, &amp;amp; M. Schmidt, &quot;Structured second-order methods via natural gradient descent,&quot; &lt;i&gt;arXiv preprint arXiv:2107.10884&lt;/i&gt; (2021).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2019stein&quot;&gt;[5] W. Lin, M. E. Khan, &amp;amp; M. Schmidt, &quot;Stein’s Lemma for the Reparameterization Trick with Exponential Family Mixtures,&quot; &lt;i&gt;arXiv preprint arXiv:1910.13398&lt;/i&gt; (2019).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2020handling&quot;&gt;[6] W. Lin, M. Schmidt, &amp;amp; M. E. Khan, &quot;Handling the positive-definite constraint in the bayesian learning rule,&quot; &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (PMLR, 2020), pp. 6116–6126.&lt;/span&gt;&lt;/p&gt;&lt;/p&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Matrix Lie Groups" /><category term="Exponential Family" /><summary type="html">More about this work [1]: (Youtube) talk, extended paper, short paper, poster</summary></entry></feed>