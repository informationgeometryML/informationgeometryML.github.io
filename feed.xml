<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-03-18T00:03:52-07:00</updated><id>/feed.xml</id><title type="html">Information Geometry in Machine Learning</title><subtitle>Blog website for Information Geometry in ML.</subtitle><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><entry><title type="html">Part II: Derivation of Natural-gradients</title><link href="/posts/2021/10/Geomopt02/" rel="alternate" type="text/html" title="Part II: Derivation of Natural-gradients" /><published>2021-10-04T00:00:00-07:00</published><updated>2021-10-04T00:00:00-07:00</updated><id>/posts/2021/10/Geomopt02</id><content type="html" xml:base="/posts/2021/10/Geomopt02/">&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post focuses on the derivation of natural-gradients, which are known as Riemannian gradients with the Fisher-Rao metric. We will discuss the following concepts to derive natural-gradients:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Rimannian steepest direction (i.e.,  normalized natural gradient)&lt;/li&gt;
  &lt;li&gt;The difference between a parameter space and a gradient/vector space&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The discussion here is informal and focuses more on intuitions, rather than rigor.&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Click to see how to cite this blog post&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;

      &lt;div class=&quot;language-latex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;lin2021blog02,
  title = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Derivation of Natural-gradients&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;,
  author = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Lin, Wu and Nielsen, Frank and Khan, Mohammad Emtiyaz and Schmidt, Mark&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;,
  url = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;https://informationgeometryml.github.io/posts/2021/10/Geomopt02/&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;, 
  howpublished = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;https://informationgeometryml.github.io/posts/2021/10/Geomopt02/&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;,
  year = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;2021&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;,
  note = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Accessed: 2021-10-04&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;      &lt;/div&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;In machine learning, a common derivation of natural-gradients is via a Talyor expansion of the Kullback-Leibler divergence as we will
discuss in
Part IV. However, the derivation has the following limitations:
&lt;!--[Part IV]({ post_url 2021-11-15-Geomopt04 }#proximal-gradient-descent).    --&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It does not clearly illustrate the difference between a parameter space and a (natural) gradient space.&lt;/li&gt;
  &lt;li&gt;It implicitly assumes that the Fisher information matrix (FIM) is non-singular and the parameter space is open.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These limitations become essential when it comes to constrained parameters (i.e., covariance matrix in a Gaussian family).
Moreover, a natural gradient (in a gradient space) is invariant (will show in 
Part III)
&lt;!--[Part III]({ post_url 2021-11-02-Geomopt03 }#riemannian-steepest-direction-is-invariant) )--&gt;
while a natural-gradient update (in a parameter space) is only linearly invariant (will discuss in
Part IV).
&lt;!--[Part IV]({ post_url 2021-11-15-Geomopt04 }#natural-gradient-descent-is-linearly-invariant)--&gt;&lt;/p&gt;

&lt;p&gt;We will give an alternative &lt;a href=&quot;#riemannian-steepest-direction&quot;&gt;derivation&lt;/a&gt; of natural-gradients to emphasize the subtlety.&lt;/p&gt;

&lt;h1 id=&quot;euclidean-steepest-direction&quot;&gt;Euclidean Steepest Direction&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Before we discuss natural-gradients, we first revisit Euclidean gradients.&lt;/p&gt;

&lt;p&gt;We will show a normalized Euclidean gradient can be viewed as the Euclidean steepest direction. Later, we will generalize the steepest direction in Riemannian cases and show that the Riemannian steepest direction w.r.t. the Fisher-Rao metric is indeed a normalized natural-gradient.&lt;/p&gt;

&lt;p&gt;Consider a minimization problem &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\min_{\tau \in \mathcal{R}^K } \,\,f(\mathbf{\tau})$&lt;/code&gt; over a &lt;strong&gt;parameter space&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;.
Given the smooth scalar function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\cdot)$&lt;/code&gt;, we can define the (Euclidean) steepest direction at current point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; as the optimal solution to another optimization problem,
where we assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\tau f(\mathbf{\tau}_0)  \neq \mathbf{0}$&lt;/code&gt;.
We can express the optimization problem in terms of a &lt;strong&gt;directional derivative&lt;/strong&gt; along vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v} \in T_{\tau_0} (\mathcal{R}^K)$&lt;/code&gt;, where
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$T_{\tau_0} (\mathcal{R}^K)=\mathcal{R}^K$&lt;/code&gt; is a &lt;strong&gt;gradient space&lt;/strong&gt; attached at current point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;.
The optimal directional derivative, denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{opt}}$&lt;/code&gt;,  is the steepest direction.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\|v\|^2=1   } \lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t} = ( \nabla_\tau f(\mathbf{\tau}_0) )^T \mathbf{v} 
\end{aligned}\tag{1}\label{1}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It is easy to see that the optimal solution of Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{opt}}= -\frac{\nabla_\tau f(\mathbf{\tau}_0) }{\|\nabla_\tau f(\mathbf{\tau}_0) \|}$&lt;/code&gt;, which is the (Euclidean) steepest direction at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;In Euclidean cases, there is no difference between parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt; and (Euclidean) gradient space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$T_{\tau_0} (\mathcal{R}^K)=\mathcal{R}^K$&lt;/code&gt; at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;Later, we will see that there is a key difference between a parameter space and a gradient space in manifold cases.&lt;/p&gt;
&lt;/div&gt;

&lt;h1 id=&quot;weighted-norm-induced-by-the-fisher-rao-metric&quot;&gt;Weighted Norm Induced by the Fisher-Rao Metric&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;Now, we  formulate a similar optimization problem like Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt;  in order to generalize the steepest direction at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; in a Riemannian manifold.
To do so, we have to define the length of a vector in manifold cases. In 
&lt;!--[Part III]({ post_url 2021-11-02-Geomopt03 }#standard-euclidean-gradients-are-not-invariant), --&gt;
Part III,
we will show that the (standard) length does not preserve under a parameter transformation while the length induced by the Fisher-Rao metric does.
Thanks to the Fisher-Rao metric,  we will further show that a natural-gradient is invariant under parameter transformations.&lt;/p&gt;

&lt;p&gt;As mentioned at &lt;a href=&quot;/posts/2021/09/Geomopt01/#fisher-rao-metric&quot;&gt;Part I&lt;/a&gt;, the FIM &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}$&lt;/code&gt; is positive definite everywhere in an intrinsic parameter space. We can use the FIM to define the length/norm of a vector (e.g., a Riemannian gradient) $\mathbf{v}$ at a point in a manifold via a weighted inner product. We use an intrinsic parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt; to represent this point. Note that the FIM is evaluated at current point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\|\mathbf{v}\|_F := \sqrt{\mathbf{v}^T \mathbf{F}(\tau_0) \mathbf{v}}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The positive-definiteness of the FIM is essential since we do not want a non-zero vector to have a zero length.&lt;/p&gt;

&lt;p&gt;The distance (and orthogonality) between two &lt;span style=&quot;color:red&quot;&gt;vectors&lt;/span&gt; at  &lt;span style=&quot;color:red&quot;&gt;point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;&lt;/span&gt;  is also induced by the FIM since we can define them by the inner product as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
d(\mathbf{v},\mathbf{w}) := \|\mathbf{v}-\mathbf{w}\|_F
\end{aligned}
$$&lt;/code&gt;
where vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt; live in the same (vector) space at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/tmanifold.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;In the figure,
the vector space at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;  is just a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt; space. We do not care about whether it is embedded in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^3$&lt;/code&gt; space or not.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In manifold cases, we have to distinguish the difference between a point (e.g., parameter array $\tau_0$) and a vector (e.g., Riemannian gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; under a parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;).
This difference is crucial to (natural) gradient-based methods in 
&lt;!--[Part IV]({ post_url 2021-11-15-Geomopt04 }#two-kinds-of-spaces).--&gt;
Part IV.&lt;/p&gt;

&lt;div class=&quot;notice--danger&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;We do NOT define how to compute the distance between two points in the manifold, which will be discussed &lt;a href=&quot;#riemannian-gradients-as-tangent-vectors-optional&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;We also do NOT define how to compute the distance between a vector at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt; and another vector at a distinct point
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_1$&lt;/code&gt;, which involves the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Parallel_transport&quot;&gt;parallel transport&lt;/a&gt; in a curved space. For simplicity, we avoid defining the transport since it involves partial derivatives of the Fisher-Rao metric (closely related to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Christoffel_symbols#Relationship_to_parallel_transport_and_derivation_of_Christoffel_symbols_in_Riemannian_space&quot;&gt;Christoffel symbols&lt;/a&gt;).&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;h1 id=&quot;directional-derivatives-in-a-manifold&quot;&gt;Directional Derivatives in a Manifold&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;As we shown before, the objective function in Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; is a directional derivative in Euclidean cases.
The next step is to generalize the concept of directional derivatives in a manifold.&lt;/p&gt;

&lt;p&gt;Recall that a manifold should be locally like a vector space under &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;&lt;strong&gt;intrinsic&lt;/strong&gt; parameterization&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}$&lt;/code&gt;.
Using this parameterization, consider an optimization problem $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$, where the parameter space $\Omega_\tau$ is determined by the choice of a parameterization and the manifold. Recall that we have a local vector space structure in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; if we parametrize the manifold with an intrinsic parameterization.&lt;/p&gt;

&lt;p&gt;Therefore, we can similarly define a directional derivative&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; along Riemannian vector&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; $\mathbf{v}$ as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t}$&lt;/code&gt;, where $t$ is a scalar real number. The main point is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v}$&lt;/code&gt; stays in the parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; thanks to the &lt;strong&gt;local vector space&lt;/strong&gt; structure.&lt;/p&gt;

&lt;p&gt;Recall that we allow a &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;small perturbation&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$E$&lt;/code&gt; around &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt; contained in  parameter space  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$E \subset \Omega_\tau$&lt;/code&gt;) since  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}$&lt;/code&gt; is an intrinsic parameterization.
Therefore, when $|t|$ is small enough, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v} $&lt;/code&gt; stays in the parameter space and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\mathbf{\tau}_0+t\mathbf{v})$&lt;/code&gt; is well-defined.
Note that we only require &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v} \in \Omega_\tau$&lt;/code&gt; when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$|t|$&lt;/code&gt; is small enough. This is possible since a line segment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{\tau}_0+t\mathbf{v} \in E$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$E \subset \Omega_\tau$&lt;/code&gt;.
Technically, this is because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; is an open set in $\mathcal{R}^K$, where $K$ is the number of entries of parameter array &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sphere_simple.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Under &lt;strong&gt;intrinsic&lt;/strong&gt; parameterization $\mathbf{\tau}$, the directional derivative remains the same as in the Euclidean case thanks to the &lt;strong&gt;local vector space&lt;/strong&gt; structure in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$\begin{aligned} \lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t} = ( \nabla_\tau f(\mathbf{\tau}_0))^T \mathbf{v}. \end{aligned}$$&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;Note:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v}$&lt;/code&gt; stays in the parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; when scalar &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$|t|$&lt;/code&gt; is small enough.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; stays in the tangent vector space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$T_{\tau_0} (\Omega_\tau)$&lt;/code&gt; at current point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;In K-&lt;a href=&quot;/posts/2021/09/Geomopt01/#dimensionality-of-a-manifold&quot;&gt;dimensional&lt;/a&gt; manifold cases, we have  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau \subset T_{\tau_0} (\Omega_\tau)=\mathcal{R}^K$&lt;/code&gt;.
This is the &lt;strong&gt;key difference&lt;/strong&gt; between intrinsic parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; and  tangent space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$T_{\tau_0} (\Omega_\tau)$&lt;/code&gt; as illustrated by the following figure.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/img/sphere-1.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following example illustrates directional derivatives in manifold cases.&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Valid case: (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is a &lt;strong&gt;local intrinsic&lt;/strong&gt; parameterization for the unit sphere.&lt;/p&gt;

        &lt;p&gt;The line segment from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v} $&lt;/code&gt;  is shown in blue, which is the parameter representation of the yellow curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt; in the manifold.
We will show later that Riemannian gradient vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; under this parametrization at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; is the &lt;strong&gt;parameter representation&lt;/strong&gt; of the tangent vector of curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt; at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/img/sphere_simple.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

        &lt;div class=&quot;notice--danger&quot;&gt;
          &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;:
Curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt; often is NOT the shortest curve in the manifold from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt; to  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_1$&lt;/code&gt;.&lt;/p&gt;
        &lt;/div&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Invalid case: (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;

        &lt;p&gt;A directional derivative can be ill-defined under a &lt;strong&gt;non-intrinsic&lt;/strong&gt; parameterization.&lt;/p&gt;

        &lt;p&gt;We use &lt;a href=&quot;/posts/2021/09/Geomopt01/#intrinsic-parameterizations&quot;&gt;parameterization 3&lt;/a&gt; for unit circle &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{S}^1$&lt;/code&gt;, where the red line segment passes through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0=(0,1) \in \mathcal{S}^1 $&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/img/tangent_non.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;Any  point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0 + t\mathbf{v}$&lt;/code&gt; in the line segment leaves the manifold when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$t\neq 0$&lt;/code&gt;.  Thus, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\mathbf{\tau}_0+t\mathbf{v})$&lt;/code&gt; is not well-defined.
The main reason is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is not an intrinsic parameterization.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h1 id=&quot;riemannian-steepest-direction&quot;&gt;Riemannian Steepest Direction&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Recall that we have defined the length of a Riemannian vector and directional derivatives in a manifold.
Now, we can introduce the Riemannian steepest direction &lt;a class=&quot;citation&quot; href=&quot;#absil2009optimization&quot;&gt;[1]&lt;/a&gt; . We will use this to define/compute natural-gradients.&lt;/p&gt;

&lt;p&gt;By choosing an intrinsic parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}$&lt;/code&gt;, a minimization problem over a manifold  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{M}$&lt;/code&gt; can be translated as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$&lt;/code&gt; over  parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt;. Recall that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; has a local vector-space structure.&lt;/p&gt;

&lt;p&gt;Assuming &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\tau f(\mathbf{\tau}_0)  \neq \mathbf{0}$&lt;/code&gt;,  we define the Riemannian steepest direction as the
optimal solution to the following new optimization problem.  The optimization problem is expressed in terms of a
directional derivative along Riemannian vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v} \in T_{\tau_0} (\Omega_\tau)$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$T_{\tau_0}
(\Omega_\tau) = \mathcal{R}^K$&lt;/code&gt; is a vector space attached at current point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{ \color{red} {\|v\|_{F}^2=1} } ( \nabla_\tau f(\mathbf{\tau}_0) )^T  \mathbf{v} 
\end{aligned} \tag{2}\label{2}
$$&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;The optimal solution of  Eq. $\eqref{2}$ is $\mathbf{v}_{\text{opt}}= -\frac{ \mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0) }{\| \mathbf{F}^{-1}(\mathbf{\tau}_0)\nabla_\tau f(\mathbf{\tau}_0) \|_F}$ (click to see the derivation)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;

      &lt;p&gt;The Lagrangian function of this problem is given below, where $\lambda$ is a Lagrange multiplier. 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
L(\mathbf{v},\lambda) =  ( \nabla_\tau f(\mathbf{\tau}_0))^T \mathbf{v} + \lambda (\|v\|_{F}^2 - 1) = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0) + \lambda (\mathbf{v}^T \mathbf{F}(\mathbf{\tau}_0) \mathbf{v}  - 1) 
\end{aligned}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}(\mathbf{\tau}_0)$&lt;/code&gt; is the FIM evaluated at point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt;.&lt;/p&gt;

      &lt;p&gt;One of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions&quot;&gt;Karush–Kuhn–Tucker&lt;/a&gt; (KKT) necessary conditions implies that
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{0} = \nabla_{v} L(\mathbf{v}_{\text{opt}},\lambda) = \nabla_\tau f(\mathbf{\tau}_0) + 2 \lambda \mathbf{F}(\mathbf{\tau}_0) \mathbf{v}_{\text{opt}}
\end{aligned}
$$&lt;/code&gt;
Since we assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\tau f(\mathbf{\tau}_0)\neq 0$&lt;/code&gt;, the KKT condition implies that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda \neq 0$&lt;/code&gt;.
Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda \neq 0$&lt;/code&gt;, vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{opt}}$&lt;/code&gt; should be proportional to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0)$&lt;/code&gt;, where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}^{-1}(\mathbf{\tau}_0)$&lt;/code&gt; is well-defined since the FIM &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}(\mathbf{\tau}_0)$&lt;/code&gt; is positive definite.&lt;/p&gt;

      &lt;p&gt;Thus, the optimal solution of Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{opt}}= -\frac{ \mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0) }{\| \mathbf{F}^{-1}(\mathbf{\tau}_0)\nabla_\tau f(\mathbf{\tau}_0) \|_F}$&lt;/code&gt;, which gives us the Riemannian steepest direction at current &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;.&lt;/p&gt;

      &lt;p&gt;Note that the &lt;strong&gt;Euclidean&lt;/strong&gt; steepest direction &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{euclid}}= -\frac{ \nabla_\tau f(\mathbf{\tau}_0) }{\| \nabla_\tau f(\mathbf{\tau}_0) \|_F}$&lt;/code&gt; is &lt;strong&gt;not&lt;/strong&gt; the optimal solution of  Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}(\tau_0) \neq \mathbf{I}$&lt;/code&gt;.
We illustrate this by the following example.&lt;/p&gt;

      &lt;blockquote&gt;

        &lt;p&gt;Consider &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}(\tau_0)=\begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; \frac{1}{2} \end{bmatrix}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\tau f(\mathbf{\tau}_0)=\begin{bmatrix} 1\\1 \end{bmatrix}$&lt;/code&gt;.
We have the following results
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\| \mathbf{F}^{-1}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) \|_F^2  =  \nabla_\tau^T f(\mathbf{\tau}_0) \mathbf{F}^{-1}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) = 3; \,\,\,
\| \nabla_\tau f(\mathbf{\tau}_0) \|_F^2  =  \nabla_\tau^T f(\mathbf{\tau}_0) \mathbf{F}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) = \frac{3}{2}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{v}_{\text{opt}} = -\begin{bmatrix} \frac{1}{\sqrt{3}} \\ \frac{2}{\sqrt{3}} \end{bmatrix}; \,\,\,
\mathbf{v}_{\text{euclid}}=
-\begin{bmatrix} \sqrt{\frac{2}{3}} \\ \sqrt{\frac{2}{3}} \end{bmatrix}\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{v}_{\text{opt}}^T \nabla_\tau f(\mathbf{\tau}_0)= -\sqrt{3}  &amp;lt;  -\frac{2\sqrt{2}}{\sqrt{3}} = \mathbf{v}_{\text{euclid}}^T \nabla_\tau f(\mathbf{\tau}_0) 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;Therefore, the Euclidean steepest direction &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}_{\text{euclid}}$&lt;/code&gt; is not the optimal solution of  Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt;.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h2 id=&quot;definition-of-natural-gradients&quot;&gt;Definition of Natural-gradients&lt;/h2&gt;

&lt;p&gt;Given a scalar function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\mathbf{\tau})$&lt;/code&gt; with an intrinsic parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;, we define a (un-normalized) &lt;strong&gt;Riemannian gradient&lt;/strong&gt; as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{F}_\tau^{-1}(\mathbf{\tau}) \nabla_\tau f(\mathbf{\tau})$&lt;/code&gt;, where we denote the corresponding (un-normalized) &lt;strong&gt;Euclidean gradient&lt;/strong&gt; by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_\tau f(\mathbf{\tau})$&lt;/code&gt;.
In machine learning, we often use a learning-rate to control the length of a gradient instead of normalizing its length. 
Since we use the Fisher-Rao metric &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}$&lt;/code&gt;, the Riemannian gradient is also known as the &lt;strong&gt;natural gradient&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Now, we give an example to illustrate the definition.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Example: Univariate Gaussian&lt;/p&gt;

  &lt;p&gt;Consider the following scalar function
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
f(\tau):= E_{q(w|\tau)} [ w^2 + \log q(w|\tau) ]
= \mu^2 + \frac{1}{s} + \frac{1}{2} \log(s) - \frac{1}{2}(1+\log(2\pi))
\end{aligned}
$$&lt;/code&gt;
where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)= \mathcal{N}(w|\mu,s^{-1})$&lt;/code&gt; is a Gaussian family with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt;, variance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$s^{-1}$&lt;/code&gt;, 
  intrinsic parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=(\mu,s)$&lt;/code&gt;, and parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau=\{(\mu,s)|\mu \in \mathcal{R},s&amp;gt;0 \}$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;The FIM of Gaussian $q(w|\tau)$ under this parametrization is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}_\tau (\tau)  = -E_{q(w|\tau)} [ \nabla_\tau^2 \log q(w|\tau) ] 
=
\begin{bmatrix}
s &amp;amp; 0 \\
0 &amp;amp; \frac{1}{2s^2}
\end{bmatrix}
\end{aligned}
$$&lt;/code&gt;
Now, we consider a member $\tau_0=(0.5,1)$ in the Gaussian family.
The Euclidean gradient is 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\nabla_\tau f(\tau_0) =
\begin{bmatrix}
2 \mu \\
-\frac{1}{s^2} +\frac{1}{2s}
\end{bmatrix}_{\tau=\tau_0}
=\begin{bmatrix}
1 \\ -\frac{1}{2}
\end{bmatrix}
\end{aligned}
$$&lt;/code&gt;
The natural/Riemannian gradient is 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}_\tau^{-1} (\tau_0) \nabla_\tau f(\tau_0) =
\begin{bmatrix}
2 \mu s^{-1}  \\
( -\frac{1}{s^2} +\frac{1}{2s} ) (2s^2)
\end{bmatrix}_{\tau=\tau_0}
=\begin{bmatrix}
1 \\ -1
\end{bmatrix}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Example: Multivariate Gaussian (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;

        &lt;p&gt;Consider a $d$-dimensional Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(\mathbf{w} |\mathbf{0},\mathbf{S}^{-1}) \Big| \mathbf{S}   \succ \mathbf{0} \}$&lt;/code&gt; with zero mean and precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt; discussed in &lt;a href=&quot;/posts/2021/09/Geomopt01/#dimensionality-of-a-manifold&quot;&gt;Part I&lt;/a&gt;.&lt;/p&gt;

        &lt;p&gt;Parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \mathrm{vech}(\mathbf{S})$&lt;/code&gt; is intrinsic while
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta = \mathrm{vec}(\mathbf{S})$&lt;/code&gt; is not, where
map $\mathrm{vech}()$ is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization&quot;&gt;half-vectorization map&lt;/a&gt; and map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vec}()$&lt;/code&gt; is the standard vectorization map.
Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d(d+1)}{2}$&lt;/code&gt;-dim parameter array while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$d^2$&lt;/code&gt;-dim parameter array,&lt;/p&gt;

        &lt;p&gt;Recall that the FIM w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;  is singular since  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta = \mathrm{vec}(\mathbf{S})$&lt;/code&gt; is a non-intrinsic parameter with $d^2$ degrees of freedom.
Strictly speaking, a natural gradient/vector w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt; is not well-defined since the FIM is singular.&lt;/p&gt;

        &lt;p&gt;In the literature, a natural gradient w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt; is  defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{MatH}(\mathbf{v})$&lt;/code&gt;, where
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; is a valid natural gradient w.r.t. intrinsic parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=\mathrm{vech}(\mathbf{S})$&lt;/code&gt;
(see&lt;br /&gt;
&lt;!--[Part V]({ post_url 2021-12-14-Geomopt05 }#efficient-ngd-for-multivariate-gaussian)--&gt;
Part V
for the
details.)&lt;/p&gt;
      &lt;/blockquote&gt;

    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h1 id=&quot;riemannian-gradients-as-tangent-vectors-optional&quot;&gt;Riemannian Gradients as Tangent Vectors (Optional)&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;In the previous section, we only consider Riemannian vectors/gradients under a parametrization $\tau$.
Now, we will discuss abstract Riemannian vectors without specifying a parametrization &lt;a class=&quot;citation&quot; href=&quot;#tu2011introduction&quot;&gt;[2]&lt;/a&gt;. This concept is often used to show the invariance of Riemannian gradients, which will be discussed in 
&lt;!--[Part III]({ post_url 2021-11-02-Geomopt03 }#parameter-transform-and-invariance).--&gt;
Part III.&lt;/p&gt;

&lt;p&gt;A Riemannian gradient denoted by $\mathbf{v}(\tau)$ is indeed a tangent vector $\mathbf{v}$ of a smooth curve in the manifold under the parametrization $\tau$. 
The set of tangent vectors evaluated at $\mathbf{\tau}_0$ is called the tangent space at the corresponding point. 
We will illustrate this by an example.&lt;/p&gt;

&lt;p&gt;Let’s denote the unit sphere by $\mathcal{M}$, where we set the origin to be the center of the sphere. Point $\mathbf{x_0}=(0,0,1)$ is the north pole.
We use the following parameterization, where the top half of the sphere can be locally expressed as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{(\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2})|  \tau_x^2 + \tau_y^2 &amp;lt;1 \}$&lt;/code&gt; with parameter $\mathbf{\tau}=(\tau_x,\tau_y)$. 
Under parametrization $\mathbf{\tau}$, we have the following parametric representations.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;     &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Parametric representation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;North pole  $\mathbf{x_0}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$\mathbf{\tau}_0=(0,0)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Intrinsic parameter space&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;red space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau:=\{ (\tau_x,\tau_y)| \tau_x^2 + \tau_y^2 &amp;lt;1 \}$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Tangent space at $\mathbf{x_0}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;green space  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Yellow curve from $\mathbf{x_0}$ to $\mathbf{x_1}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;blue line segment from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0$&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{\tau}_0+t\mathbf{v}(\tau_0)$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/img/sphere.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0$&lt;/code&gt; is a parameter array, which is a representation of a point $\mathbf{x}_0$ while $\mathbf{v}(\tau_0)$ is  a Riemannian gradient, which is a representation of the tangent vector of curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma$&lt;/code&gt; at point $\mathbf{x}_0$.&lt;/p&gt;

&lt;div class=&quot;notice--danger&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;:
Be aware of the differences shown in the table.&lt;/p&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;     &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;parametric representation of&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;supported operations&lt;/th&gt;
      &lt;th&gt;distance  discussed in this post&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt; (vector/natural-gradient) space&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;tangent vector space at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;real scalar product, vector addition&lt;/td&gt;
      &lt;td&gt;defined&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; (point/parameter) space&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;top half of the manifold&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;span style=&quot;color:red&quot;&amp;gt; **local** &amp;lt;/span&amp;gt; scalar product, &amp;lt;span style=&quot;color:red&quot;&amp;gt;**local** &amp;lt;/span&amp;gt; vector addition&lt;/td&gt;
      &lt;td&gt;undefined&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Under &lt;strong&gt;intrinsic&lt;/strong&gt; parametrization $\tau$, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau \subset \mathcal{R}^2$&lt;/code&gt;. Thus, we can perform this operation in $\Omega_\tau$ space: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau_0 +t\mathbf{v}(\tau_0) \in \Omega_\tau$&lt;/code&gt; when scalar &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$|t|$&lt;/code&gt; is small enough. Note that we only define the &lt;a href=&quot;#weighted-norm-induced-by-the-fisher-rao-metric&quot;&gt;distance&lt;/a&gt; between two (Riemannian gradient) vectors in the tangent space. The distance between two points in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; space is undefined in this post.&lt;/p&gt;

&lt;h2 id=&quot;parameterization-free-representation-of--vector-mathbfv&quot;&gt;Parameterization-free Representation of  Vector $\mathbf{v}$&lt;/h2&gt;

&lt;p&gt;The tangent vector $\mathbf{v}$ at point $\mathbf{x_0}$  can be viewed as the &lt;strong&gt;tangent direction&lt;/strong&gt; of a (1-dimensional) smooth curve $\gamma(t) \in \mathcal{M}$, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(0)=\mathbf{x_0}$&lt;/code&gt; and   &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d {\gamma}(t) }{d t} \Big|_{t=0}=\mathbf{v}$&lt;/code&gt; and the support of $\gamma(t)$ denoted by $\mathbf{I}$ is an open interval in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^1$&lt;/code&gt; containing 0. 
Since a curve $\gamma(t)$ is a geometric object,  its tangent direction is also a geometric object.
Note that a parametrization is just a representation of a geometric object.
Thus, the tangent direction should be a parameterization-free representation of vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt;. 
For example, in physics, the concept of parameterization-free representation means that a law of physics should be independent of any (reference) coordinate system chosen by an observer.&lt;/p&gt;

&lt;p&gt;Readers who are not familiar with this abstract concept can safely skip this since in practice, we will always choose a
parametrization when it comes to computation.
A key takeaway is that a geometric property (i.e., length of a vector) should be invariant to the choice of valid parametrizations.&lt;/p&gt;

&lt;h2 id=&quot;parameterization-dependent-representation-of-vector-mathbfv&quot;&gt;Parameterization-dependent Representation of Vector $\mathbf{v}$&lt;/h2&gt;

&lt;p&gt;Given intrinsic parametrization $\tau$, we can define the parametric representation of the curve denoted by ${\gamma}_\tau(t)$, where the domain is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau \subset \mathcal{R}^1$&lt;/code&gt;.
The parametric representation of vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}$&lt;/code&gt; is defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0):= \frac{d {\gamma}_{\tau}(t) }{d t} \Big|_{t=0}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\gamma}_{\tau}(0)=\tau_0$&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example&lt;/p&gt;

  &lt;p&gt;Consider the yellow curve in the figure $\gamma(t) = (t v_{x}, t v_{y}, \sqrt{1 - t^2(v_{x}^2 + v_{y}^2) } ) \in \mathcal{M} $ 
and the blue line segment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\gamma}_{\tau}(t)= (t v_{x} , t v_y  ) \in \Omega_\tau $&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$|t|$&lt;/code&gt; must be small enough.&lt;/p&gt;

  &lt;p&gt;The parametric  representation of the vector is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0):= \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}=(v_x,v_y)$&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/img/sphere.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A Riemannian gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0)$&lt;/code&gt; can be viewed as a parametric representation of tangent vector  $\mathbf{v}$ as shown below.&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;Consider a smooth scalar function defined in the manifold $h: \mathcal{M} \to \mathcal{R}$. In the unit sphere case, consider &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h(\mathbf{z})$&lt;/code&gt; subject to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{z}^T \mathbf{z}=1$&lt;/code&gt;.
Under parameterization $\mathbf{\tau}$, we can locally re-expressed the function as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h_\tau(\mathbf{\tau}):=h( (\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2}) )$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau \in \Omega_\tau$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;By the definition of a directional derivative, the following identity holds for any smooth scalar function $h$: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$[\nabla h_\tau(\mathbf{\tau}_0)]^T \mathbf{v}(\mathbf{\tau}_0) =\frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0}$&lt;/code&gt;, where $h_\tau$ is the parametric representation of  $h$ . Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(h_\tau \circ {\gamma}_\tau) (t)$&lt;/code&gt; is a function defined from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau $&lt;/code&gt; to $\mathcal{R}^1$, where domain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{I}_\tau \subset \mathcal{R}^1$&lt;/code&gt;.&lt;/p&gt;

  &lt;div class=&quot;notice--success&quot;&gt;
    &lt;p&gt;The &lt;strong&gt;key&lt;/strong&gt; observation:&lt;/p&gt;

    &lt;p&gt;Function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(h_\tau \circ {\gamma}_\tau) (t)$&lt;/code&gt; becomes a standard real-scalar function thanks to parametrization $\tau$. Thus, we can safely use the standard chain rule.&lt;/p&gt;
  &lt;/div&gt;

  &lt;p&gt;By the chain rule, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0}=[\nabla h_\tau(\mathbf{\tau}_0)]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\gamma}_\tau(0)=\tau_0$&lt;/code&gt;. Thus,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0) =  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$&lt;/code&gt; since (Euclidean gradient) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla h_\tau(\mathbf{\tau}_0)$&lt;/code&gt; is an arbitrary vector in $\mathcal{R}^2$ and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; is a 2-dim parameter array.&lt;/p&gt;

  &lt;p&gt;In summary, a Riemannian gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{v}(\mathbf{\tau}_0)$&lt;/code&gt; can be viewed as a parametric representation of the tangent vector 
of curve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{x}_0$&lt;/code&gt; since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\gamma}_\tau(t)$&lt;/code&gt; is the parametric representation of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma(t)$&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;riemannian-gradient-space-has-a-vector-space-structure&quot;&gt;(Riemannian) Gradient Space has a Vector-space Structure&lt;/h2&gt;
&lt;p&gt;We can similarly define vector additions and real scalar products in a tangent vector space by using the tangent direction of a curve in the manifold with/without a parameterization.&lt;/p&gt;

&lt;p&gt;A key takeaway is that a vector space structure is an integral part of a tangent &lt;strong&gt;vector&lt;/strong&gt; space. On the other hand, we have to use an intrinsic parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; to artificially create a local vector space structure in parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt;.
We will discuss more about this difference in Part IV. 
&lt;!--[Part IV]({ post_url 2021-11-15-Geomopt04 }#two-kinds-of-spaces). --&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p class=&quot;bibliography&quot;&gt;&lt;p&gt;&lt;span id=&quot;absil2009optimization&quot;&gt;[1] P.-A. Absil, R. Mahony, &amp;amp; R. Sepulchre, &lt;i&gt;Optimization algorithms on matrix manifolds&lt;/i&gt; (Princeton University Press, 2009).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;tu2011introduction&quot;&gt;[2] L. W. Tu, &quot;An introduction to manifolds. Second,&quot; &lt;i&gt;New York, US: Springer&lt;/i&gt; (2011).&lt;/span&gt;&lt;/p&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes:&lt;/h2&gt;

&lt;!--[Part III]({ post_url 2021-11-02-Geomopt03 }). --&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For simplicity, we avoid defining a (coordinate-free) &lt;a href=&quot;https://en.wikipedia.org/wikiCovariant_derivative&quot;&gt;covariant derivative&lt;/a&gt;, which induces parallel transport. Given a smooth scalar field/function on a manifold, a coordinate representation of the covariant derivative remains the same as the Euclidean case. Note that the standard coordinate derivative  is identical to  the coordinate representation of the covariant derivative when it comes to a scalar field. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A Riemannian gradient is a coordinate representation of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors&quot;&gt;contravariant vector&lt;/a&gt; (A.K.A. a Riemannian vector) while a Euclidean gradient is a coordinate representation of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors&quot;&gt;covariant vector&lt;/a&gt; (A.K.A. a Riemannian covector). We will discuss their transformation rules in  Part III. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><summary type="html">Goal This blog post focuses on the derivation of natural-gradients, which are known as Riemannian gradients with the Fisher-Rao metric. We will discuss the following concepts to derive natural-gradients: Rimannian steepest direction (i.e., normalized natural gradient) The difference between a parameter space and a gradient/vector space</summary></entry><entry><title type="html">Part I: Smooth Manifolds with the Fisher-Rao Metric</title><link href="/posts/2021/09/Geomopt01/" rel="alternate" type="text/html" title="Part I: Smooth Manifolds with the Fisher-Rao Metric" /><published>2021-09-06T00:00:00-07:00</published><updated>2021-09-06T00:00:00-07:00</updated><id>/posts/2021/09/Geomopt01</id><content type="html" xml:base="/posts/2021/09/Geomopt01/">&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;This blog post focuses on the Fisher-Rao metric, which gives rise to the Fisher information matrix (FIM). We will introduce the following concepts, useful to ensure non-singular FIMs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regularity conditions and intrinsic parameterizations of a distribution&lt;/li&gt;
  &lt;li&gt;Dimensionality of a smooth manifold&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The discussion here is informal and focuses on more on intuitions, rather than rigor.&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Click to see how to cite this blog post&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;

      &lt;div class=&quot;language-latex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;lin2021blog01,
  title = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Smooth Manifolds with the Fisher-Rao Metric&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;,
  author = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Lin, Wu and Nielsen, Frank and Khan, Mohammad Emtiyaz and Schmidt, Mark&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;,
  url = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;https://informationgeometryml.github.io/posts/2021/09/Geomopt01/&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;, 
  howpublished = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;https://informationgeometryml.github.io/posts/2021/09/Geomopt01/&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;,
  year = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;2021&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;,
  note = &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Accessed: 2021-09-06&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;      &lt;/div&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;The goal of this blog is to introduce geometric structures associated with probability distribution. Why should we care about such geometric structures?
By exploiting the structures, we can&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;design efficient and simple algorithms &lt;a class=&quot;citation&quot; href=&quot;#amari1998natural&quot;&gt;[1]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;design robust methods that are less sensitive to re-parametrization &lt;a class=&quot;citation&quot; href=&quot;#lin2021tractable&quot;&gt;[2]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;understand the behavior of models/algorithms using tools from differential geometry, information geometry, and invariant theory &lt;a class=&quot;citation&quot; href=&quot;#liang2019fisher&quot;&gt;[3]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These benefits are relevant for the majority of machine learning methods, all of which make use of probability distributions of various kinds.&lt;/p&gt;

&lt;p&gt;Below, we give some common examples from the literature. A reader familiar with such examples can skip this part.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Empirical Risk Minimization&lt;/strong&gt; (frequentist estimation):&lt;/p&gt;

  &lt;p&gt;Given N input-output pairs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(x_i,y_i)$&lt;/code&gt;,  the least-square loss can be viewed as a finite-sample approximation of the expectation w.r.t. a probability distribution (data generating distribution),
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau}  \frac{1}{2n} \sum_{i=1}^{n}   (y_i-x_i^T\tau)^2 
 &amp;amp;= - \frac{1}{n}  \sum_{i=1}^{n}  \log \mathcal{N}(y_i | x_i^T\tau,1) + \text{constant}\\
&amp;amp; \approx  E_{ \color{red}  { p(x,y | \tau) } } [ - \log  p(x,y | \tau) ]
\end{aligned} \tag{1}\label{1}
$$&lt;/code&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(x,y | \tau) = \mathcal{N}(y | x^T\tau,1) p(x) $&lt;/code&gt; is assumed to be the data-generating distribution. Here, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathcal{N} (y | m, v) $&lt;/code&gt; denotes a normal distribution over &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ y $&lt;/code&gt; with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ m $&lt;/code&gt; and variance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ v $&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;Well-known algorithms such as  &lt;a href=&quot;https://en.wikipedia.org/wiki/Scoring_algorithm#Fisher_scoring&quot;&gt;&lt;strong&gt;Fisher scoring&lt;/strong&gt;&lt;/a&gt;  and &lt;strong&gt;(empirical) natural-gradient descent&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#martens2020new&quot;&gt;[4]&lt;/a&gt; are commonly used methods that exploit the geometric structure of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$p(x,y | \tau)$&lt;/code&gt;. These are examples of algorithms derived from a frequentist perspective, which can also be generalized to neural networks &lt;a class=&quot;citation&quot; href=&quot;#martens2020new&quot;&gt;[4]&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Variational Inference&lt;/strong&gt; (Bayesian estimation):&lt;/p&gt;

  &lt;p&gt;Given a prior &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(z) $&lt;/code&gt; and a likelihood &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(\mathcal{D} | z ) $&lt;/code&gt; over a latent vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$z$&lt;/code&gt; and known data &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathcal{D} $&lt;/code&gt;, we can approximate the exact posterior &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p( z | \mathcal{D} ) =\frac{p(z,\mathcal{D})}{p(\mathcal{D})} $&lt;/code&gt; by optimizing a variational objective with respect to  an approximated distribution &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ q(z | \tau) $&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau} \mathrm{KL} [ { q(z | \tau) || p( z | \mathcal{D} ) }   ] = E_{ \color{red}  {q(z | \tau)} } [ \log q(z | \tau) - \log p( z , \mathcal{D} )    ]  + \text{constant} 
\end{aligned} \tag{2}\label{2}
$$&lt;/code&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathrm{KL} [ q(z) ||  p(z) ] :=  E_{ {q(z)} } [ \log \big(\frac{q(z)}{p(z)}\big) ]$&lt;/code&gt; is the Kullback–Leibler divergence.&lt;/p&gt;

  &lt;p&gt;The &lt;strong&gt;natural-gradient variational inference&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#khan2017conjugate&quot;&gt;[5]&lt;/a&gt; is an algorithm that speeds up the inference by exploiting the geometry of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(z|\tau)$&lt;/code&gt; induced by the Fisher-Rao metric.
This approach is derived from a Bayesian  perspective, and can also be generalized to neural networks &lt;a class=&quot;citation&quot; href=&quot;#lin2021structured&quot;&gt;[6]&lt;/a&gt; and Bayesian neural networks &lt;a class=&quot;citation&quot; href=&quot;#osawa2019practical&quot;&gt;[7]&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Evolution Strategies and Policy-Gradient Methods&lt;/strong&gt; (Global optimization):&lt;/p&gt;

  &lt;p&gt;Global optimization methods often use a search distribution, denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi(a | \tau ) $&lt;/code&gt;, to find the global maximum of an objective &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$h(a)$&lt;/code&gt; by solving a problem of the following form:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\tau}  E_{ \color{red}  {\pi(a | \tau)} } [ h(a)  ]
\end{aligned} \tag{3}\label{3}
$$&lt;/code&gt;
Samples from the search distribution are evaluated through a “fitness” function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ h(a) $&lt;/code&gt;, and guide the optimization towards better optima.&lt;/p&gt;

  &lt;p&gt;The &lt;strong&gt;natural evolution strategies&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#wierstra2014natural&quot;&gt;[8]&lt;/a&gt; is an algorithm that speeds up the search process by exploiting the geometry of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi(a|\tau)$&lt;/code&gt;.
In the context of reinforcement learning,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi(a | \tau ) $&lt;/code&gt; is known as the policy distribution to generate actions and the natural evolution strategies is known as the &lt;strong&gt;natural policy gradient&lt;/strong&gt; method &lt;a class=&quot;citation&quot; href=&quot;#kakade2001natural&quot;&gt;[9]&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In all of the examples above, the objective function is expressed in terms of an expectation w.r.t. a distribution in red, parameterized with the parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \tau $&lt;/code&gt;. 
The geometric structure of a distribution &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w|\tau) $&lt;/code&gt; for the quantity &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ w $&lt;/code&gt; can be exploited to improve the learning algorithms. The table below summarizes the three examples. 
More applications of similar nature are discussed in &lt;a class=&quot;citation&quot; href=&quot;#le2007topmoumoute&quot;&gt;[10]&lt;/a&gt; and &lt;a class=&quot;citation&quot; href=&quot;#duan2020ngboost&quot;&gt;[11]&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Example       &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;meaning of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$w$&lt;/code&gt;        &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;distribution &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w|\tau) $&lt;/code&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Empirical Risk Minimization&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;observation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(x,y)$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$p(x,y|\tau)$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Variational Inference&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;latent variable $z$&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(z|\tau)$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Evolution Strategies&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;decision variable $a$&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi(a|\tau)$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;In general, we may have to compute or estimate the inverse of the FIM. However, in many useful machine learning applications, algorithms such as &lt;a class=&quot;citation&quot; href=&quot;#lin2021tractable&quot;&gt;[2]&lt;/a&gt;  &lt;a class=&quot;citation&quot; href=&quot;#martens2020new&quot;&gt;[4]&lt;/a&gt; &lt;a class=&quot;citation&quot; href=&quot;#khan2017conjugate&quot;&gt;[5]&lt;/a&gt; &lt;a class=&quot;citation&quot; href=&quot;#osawa2019practical&quot;&gt;[7]&lt;/a&gt;  &lt;a class=&quot;citation&quot; href=&quot;#wierstra2014natural&quot;&gt;[8]&lt;/a&gt;  &lt;a class=&quot;citation&quot; href=&quot;#kakade2001natural&quot;&gt;[9]&lt;/a&gt;  &lt;a class=&quot;citation&quot; href=&quot;#le2007topmoumoute&quot;&gt;[10]&lt;/a&gt; can be efficiently implemented without
explicitly computing the inverse of the FIM.&lt;/p&gt;

  &lt;p&gt;We discuss this in other posts; see&lt;br /&gt;
&lt;!--[Part V]({ post_url 2021-12-14-Geomopt05 }#efficient-ngd-for-multivariate-gaussians)--&gt;
Part V
and 
&lt;a href=&quot;/posts/2021/07/ICML/&quot;&gt;our ICML work&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In the rest of the post, we will mainly focus on the geometric structure of (finite-dimensional) parametric families, for example, a univariate Gaussian family.
The following figure illustrates four distributions in the Gaussian family denoted by
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&amp;gt;0 \}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w | \tau ): = \mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $&lt;/code&gt;  and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau :=(\mu,\sigma) $&lt;/code&gt;. We will later see that this family is a 2-dimensional manifold in the parameter space.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/gauss1d.png&quot; alt=&quot;Figure 2&quot; title=&quot;Source:Wikipedia&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;intrinsic-parameterizations&quot;&gt;Intrinsic Parameterizations&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;We start by discussing a special type of parameterizations, we call intrinsic parameterizations, which are useful to obtain non-singular FIMs.
An arbitrary parameterization may not always be appropriate for a smooth manifold &lt;a class=&quot;citation&quot; href=&quot;#tu2011introduction&quot;&gt;[12]&lt;/a&gt;. Rather, the parameterization should be such that the manifold is locally like a &lt;em&gt;flat&lt;/em&gt; vector space, for example, how the curved Earth surface looks flat to us, locally.
We will refer to such flat vector space as a &lt;em&gt;local&lt;/em&gt; vector-space structure (denote it by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ E $&lt;/code&gt;).&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;Local &lt;strong&gt;vector-space structure&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;It supports local &lt;strong&gt;vector additions&lt;/strong&gt;,  local &lt;strong&gt;real scalar products&lt;/strong&gt;, and their algebraic laws (i.e., the distributive law). (see 
&lt;!--[Part II]({ post_url 2021-10-04-Geomopt02 }#riemannian-gradients-as-tangent-vectors-optional)--&gt;
Part II
for the details.)&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Intrinsic parameterizations&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; are those that satisfy the following two conditions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We require that the parameter space of $\tau$, denoted by $\Omega_\tau$, be an &lt;strong&gt;open&lt;/strong&gt; set in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$K$&lt;/code&gt; is the number of entries of a parameter array. Intuitively, this ensures a local vector-space structure throughout the parameter space, which then ensures that a small, local perturbation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ E $&lt;/code&gt; at each point stays within $\Omega_\tau$.&lt;/li&gt;
  &lt;li&gt;We also require that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ E $&lt;/code&gt;  &lt;strong&gt;uniquely&lt;/strong&gt; and &lt;strong&gt;smoothly&lt;/strong&gt; represents  points in a manifold. The condition ensures arbitrary (smooth) parameter transformations should still represent the same sub-set. In other words, we require that
    &lt;ul&gt;
      &lt;li&gt;there exists &lt;strong&gt;bi-jective&lt;/strong&gt; maps among such parameterizations if they represent a common sub-set of points in the manifold.&lt;/li&gt;
      &lt;li&gt;the maps and their inverse maps are both &lt;strong&gt;smooth&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;In differential geometry, this  requirement is known as a diffeomorphism, which is a formal but more abstract definition of this requirement.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Intrinsic parameterizations satisfy the above two conditions, and lead to non-singular FIMs, as we will see soon.&lt;/p&gt;

&lt;p&gt;We will now discuss a simple case of a manifold, a unit circle in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt;, and give an example of an intrinsic parameterization and three non-intrinsic ones due to different reasons such as non-smoothness, non-openness, and non-uniqueness.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/circle.png&quot; title=&quot;Source:Wikipedia&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Parameterization 1 (an intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;A (local) parametrization at $ (0,1) $ highlighted in red for the circle is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ (t,\sqrt{1-t^2}) | -h&amp;lt;t&amp;lt;h \} $&lt;/code&gt;, where $h=0.1$. We use &lt;strong&gt;one&lt;/strong&gt; (scalar) parameter in this parametrization. The manifold is (locally) “flat” since we can always find a small &lt;strong&gt;1-dimensional&lt;/strong&gt; perturbation $E$ in the &lt;strong&gt;1-dimensional&lt;/strong&gt; parameter space  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_t=\{t|-h&amp;lt;t&amp;lt;h \} $&lt;/code&gt;. Therefore, this is an intrinsic parameterization.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/img/1d-perturbation.png&quot; title=&quot;Fig&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

  &lt;p&gt;We can similarly define a (local) parametrization at each point of the circle. In fact, we can use  finite (local) parameterizations to represent the whole circle as shown below.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/img/charts.png&quot; title=&quot;Source:Wikipedia&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, we discuss invalid cases, where not all conditions are satisfied.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Parameterization 2 (a non-intrinsic parameterization due to non-smoothness):&lt;/p&gt;

  &lt;p&gt;Let’s define a map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f : [0,2\pi) \rightarrow \mathcal{S}^1 $&lt;/code&gt; such that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$f(\theta) = (\sin \theta, \cos \theta ) $&lt;/code&gt;, where we use $\mathcal{S}^1$ to denote the circle.&lt;/p&gt;

  &lt;p&gt;A (global) parametrization of the circle is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ f(\theta) | \theta \in [0,2\pi)  \}$&lt;/code&gt;, where we use one (scalar) parameter.&lt;/p&gt;

  &lt;p&gt;This map $f$ is bijective and smooth. However, the parameter space is not open in $\mathcal{R}$, and its inverse map $f^{-1}$ is &lt;strong&gt;not&lt;/strong&gt; continunous at point $(0,1) \in  \mathcal{S}^1$. Therefore, this parametrization is not intrinsic.
In fact, there does not exist a (single) &lt;strong&gt;global&lt;/strong&gt; and intrinsic parametrization to represent the circle.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Smoothness of the inverse map is essential when it comes to reparametrization (A.K.A. parameter transformation). The smoothness, along with the inverse map, gives us a way to generate new intrinsic parameterizations. Essentially, in such case, the Jacobian matrix (to change between the parameterizations) is non-singular everywhere, and we can use the chain rule and inverse function theorem to jump between different intrinsic parameterizations.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Parametrization 3 (a non-intrinsic parameterization due to non-openness):&lt;/p&gt;

  &lt;p&gt;The circle does &lt;strong&gt;not&lt;/strong&gt; look like a flat space under the following parametrization
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ (x,y) | x^2+y^2=1, x,y \in \mathcal{R} \} $&lt;/code&gt;. The number of entries in this parameter array is 2.&lt;/p&gt;

  &lt;p&gt;The reason is that we cannot find a small &lt;strong&gt;2-dimensional&lt;/strong&gt; perturbation $E$ in the &lt;strong&gt;2-dimensional&lt;/strong&gt; parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_{\tau}=\{(x,y)|x^2+y^2=1 \} $&lt;/code&gt; due to the constraint $x^2+y^2=1$. In other words, $\Omega_\tau$ is not open in $\mathcal{R}^2$.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/img/2d-perturbation.png&quot; title=&quot;Fig&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Parametrization 4 (a non-intrinsic parameterization due to non-uniqueness):&lt;/p&gt;

  &lt;p&gt;Let’s consider the following non-intrinsic parametrization $\tau$ of the circle: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ (\frac{x}{\sqrt{x^2+y^2}}, \frac{y}{\sqrt{x^2+y^2}}) | x^2+y^2 \neq 0, x,y \in \mathcal{R}  \}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=(x,y)$&lt;/code&gt;. The parameter space $\Omega_\tau$ is open in $\mathcal{R}^2$.&lt;/p&gt;

  &lt;p&gt;This parametrization is not intrinsic since it does not uniquely represent a point in the circle. It is obvious to see that  $\tau_1=(x_1,y_1)$ and $\alpha \tau_1=(\alpha x_1,\alpha y_1)$ both represent the same point in the circle when scalar $\alpha&amp;gt;0$.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;h1 id=&quot;intrinsic-parameterizations-for-parametric-families&quot;&gt;Intrinsic Parameterizations for Parametric families&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;The examples in the previous section clearly show the importance of parameterization, and that it should be chosen carefully. Now, we discuss how to choose such a parameterization for a given parametric family.&lt;/p&gt;

&lt;p&gt;Roughly speaking, a parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \tau $&lt;/code&gt; for a family of distribution &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ p(w|\tau) $&lt;/code&gt; is intrinsic if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\log
p(w|\tau) $&lt;/code&gt; is both smooth and unique w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \tau $&lt;/code&gt; in its parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt;.
Below is the formal condition.&lt;/p&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Regularity Condition&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;For any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau \in \Omega_\tau$&lt;/code&gt;,  the set of partial derivatives 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \partial_{\tau_i} \log p(w|\tau) \} $&lt;/code&gt; is smooth w.r.t. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; and is a set of linearly independent functions of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$w$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;In other words, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\sum_i c_i [ \partial_{\tau_i} \log p(w|\tau)] = 0 $&lt;/code&gt; holds only when constant &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_i$&lt;/code&gt; is zero and the value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_i$&lt;/code&gt; does not depend on  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$w$&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Note that, due to the definition of the partial derivatives, this regularity condition implicitly assumes that the parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; is an open set in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^K$&lt;/code&gt;, where K is the number of entries in parameter array &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt;.
In other words, the openness requirement is not explicit and hidden within the regularity condition.
We will discuss more about this at &lt;a href=&quot;#caveats-of-the-fisher-matrix-computation&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The following examples illustrate the regularity condition.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 1 (regularity condition for an intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;We will write the regularity condition at a point for an intrinsic parameterization. Consider a 1-dimensional Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |\mu,v) \Big| \mu \in \mathcal{R}, v&amp;gt;0 \}$&lt;/code&gt; with mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt;, variance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$v$&lt;/code&gt;, and parametrization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\mu,v) $&lt;/code&gt;.
The partial derivatives are the following,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\mu} \log \mathcal{N}(w |\mu,v) = \frac{w-\mu}{v}, \,\,\, \partial_{v} \log \mathcal{N}(w |\mu,v) = \frac{ (w-\mu)^2 }{2 v^2} - \frac{1}{2 v} 
\end{aligned}
$$&lt;/code&gt; 
It is easy to see that these partial derivatives are smooth w.r.t. $\tau=(\mu,v)$ in its parameter space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau=\{(\mu,v)|\mu\in \mathcal{R}, v&amp;gt;0\}$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;Consider the partial derivatives at a point $(\mu=0, v=1)$,&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\mu} \log \mathcal{N}(w |\mu,v) \Big|_{\mu=0,v=1}= w, \,\,\, \partial_{v} \log \mathcal{N}(w |\mu,v) \Big|_{\mu=0,v=1} = \frac{ w^2 -1 }{2}
\end{aligned}
$$&lt;/code&gt;
For this point, the regularity condition will be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ c_1 w + c_2  (\frac{ w^2 -1 }{2}) = 0$&lt;/code&gt;.
For this to hold for all $w$, it is necessarily that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_1=c_2=0$&lt;/code&gt;, which implies linear independence.&lt;/p&gt;

  &lt;p&gt;A formal proof can be built to show that this holds for any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu \in \mathcal{R}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$v &amp;gt;0$&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 2 (regularity condition for a non-intrinsic parameterization):&lt;/p&gt;

  &lt;p&gt;By using a counterexample, we will show that the regularity condition fails for a non-intrinsic parameterization. Consider a Bernoulli family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0  \}$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1) $&lt;/code&gt;, where function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathcal{I}(\cdot) $&lt;/code&gt; is the indicator function.
The partial derivatives are&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\pi_0} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{\pi_1}{(\pi_0+\pi_1)^2}   
\end{aligned}
$$&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\partial_{\pi_1} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} 
\end{aligned}
$$&lt;/code&gt;
Note that when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_0 = \pi_0 \neq 0 $&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_1= \pi_1 \neq 0$&lt;/code&gt;, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$c_0 \frac{\pi_1}{(\pi_0+\pi_1)^2}  + c_1 \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} = 0$&lt;/code&gt;. 
Therefore, the partial derivatives are linearly dependent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In a similar fashion, we will also see (soon) that the regularity condition is also not satisfied for the following parameterization: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0, \pi_0+\pi_1=1  \}$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1)$&lt;/code&gt;. The main reason is that the parameter space is not open in $\mathcal{R}^2$.&lt;/p&gt;

&lt;p&gt;On the other hand, the condition holds for the following parameterization: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) (1-\pi_0)  \Big| 0&amp;lt;\pi_0&amp;lt;1  \}$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \pi_0$&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;fisher-rao-metric&quot;&gt;Fisher-Rao Metric&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;Given an intrinsic parameterization, the Fisher-Rao metric is defined as follows,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log p(w|\tau) \Big) ].
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We can also express the metric in a matrix form as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{F}(\tau) := E_{ p(w|\tau) }  [ \Big( \nabla_{\tau} \log p(w|\tau ) \Big)  \Big(\nabla_{\tau} \log p(w|\tau) \Big)^T ],
\end{aligned}
$$&lt;/code&gt;
where $K$ is the number of entries of parameter array $\tau$ and 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \nabla_{\tau} \log p(w|\tau ) := [ \partial_{\tau_1} \log p(w|\tau ), \cdots, \partial_{\tau_K} \log p(w|\tau ) ]^T  $&lt;/code&gt; is a column vector. The matrix form is also known as the &lt;strong&gt;Fisher information matrix&lt;/strong&gt; (FIM). Obviously, the form of the FIM depends on the choice of parameterizations. In many cases,  we could also compute the FIM as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{F}(\tau) := - E_{p(w|\tau) }  [  \nabla_{\tau}^2 \log p(w|\tau )  ]$&lt;/code&gt;.
The regularity condition guarantees that the FIM is non-singular if the matrix exists, that is, the expectation in the definition exists.&lt;/p&gt;

&lt;p&gt;In what follows, we will assume the metric to be well-defined, which makes the Fisher-Rao metric a valid Riemannian metric &lt;a class=&quot;citation&quot; href=&quot;#lee2018introduction&quot;&gt;[13]&lt;/a&gt; since the corresponding FIM is positive definite everywhere in its intrinsic parameter space.&lt;/p&gt;

&lt;h1 id=&quot;caveats-of-the-fisher-matrix-computation&quot;&gt;Caveats of the Fisher matrix computation&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;There are some caveats when it comes to the Fisher matrix computation. In particular, the regularity condition should be satisfied.
It is possible to define the FIM under a non-intrinsic parameterization. However, the FIM often is singular or ill-defined under a non-intrinsic  parameterization as shown below.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 1 (Ill-defined FIM):&lt;/p&gt;

  &lt;p&gt;Consider Bernoulli family  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0, \pi_0+\pi_1=1  \}$&lt;/code&gt; with non-intrinsic parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1)$&lt;/code&gt;.
The following computation is not correct. Do you make similar mistakes like this?&lt;/p&gt;

  &lt;p&gt;Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  p(w|\tau ) =  \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1)$&lt;/code&gt;. The derivative is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$  \begin{aligned} \nabla_{\tau} \log p(w|\tau ) = \frac{1}{p(w|\tau )} [\mathcal{I}(w=0) ,  \mathcal{I}(w=1)]^T  \end{aligned} \tag{4}\label{4}$$&lt;/code&gt;
Thus, by Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt;, the FIM under this  parameterization is&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$  \begin{aligned} F(\tau) = E_{p(w|\tau) } [ \frac{1}{p^2(w|\tau)} \begin{bmatrix} \mathcal{I}^2(w=0) &amp;amp;  \mathcal{I}(w=1)  \mathcal{I}(w=0) \\  \mathcal{I}(w=0)  \mathcal{I}(w=1) &amp;amp;  \mathcal{I}^2(w=1) \end{bmatrix} ]   = \begin{bmatrix} \frac{1}{\pi_0} &amp;amp;  0 \\ 0 &amp;amp;  \frac{1}{\pi_1} \end{bmatrix} \end{aligned}$$&lt;/code&gt;&lt;/p&gt;
  &lt;div class=&quot;notice--danger&quot;&gt;
    &lt;p&gt;This computation is not correct. Do you know why?&lt;/p&gt;
  &lt;/div&gt;

  &lt;div class=&quot;notice--info&quot;&gt;
    &lt;details&gt;
&lt;summary&gt;Reason: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
        &lt;p&gt;The key reason is that the parameter space is not open in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{R}^2$&lt;/code&gt; due to the equality constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi_0+\pi_1=1 $&lt;/code&gt;. Thus, Eq. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; is &lt;strong&gt;incorrect&lt;/strong&gt;.&lt;/p&gt;

        &lt;p&gt;By definition, a Bernoulli distribution is valid only when the constraint holds. 
Thus, the constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi_0+\pi_1=1 $&lt;/code&gt; must be satisfied when we compute the Fisher matrix since the computation involves computing the expectation w.r.t. this distribution.&lt;/p&gt;

        &lt;p&gt;Note that the gradient is defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \nabla_{\tau} \log p(w|\tau ) := [ \partial_{\pi_0} \log p(w|\tau ), \partial_{\pi_1} \log p(w|\tau ) ]^T $&lt;/code&gt;.&lt;/p&gt;

        &lt;p&gt;Unfortunately, these partial derivatives do not exist. By the definition of  partial derivative &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \partial_{\pi_0} \log p(w|\tau )$&lt;/code&gt;, we fix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi_1$&lt;/code&gt; and allow &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi_0$&lt;/code&gt; to change.
However, given that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi_1$&lt;/code&gt; is fixed and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi_0 $&lt;/code&gt; is fully determined by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\pi_1$&lt;/code&gt; due to the equality constraint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \pi_0+\pi_1=1 $&lt;/code&gt;. 
Therefore, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  \partial_{\pi_0} \log p(w|\tau ) $&lt;/code&gt; is not well-defined.
In other words, the above Fisher matrix computation is not correct since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \nabla_{\tau} \log p(w|\tau ) $&lt;/code&gt; does not exist.&lt;/p&gt;
      &lt;/fieldset&gt;
&lt;/details&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 2  (Singular FIM):&lt;/p&gt;

  &lt;p&gt;Consider Bernoulli family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0&amp;gt;0, \pi_1&amp;gt;0  \}$&lt;/code&gt; with  non-intrinsic parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1) $&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;Note that a Bernoulli distribution in the family is not uniquely represented by this parametrization. It is obvious to
see that $\tau_1 = (1,1)$ and $\tau_2=(2,2)$ both represent the same Bernoulli distribution.&lt;/p&gt;

  &lt;p&gt;The FIM under this  parameterization is singular as shown below.&lt;/p&gt;

  &lt;p&gt;Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  p(w|\tau ) =  \mathcal{I}(w=0)  \frac{\pi_0}{\pi_0+\pi_1}+ \mathcal{I}(w=1)  \frac{\pi_1}{\pi_0+\pi_1}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = (\pi_0,\pi_1)$&lt;/code&gt;. The derivative is&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$ 
\begin{aligned}
\nabla_{\tau} \log p(w|\tau ) = \frac{ \mathcal{I}(w=0) - \mathcal{I}(w=1)  }{p(w|\tau )} [\frac{\pi_1}{(\pi_0+\pi_1)^2} , \frac{-\pi_0}{(\pi_0+\pi_1)^2}]^T 
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;Thus, the FIM under this  parameterization is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned} 
F(\tau) = E_{p(w|\tau) } [ \frac{( \mathcal{I}(w=0) - \mathcal{I}(w=1))^2 }{p^2(w|\tau)} \begin{bmatrix} \frac{\pi_1^2}{(\pi_0+\pi_1)^4}  &amp;amp; \frac{-\pi_0 \pi_1}{(\pi_0+\pi_1)^4} \\ \frac{-\pi_0 \pi_1}{(\pi_0+\pi_1)^4}  &amp;amp;  \frac{\pi_0^2}{(\pi_0+\pi_1)^4} \end{bmatrix} ]   =\frac{1}{(\pi_0+\pi_1)^2} \begin{bmatrix} \frac{\pi_1}{\pi_0} &amp;amp;  -1 \\ -1 &amp;amp;  \frac{\pi_0}{\pi_1} \end{bmatrix}
\end{aligned}
$$&lt;/code&gt;
where this FIM is singular since the matrix determinant is zero as shown below. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathrm{det}\big( \begin{bmatrix} \frac{\pi_1}{\pi_0} &amp;amp;  -1 \\ -1 &amp;amp;  \frac{\pi_0}{\pi_1} \end{bmatrix}\big) = 0.
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, we give an example to show that the FIM of a Bernoulli family can be non-singular when we use an intrinsic parameterization.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example 3  (Non-singular FIM):&lt;/p&gt;

  &lt;p&gt;Consider Bernoulli family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi)  \Big| 0&amp;lt;\pi&amp;lt;1 \}$&lt;/code&gt; with  &lt;strong&gt;intrinsic&lt;/strong&gt; parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \pi $&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;The FIM under this parameterization is non-singular as shown below.&lt;/p&gt;

  &lt;p&gt;Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  p(w|\tau ) =  \mathcal{I}(w=0) \pi+ \mathcal{I}(w=1) (1-\pi)$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \pi$&lt;/code&gt;. The derivative is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned} \nabla_{\tau} \log p(w|\tau ) = \frac{\mathcal{I}(w=0) - \mathcal{I}(w=1)}{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) }  \end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;Thus, the FIM under this  parameterization is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
F(\tau) &amp;amp;= E_{p(w|\tau) } [ \frac{ (\mathcal{I}(w=0) - \mathcal{I}(w=1) )^2 }{ (\mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) )^2 }  ]\\
&amp;amp; = \pi \frac{1^2}{ \pi^2} + (1-\pi) \frac{(-1)^2}{(1-\pi)^2} \\
&amp;amp;= \frac{1}{\pi } + \frac{1}{1-\pi} = \frac{1}{\pi(1-\pi)}&amp;gt; 0
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;/blockquote&gt;

&lt;h1 id=&quot;dimensionality-of-a-manifold&quot;&gt;Dimensionality of a manifold&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;We can define the dimension of a manifold by using the  degrees of freedom of an intrinsic parametrization.
Mathematically speaking, we can show that any intrinsic parametrization of a manifold has the same degrees of freedom &lt;a class=&quot;citation&quot; href=&quot;#tu2011introduction&quot;&gt;[12]&lt;/a&gt;.
This also gives us a tool to  identify non-manifold cases.
We now illustrate this by examples.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;unit circle&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;open unit ball&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;closed unit ball&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/circle-org.png&quot; alt=&quot;Source:Wikipedia&quot; width=&quot;200&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/open-ball.png&quot; alt=&quot;Source:Wikipedia&quot; width=&quot;200&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/closed-ball.png&quot; alt=&quot;Source:Wikipedia&quot; width=&quot;200&quot;/&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1-dim manifold&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2-dim manifold&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;non-manifold, which is indeed a manifold with (closed) boundary&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As we shown in &lt;a href=&quot;#intrinsic-parameterizations&quot;&gt;the previous section&lt;/a&gt;, a unit circle is a 1-dimensional manifold.  We can similarly show that an open unit ball is a 2-dimensional manifold.&lt;/p&gt;

&lt;p&gt;However, a closed
unit ball is NOT a manifold since its interior is an open unit ball and its boundary is a unit circle.
The  circle and the open unit ball do not have the same dimensionality.&lt;/p&gt;

&lt;p&gt;For statistical  manifolds, 
 consider the following examples. We will discuss more about them in 
 &lt;!--[Part II]({ post_url 2021-10-04-Geomopt02 }#riemannian-steepest-direction).--&gt;
 Part II.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;1-dim Gaussian with zero mean&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$d$-dim Gaussian with zero mean&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(w |0,s^{-1}) \Big|  s&amp;gt;0 \}$&lt;/code&gt; with precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$s$&lt;/code&gt; &lt;br /&gt; under intrinsic parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = s $&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \{ \mathcal{N}(\mathbf{w} |\mathbf{0},\mathbf{S}^{-1}) \Big| \mathrm{MatH}(\tau) = \mathbf{S}   \succ \mathbf{0} \}$&lt;/code&gt; with precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;  &lt;br /&gt; under intrinsic parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau = \mathrm{vech}(\mathbf{S})$&lt;/code&gt;.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1-dim statistical manifold&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d(d+1)}{2}$&lt;/code&gt;-dim statistical  manifold&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We  use 
the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization&quot;&gt;half-vectorization map&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}()$&lt;/code&gt;.
The map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}(\mathbf{S})$&lt;/code&gt;  returns  a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\frac{d(d + 1)}{2}$&lt;/code&gt;-dim array obtained by vectorizing only the lower triangular part of a (symmetric) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$d$&lt;/code&gt;-by-&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$d$&lt;/code&gt; matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;.
The map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{MatH}()$&lt;/code&gt; is the inverse map of  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}()$&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
&lt;summary&gt;Illustration of these two maps (click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;blockquote&gt;
        &lt;p&gt;Consider the following symmetric 2-by-2 matrix&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\mathbf{S} = 
\begin{aligned}
\begin{bmatrix} 2 &amp;amp;  -1 \\  -1  &amp;amp; 3  \end{bmatrix}
\end{aligned}
$$&lt;/code&gt;
The output of map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{vech}(\mathbf{S})$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\mathbf{v}:=\mathrm{vech}(\mathbf{S}) = 
\begin{aligned}
\begin{bmatrix} 2  \\  -1  \\ 3  \end{bmatrix}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

        &lt;p&gt;The output of  map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{MatH}(\mathbf{v})$&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\mathrm{MatH}(\mathbf{v}) = 
\begin{aligned}
\begin{bmatrix} 2 &amp;amp;  -1 \\  -1  &amp;amp; 3  \end{bmatrix}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p class=&quot;bibliography&quot;&gt;&lt;p&gt;&lt;span id=&quot;amari1998natural&quot;&gt;[1] S.-I. Amari, &quot;Natural gradient works efficiently in learning,&quot; &lt;i&gt;Neural computation&lt;/i&gt; &lt;b&gt;10&lt;/b&gt;:251–276 (1998).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2021tractable&quot;&gt;[2] W. Lin, F. Nielsen, M. E. Khan, &amp;amp; M. Schmidt, &quot;Tractable structured natural gradient descent using local parameterizations,&quot; &lt;i&gt;International Conference on Machine Learning (ICML)&lt;/i&gt; (2021).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;liang2019fisher&quot;&gt;[3] T. Liang, T. Poggio, A. Rakhlin, &amp;amp; J. Stokes, &quot;Fisher-rao metric, geometry, and complexity of neural networks,&quot; &lt;i&gt;The 22nd International Conference on Artificial Intelligence and Statistics&lt;/i&gt; (PMLR, 2019), pp. 888–896.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;martens2020new&quot;&gt;[4] J. Martens, &quot;New Insights and Perspectives on the Natural Gradient Method,&quot; &lt;i&gt;Journal of Machine Learning Research&lt;/i&gt; &lt;b&gt;21&lt;/b&gt;:1–76 (2020).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;khan2017conjugate&quot;&gt;[5] M. Khan &amp;amp; W. Lin, &quot;Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models,&quot; &lt;i&gt;Artificial Intelligence and Statistics&lt;/i&gt; (PMLR, 2017), pp. 878–887.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2021structured&quot;&gt;[6] W. Lin, F. Nielsen, M. E. Khan, &amp;amp; M. Schmidt, &quot;Structured second-order methods via natural gradient descent,&quot; &lt;i&gt;arXiv preprint arXiv:2107.10884&lt;/i&gt; (2021).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;osawa2019practical&quot;&gt;[7] K. Osawa, S. Swaroop, A. Jain, R. Eschenhagen, R. E. Turner, R. Yokota, &amp;amp; M. E. Khan, &quot;Practical deep learning with Bayesian principles,&quot; &lt;i&gt;Proceedings of the 33rd International Conference on Neural Information Processing Systems&lt;/i&gt; (2019), pp. 4287–4299.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;wierstra2014natural&quot;&gt;[8] D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters, &amp;amp; J. Schmidhuber, &quot;Natural evolution strategies,&quot; &lt;i&gt;The Journal of Machine Learning Research&lt;/i&gt; &lt;b&gt;15&lt;/b&gt;:949–980 (2014).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;kakade2001natural&quot;&gt;[9] S. M. Kakade, &quot;A natural policy gradient,&quot; &lt;i&gt;Advances in neural information processing systems&lt;/i&gt; &lt;b&gt;14&lt;/b&gt; (2001).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;le2007topmoumoute&quot;&gt;[10] N. Le Roux, P.-A. Manzagol, &amp;amp; Y. Bengio, &quot;Topmoumoute Online Natural Gradient Algorithm.,&quot; &lt;i&gt;NIPS&lt;/i&gt; (Citeseer, 2007), pp. 849–856.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;duan2020ngboost&quot;&gt;[11] T. Duan, A. Anand, D. Y. Ding, K. K. Thai, S. Basu, A. Ng, &amp;amp; A. Schuler, &quot;Ngboost: Natural gradient boosting for probabilistic prediction,&quot; &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (PMLR, 2020), pp. 2690–2700.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;tu2011introduction&quot;&gt;[12] L. W. Tu, &quot;An introduction to manifolds. Second,&quot; &lt;i&gt;New York, US: Springer&lt;/i&gt; (2011).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lee2018introduction&quot;&gt;[13] J. M. Lee, &lt;i&gt;Introduction to Riemannian manifolds&lt;/i&gt; (Springer, 2018).&lt;/span&gt;&lt;/p&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes:&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In differential geometry, an intrinsic parametrization is known as a coordinate chart. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Riemannian Manifold" /><summary type="html">Goal This blog post focuses on the Fisher-Rao metric, which gives rise to the Fisher information matrix (FIM). We will introduce the following concepts, useful to ensure non-singular FIMs: Regularity conditions and intrinsic parameterizations of a distribution Dimensionality of a smooth manifold</summary></entry><entry><title type="html">Structured Natural Gradient Descent (ICML 2021)</title><link href="/posts/2021/07/ICML/" rel="alternate" type="text/html" title="Structured Natural Gradient Descent (ICML 2021)" /><published>2021-07-05T00:00:00-07:00</published><updated>2021-07-05T00:00:00-07:00</updated><id>/posts/2021/07/GeomProj01</id><content type="html" xml:base="/posts/2021/07/ICML/">&lt;p&gt;More about this work &lt;a class=&quot;citation&quot; href=&quot;#lin2021tractable&quot;&gt;[1]&lt;/a&gt;: &lt;a href=&quot;https://www.youtube.com/watch?v=vEY1ZxDJX8o&amp;amp;t=11s&quot;&gt;(Youtube) talk&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2102.07405&quot;&gt;extended paper&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2107.10884&quot;&gt;short paper&lt;/a&gt;,
&lt;a href=&quot;/img/poster.pdf&quot;&gt;poster&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Many problems in optimization, search, and inference can be solved via natural-gradient descent (NGD)&lt;/p&gt;

&lt;p&gt;Structures play an essential role in&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Preconditioners of first-order and second-order optimization, gradient-free search.&lt;/li&gt;
  &lt;li&gt;Covariance matrices of variational Gaussian inference &lt;a class=&quot;citation&quot; href=&quot;#opper2009variational&quot;&gt;[2]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Natural-gradient descent on structured parameter spaces is computationally challenging.&lt;/p&gt;

&lt;p&gt;Limitations of existing NGD methods:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Limited structures due to the complicated Fisher information matrix (FIM)&lt;/li&gt;
  &lt;li&gt;Ad-hoc approximations for handling the singular FIM and cost reductions&lt;/li&gt;
  &lt;li&gt;Inefficient and complicated natural-gradient computation&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Existing approach for rank-one covariance&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Our NGD for rank-one covariance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-fig02.png&quot;  width=&quot;465&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-fig04.png&quot;  width=&quot;495&quot;/&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;our-contributions&quot;&gt;Our Contributions&lt;/h2&gt;

&lt;p&gt;We propose a flexible and efficient NGD method to incorporate structures via matrix Lie groups.&lt;/p&gt;

&lt;p&gt;Our NGD method&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;generalizes the exponential natural evolutionary strategy &lt;a class=&quot;citation&quot; href=&quot;#glasmachers2010exponential&quot;&gt;[3]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;recovers existing  Newton-like algorithms&lt;/li&gt;
  &lt;li&gt;yields new structured 2nd-order methods and adaptive-gradient methods with group-structural invariance &lt;a class=&quot;citation&quot; href=&quot;#lin2021structured&quot;&gt;[4]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;gives new NGD updates to learn structured covariances of Gaussian, Wishart and their mixtures&lt;/li&gt;
  &lt;li&gt;is a systematic approach to incorporate a range of structures&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;#applications&quot;&gt;Applications&lt;/a&gt; of our method:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;deep learning (structured adaptive-gradient),&lt;/li&gt;
  &lt;li&gt;non-convex optimization (structured 2nd-order),&lt;/li&gt;
  &lt;li&gt;evolution strategies (structured gradient-free),&lt;/li&gt;
  &lt;li&gt;variational mixture of Gaussians (Monte Carlo gradients for structured covariance).&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;ngd-for-optimization-inference-and-search&quot;&gt;NGD for Optimization, Inference, and Search&lt;/h1&gt;

&lt;p&gt;A unified  view for problems in optimization, inference, and search
as optimization over  (variational) parametric family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)$&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
   \min_{ \tau \in \Omega_\tau } \mathcal{L}(\tau):= \mathrm{E}_{q(\text{w}| \tau )} \big[ \ell(\mathbf{w}) \big] + \gamma \mathrm{E}_{q(\text{w} |\tau )} \big[ \log q(w|\tau) \big] 
\end{aligned} \tag{1}\label{1}
$$&lt;/code&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{w}$&lt;/code&gt; is the decision variable,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\ell(\mathbf{w})$&lt;/code&gt; is a loss function, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Omega_\tau$&lt;/code&gt; is the parameter space of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q$&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma\ge 0$&lt;/code&gt; is a constant.&lt;/p&gt;

&lt;p&gt;Using gradient descent and natural-gradient descent to solve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\textrm{GD: } &amp;amp;\tau_{t+1} \leftarrow \tau_t - \alpha \nabla_{\tau_t} \mathcal{L}(\tau) \\
\textrm{Standard NGD: } &amp;amp; \tau_{t+1} \leftarrow \tau_t - \beta\,\, \big[ \mathbf{F}_{\tau} (\tau_t) \big]^{-1} \nabla_{\tau_t} \mathcal{L}(\tau)
\end{aligned} 
$$&lt;/code&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_{\tau} (\tau_t)$&lt;/code&gt; is the FIM of distribution &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=\tau_t$&lt;/code&gt;.
For an introduction to natural-gradient methods, see this &lt;a href=&quot;/posts/2021/09/Geomopt01/&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Advantages of NGD:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;recovers a Newton-like update for Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(\mathbf{w}|\mu,\mathbf{S})$&lt;/code&gt; with parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=(\mu,\mathbf{S})$&lt;/code&gt;, mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt;, and precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mu_{t+1}  &amp;amp; \leftarrow \mu_t - \beta \mathbf{S}_{t}^{-1} E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w \ell( \mathbf{w}) \big] } \\
\mathbf{S}_{t+1}  &amp;amp; \leftarrow (1-\beta \gamma)\mathbf{S}_t + \beta  E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) \big] }
\end{aligned} \tag{2}\label{2}
$$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;is less sensitive to parameter transformations  than GD&lt;/li&gt;
  &lt;li&gt;converges faster than GD&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/icml2021-fig01.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Challenges of standard NGD:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;NGD could violate parameterization constraints (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}_{t+1}$&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; may not be positive-definite)&lt;/li&gt;
  &lt;li&gt;Singular Fisher information matrix (FIM) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_{\tau}(\tau)$&lt;/code&gt; of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Limited precision/covariance structures&lt;/li&gt;
  &lt;li&gt;Ad-hoc approximations for cost reductions&lt;/li&gt;
  &lt;li&gt;Complicated and inefficient natural-gradient computation&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;ngd-using-local-parameterizations&quot;&gt;NGD using Local Parameterizations&lt;/h1&gt;

&lt;p&gt;Our method performs NGD updates in local parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; while maintaining structures via matrix groups in auxiliary parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt;. This decoupling enables a &lt;span style=&quot;color:red&quot;&gt;tractable&lt;/span&gt; update that exploits the structures in auxiliary parameter spaces.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&amp;lt;img src=&quot;/img/icml2021-fig03.png&quot;  width=&quot;500&quot;/&amp;gt;&lt;/td&gt;
      &lt;td&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; space has a local vector-space structure, &lt;br /&gt; standard NGD in $\tau$ space is a special case of our NGD,  &lt;br /&gt; where we choose &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\psi$&lt;/code&gt; to be the identity map and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\phi_{\lambda_t}$&lt;/code&gt; to be a linear map.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We consider the following three kinds of parameterizations.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Global (original) parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau$&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\tau)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;New auxiliary parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt;  with a surjective map: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau= \psi(\lambda)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Local parameterization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta$&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda$&lt;/code&gt; at a current value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t$&lt;/code&gt; with a local map:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda = \phi_{\lambda_t} (\eta)$&lt;/code&gt;,&lt;br /&gt; where  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\phi_{\lambda_t}$&lt;/code&gt; is &lt;span style=&quot;color:red&quot;&gt; tight &lt;/span&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t$&lt;/code&gt;: $\lambda_t \equiv \phi_{\lambda_t} (\eta_0)$, and  we assume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0 =\mathbf{0}$&lt;/code&gt; to be a relative origin.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;fieldset class=&quot;field-set&quot;&gt;
    &lt;legend class=&quot;leg-title&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;Our NGD:&lt;/span&gt;&lt;/legend&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$ 
\begin{aligned} 
\lambda_{t+1} &amp;amp; \leftarrow  \phi_{\lambda_t} \big( \overbrace{\eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} }^{ \text{NGD in local space} } \big) \,\,\,\, \textrm{(structure-preserving update in auxiliary space)}\\
\tau_{t+1} &amp;amp; \leftarrow \psi\big( \lambda_{t+1} \big) \,\,\,\, \textrm{(constraint-satisfaction update in global space)}
\end{aligned} 
$$&lt;/code&gt;&lt;/p&gt;
  &lt;/fieldset&gt;
&lt;/div&gt;
&lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\eta_0}^{(t)}$&lt;/code&gt; is
 the natural-gradient &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\eta_0}^{(t)}$&lt;/code&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0$&lt;/code&gt; tied to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t$&lt;/code&gt;, which is computed by the chain rule,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned} 
        \hat{\mathbf{g}}_{\eta_0}^{(t)} &amp;amp;=  \color{green}{\mathbf{F}_{\eta}(\eta_0)^{-1} } 
         \,\, \big[ \nabla_{\eta_0} \big[ \psi \circ \phi_{\lambda_t} (\eta) \big]
        \nabla_{\tau_t}\mathcal{L}(\tau) \big]  
\end{aligned} 
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_{\eta}(\eta_0)$&lt;/code&gt;  is the (exact) FIM for  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0$&lt;/code&gt; tied to  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t$&lt;/code&gt;. 
Our method allows us to choose map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\psi \circ \phi_{\lambda_t}$&lt;/code&gt; so that
the FIM &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{F}_{\eta}(\eta_0)$&lt;/code&gt; is easy to inverse at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0$&lt;/code&gt;, which enables tractable natural-gradient
computation.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;gaussian-example-with-full-precision&quot;&gt;Gaussian Example with Full Precision&lt;/h1&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;Notations:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{GL}^{p\times p}$&lt;/code&gt;: Invertible Matrices (General Linear Group),&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{D}^{p\times p}$&lt;/code&gt;: Diagonal Matrices,&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{D}_{++}^{p\times p}$&lt;/code&gt;: Diagonal and invertible Matrices (Diagonal Matrix Group),&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{S}_{++}^{p\times p}$&lt;/code&gt;: (Symmetric) positive-definite Matrices,&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathcal{S}^{p\times p}$&lt;/code&gt;: Symmetric Matrices.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Consider a Gaussian family &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(w|\mu,\mathbf{S})$&lt;/code&gt; with mean $\mu$ and precision $\mathbf{S}=\Sigma^{-1}$.&lt;/p&gt;

&lt;p&gt;The global, auxiliary, and local parameterizations are:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
        \tau &amp;amp;= \Big\{\mu \in \mathcal{R}^p, \mathbf{S} \in \mathcal{S}_{++}^{p\times p} \Big\},  &amp;amp; \mathbf{S}: \text{positive-definite matrix} \\
        \lambda &amp;amp; = \Big\{ \mu \in  \mathcal{R}^p , \mathbf{B} \in\mathrm{GL}^{p\times p} \Big\}, &amp;amp;\mathbf{B}: \text{ (closed) matrix Lie group member}\\
        \eta &amp;amp;= \Big\{ \delta\in  \mathcal{R}^p, \mathbf{M} \in\mathcal{S}^{p\times p}  \Big\}, &amp;amp; \mathbf{M}: \text{ Lie sub-algebra member}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Define &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{h}(\mathbf{M}):=\mathbf{I}+\mathbf{M}+\frac{1}{2} \mathbf{M}^2$&lt;/code&gt;. 
Maps &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\psi$&lt;/code&gt; and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\phi_{\lambda_t}$&lt;/code&gt; are :
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
        \Big\{ \begin{array}{c} \mu \\ \mathbf{S} \end{array} \Big\} = \psi(\lambda) &amp;amp; := \Big \{ \begin{array}{c} \mu \\ \mathbf{B}\mathbf{B}^\top \end{array} \Big \}, \\
       \Big \{ \begin{array}{c} \mu \\ \mathbf{B} \end{array} \Big \} = \phi_{\lambda_t}(\eta) &amp;amp; := \Big \{ \begin{array}{c} \mu_t + \mathbf{B}_t^{-T} \delta \\ \mathbf{B}_t \mathbf{h} (\mathbf{M}) \end{array} \Big \}.
\end{aligned} \tag{3}\label{3}
$$&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;

  &lt;p&gt;We propose using Lie-group retraction map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{h}()$&lt;/code&gt; to&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;keep natural-gradient computation tractable&lt;/li&gt;
    &lt;li&gt;maintain numerical stability&lt;/li&gt;
    &lt;li&gt;enable lower iteration cost compared to the matrix exponential map suggested in &lt;a class=&quot;citation&quot; href=&quot;#glasmachers2010exponential&quot;&gt;[3]&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Our NGD update in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$  \lambda $&lt;/code&gt; space is shown below, where we assume $\eta_0=\mathbf{0}$.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
  \Big\{ \begin{array}{c} \mu_{t+1} \\ \mathbf{B}_{t+1} \end{array} \Big\} = \lambda_{t+1} = 
  \phi_{\lambda_t} \big( \eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} \big)
  =\Big\{ \begin{array}{c}  \mu_t - \beta \mathbf{B}_{t}^{-T} \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ \mathbf{B}_t  \mathbf{h}\big(\beta \mathbf{B}_t^{-1}\mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \end{array} \Big\} 
 \end{aligned}
$$&lt;/code&gt;
where &lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;tractable&lt;/strong&gt;&lt;/span&gt; natural-gradient  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\hat{\mathbf{g}}_{\eta_0}^{(t)}$&lt;/code&gt;  at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eta_0=\{\delta_0, \mathbf{M}_0\}$&lt;/code&gt; tied to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\lambda_t=\{\mu_t,\mathbf{B}_t\}$&lt;/code&gt; is&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
    \hat{\mathbf{g}}_{\eta_0}^{(t)} =
 \Big(  \begin{array}{c} \hat{\mathbf{g}}_{\delta_0}^{(t)}\\ \mathrm{vec}(  \hat{\mathbf{g}}_{M_0}^{(t)})\end{array}   \Big)
= \underbrace{  \color{green}{\Big(\begin{array}{cc} \mathbf{I}_p &amp;amp; 0 \\ 0 &amp;amp; 2 \mathbf{I}_{p^2} \end{array} \Big)^{-1}}  }_{ \text{inverse of the exact FIM } } \Big[\begin{array}{c}  \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ \mathrm{vec}( -2\mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T}) \end{array} \Big] \,\,\,\,&amp;amp; (\text{tractable: easy to inverse FIM at  } \eta_0)
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_\mu$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{g}_{\Sigma}$&lt;/code&gt; are Euclidean gradients of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; computed via Stein’s lemma &lt;a class=&quot;citation&quot; href=&quot;#opper2009variational&quot;&gt;[2]&lt;/a&gt; &lt;a class=&quot;citation&quot; href=&quot;#lin2019stein&quot;&gt;[5]&lt;/a&gt; :
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_\mu = \nabla_{\mu}\mathcal{L}(\tau) = E_{q}{ \big[ \nabla_w \ell( \mathbf{w} ) \big] }, \,\,\,\,\,
\mathbf{g}_{\Sigma}  = \nabla_{S^{-1}}\mathcal{L}(\tau)
 = \frac{1}{2}  E_{q}{ \big[ \nabla_w^2 \ell( \mathbf{w}) \big] } - \frac{\gamma}{2} \mathbf{S} 
 \end{aligned} \tag{4}\label{4}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Our update on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}_{t+1}=\mathbf{B}_{t+1}\mathbf{B}_{t+1}^T$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu_{t+1}$&lt;/code&gt; is like update of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{2}$&lt;/code&gt; as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
&amp;amp; \mu_{t+1}   = \mu_t - \beta \mathbf{S}_{t}^{-1} E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w \ell( \mathbf{w} ) \big] } \\
&amp;amp;\mathbf{S}_{t+1} =   \underbrace{ \overbrace{(1-\beta \gamma)\mathbf{S}_t + \beta  E_{q(w|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) \big] }}^{\text{standard NGD on $\mathbf{S}$ }} + \color{red}{ \frac{\beta^2}{2} \mathbf{G}_t \mathbf{S}_t^{-1}\mathbf{G}_t}
}_{\color{red}{\text{ RGD with retraction}}}+ O(\beta^3)
 \end{aligned}
$$&lt;/code&gt; where $\mathbf{B}$ is a &lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;dense&lt;/strong&gt;&lt;/span&gt; matrix in matrix group &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{GL}^{p\times p}$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{G}_t := E_{q(w|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) ] } -\gamma \mathbf{S}_t$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The second-order term shown in red is used for the positive-definite constraint &lt;a class=&quot;citation&quot; href=&quot;#lin2020handling&quot;&gt;[6]&lt;/a&gt; known as a retraction in Riemannian gradient descent (RGD).  The higher-order term &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(\beta^3)$&lt;/code&gt; will be used for structured precision matrices in the next section.&lt;/p&gt;

&lt;p&gt;Well-known (group) structures in matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}$&lt;/code&gt; are illustrated in the following figure.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Dense (invertible)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Triangular (Cholesky)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Diagonal (invertible)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-full.png&quot;  width=&quot;250&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-tri.png&quot;  width=&quot;250&quot;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-diag.png&quot;  width=&quot;250&quot;/&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;structured-gaussian-with-flexible-precision&quot;&gt;Structured Gaussian with Flexible Precision&lt;/h1&gt;

&lt;p&gt;Structures in precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}=\mathbf{B}\mathbf{B}^T$&lt;/code&gt; and matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}$&lt;/code&gt;
is a sparse (group) member as below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Block lower&lt;br /&gt; triangular&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Block upper&lt;br /&gt; triangular&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Hierarchical&lt;br /&gt; (lower Heisenberg)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Kronecker product&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Triangular-Toeplitz&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Sparse Cholesky&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-low.png&quot;  width=&quot;220&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-up.png&quot;  width=&quot;220&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-hie.png&quot;  width=&quot;220&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-kro.png&quot;  width=&quot;220&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-tri-Toep.png&quot;  width=&quot;220&quot;/&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-group-sparse.png&quot;  width=&quot;220&quot;/&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} \mathbf{B}_A &amp;amp;  \mathbf{0}  \\ \mathbf{B}_C &amp;amp;  \mathbf{B}_D  \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} \mathbf{B}_A &amp;amp;  \mathbf{B}_B  \\ \mathbf{0} &amp;amp;  \mathbf{B}_D  \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} \mathbf{B}_A &amp;amp;  \mathbf{0}  \\ \mathbf{B}_C &amp;amp;  \begin{bmatrix} \mathbf{B}_{D_1} &amp;amp; \mathbf{0} \\ \mathbf{B}_{3} &amp;amp; \mathbf{B}_{4} \end{bmatrix} \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} d &amp;amp;  0  \\ s &amp;amp;  t  \end{bmatrix} \otimes \begin{bmatrix} r &amp;amp;  0 &amp;amp; 0 \\ {b}_1 &amp;amp; {o}_1 &amp;amp; 0 \\ {b}_2 &amp;amp; 0 &amp;amp; {o}_2     \end{bmatrix} $&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} r &amp;amp; 0 &amp;amp; 0 &amp;amp;0 \\ y &amp;amp;  r &amp;amp; 0 &amp;amp; 0  \\ g &amp;amp; y &amp;amp; r &amp;amp; 0 \\ b &amp;amp; g &amp;amp; y &amp;amp; r \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\begin{bmatrix} \mathbf{B}_{D_1} &amp;amp; \mathbf{0} &amp;amp; \mathbf{0} \\ \mathbf{B}_{A} &amp;amp; \mathbf{B}_{B} &amp;amp; \mathbf{0} \\ \mathbf{B}_{D_2} &amp;amp; \mathbf{0} &amp;amp; \mathbf{B}_{D_3} \end{bmatrix}$&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;a-structured-gaussian-example&quot;&gt;A Structured Gaussian Example:&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Auxiliary  parameter $\mathbf{B}$ lives in  a structured space (matrix Lie group):  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{B}_{\text{up}}}(k)$&lt;/code&gt;,
a &lt;span style=&quot;color:red&quot;&gt;block upper-triangular&lt;/span&gt; sub-group of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathrm{GL}^{p \times p}$&lt;/code&gt;;&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
{\cal{B}_{\text{up}}}(k)  := \Big\{ 
\begin{bmatrix}
\mathbf{B}_A &amp;amp;  \mathbf{B}_B  \\
\mathbf{0} &amp;amp; \mathbf{B}_D
     \end{bmatrix} \Big| &amp;amp; \mathbf{B}_A \in \mathrm{GL}^{k \times k},\,
\mathbf{B}_D  \in{\cal D}^{(p-k) \times (p-k)}_{++}  \Big\},\,\,
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$k=0$&lt;/code&gt;, the space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{B}_{\text{up}}}(0) = {\cal D}^{p \times p}_{++}$&lt;/code&gt; becomes  the diagonal case.
When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$k=p$&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{B}_{\text{up}}}(p) = \mathrm{GL}^{p\times p}$&lt;/code&gt; becomes the dense case.&lt;/p&gt;

  &lt;p&gt;Consider a local parameter space (Lie sub-algebra): &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{M}_{\text{up}}}(k)$&lt;/code&gt;.&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
{\cal{M}_{\text{up}}}(k):  = \Big\{ 
\begin{bmatrix}
\mathbf{M}_A &amp;amp;  \mathbf{M}_B  \\
\mathbf{0} &amp;amp; \mathbf{M}_D
     \end{bmatrix} \Big| &amp;amp;  \mathbf{M}_A \in{\cal S}^{k \times k}, \,
\mathbf{M}_D  \in{\cal D}^{(p-k) \times (p-k)} \Big\}
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;The global, auxiliary, and local parameterizations :
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
      \tau &amp;amp;= \Big\{\mu \in \mathcal{R}^p,  \mathbf{S}=\mathbf{B} \mathbf{B}^T \in \mathcal{S}_{++}^{p\times p} | \mathbf{B} \in {\cal{B}_{\text{up}}}(k) \Big\}, \\
       \lambda &amp;amp; = \Big\{ \mu \in \mathcal{R}^p,  \mathbf{B} \in {\cal{B}_{\text{up}}}(k) \Big\},\\
       \eta &amp;amp;= \Big\{ \delta\in \mathcal{R}^p,  \mathbf{M} \in {\cal{M}_{\text{up}}}(k)   \Big\}.
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;Maps &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\psi$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\phi_{\lambda_t}$&lt;/code&gt; are defined in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{3}$&lt;/code&gt;.
Our NGD update in the auxiliary space is shown below, where we assume $\eta_0=\mathbf{0}$.&lt;/p&gt;
  &lt;div class=&quot;notice--success&quot;&gt;
    &lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;legend class=&quot;leg-title&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;Structure-preserving update in $\lambda$ space&lt;/span&gt;&lt;/legend&gt;
      &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \Big\{ \begin{array}{c} \mu_{t+1} \\ \mathbf{B}_{t+1} \end{array} \Big\} = \lambda_{t+1} = 
 \phi_{\lambda_t} \big( \eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} \big)
 =\Big\{ \begin{array}{c}  \mu_t - \beta \mathbf{B}_{t}^{-T} \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ 
\mathbf{B}_t  \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \kappa_{\text{up}}\big( 2 \mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \Big)  \end{array} \Big\}   
\end{aligned}
$$&lt;/code&gt;&lt;/p&gt;
    &lt;/fieldset&gt;
  &lt;/div&gt;
  &lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\odot$&lt;/code&gt; is the elementwise product ,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\kappa_{\text{up}}(\mathbf{X}) \in {\cal{M}_{\text{up}}}(k)$&lt;/code&gt; extracts non-zero entries of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{M}_{\text{up}}}(k)$&lt;/code&gt; from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{X}$&lt;/code&gt;, 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{C}_{\text{up}} = 
\begin{bmatrix}
\frac{1}{2} \mathbf{J}_A &amp;amp;  \mathbf{J}_B  \\
\mathbf{0} &amp;amp; \frac{1}{2} \mathbf{I}_D
     \end{bmatrix}  \in {\cal{M}_{\text{up}}}(k)$&lt;/code&gt;, and $\mathbf{J}$ is a matrix of ones.&lt;/p&gt;

  &lt;p&gt;Note that (see &lt;a class=&quot;citation&quot; href=&quot;#lin2021tractable&quot;&gt;[1]&lt;/a&gt;  for the detail)&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ \mathbf{B}_{t+1} \in$&lt;/code&gt; matrix Lie group &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${\cal{B}_{\text{up}}}(k)$&lt;/code&gt; since 
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
      \begin{aligned}
           &amp;amp;\mathbf{h}\big(\mathbf{M}\big) \in {\cal{B}_{\text{up}}}(k) \text{ for }  \mathbf{M} \in \text{ Lie sub-algebra } \cal{M}_{\text{up}}(k) \,\,\,\,  &amp;amp;(\text{by design, } \mathbf{h}(\cdot) \text{ is a Lie-group retraction}) \\
          &amp;amp;\mathbf{B}_{t} \in {\cal{B}_{\text{up}}}(k)  \,\,\,\, &amp;amp; (\text{by construction}) \\
          &amp;amp;\mathbf{B}_{t+1} =  \mathbf{B}_{t}\mathbf{h}\big(\mathbf{M}\big)  \,\,\,\, &amp;amp; (\text{closed under the group product}) 
      \end{aligned}
  $$&lt;/code&gt;&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}$&lt;/code&gt; also induces a low-rank-plus-diagonal structure in covariance
matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\Sigma=\mathbf{S}^{-1}$&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}=\mathbf{B}\mathbf{B}^T$&lt;/code&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;notice--success&quot;&gt;
  &lt;p&gt;In summary, our NGD method:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;is a systematic approach to incorporate structures&lt;/li&gt;
    &lt;li&gt;induces exact and non-singular FIMs&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;applications&quot;&gt;Applications&lt;/h1&gt;

&lt;h2 id=&quot;structured-2nd-order-methods-for-non-convex-optimization&quot;&gt;Structured 2nd-order Methods for Non-convex Optimization&lt;/h2&gt;

&lt;p&gt;Given an optimization problem
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\min_{\mu \in \mathcal{R}^p} \ell(\mu),
 \end{aligned}\tag{5}\label{5}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;we formulate a new problem over Gaussian &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(\mathbf{w}|\tau)$&lt;/code&gt; with structured precision, which is a special case of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{1}$&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\gamma=1$&lt;/code&gt;.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
   \min_{\tau \in \Omega_\tau} E_{q(w|\tau)} \big[ \ell(\mathbf{w}) \big] +  E_{q(w|\tau)} \big[ \log q(\mathbf{w}|\tau)\big],
 \end{aligned}\tag{6}\label{6}
$$&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B} \in {\cal{B}_{\text{up}}}(k)$&lt;/code&gt; is a block upper-triangular group member,  $\tau=(\mu,\mathbf{S})$ with mean $\mu$ and precision matrix $\mathbf{S}=\mathbf{B}\mathbf{B}^T$.&lt;/p&gt;

&lt;p&gt;Using our NGD to solve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{6}$&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;gives the following update
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mu_{t+1}  &amp;amp; \leftarrow \mu_{t} - \beta \mathbf{S}_t^{-1} \mathbf{g}_{\mu_t},\\
\mathbf{B}_{t+1}  &amp;amp; \leftarrow   \mathbf{B}_t \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \kappa_{\text{up}}\big( 2 \mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \Big)
 \end{aligned}
$$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;obtains an update to solve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt; with group-structural invariance &lt;a class=&quot;citation&quot; href=&quot;#lin2021structured&quot;&gt;[4]&lt;/a&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
 \mu_{t+1} &amp;amp;  \leftarrow  \mu_t - \beta \mathbf{S}_{t}^{-1} \nabla_{\mu_t} \ell( \mu), \\ 
\mathbf{B}_{t+1} &amp;amp; \leftarrow 
   \mathbf{B}_t \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \color{red}{\kappa_{\text{up}}\big(  \mathbf{B}_t^{-1} \nabla_{\mu_t}^2 \ell( \mu)  \mathbf{B}_t^{-T} - \mathbf{I} \big)} \Big)
 \end{aligned}\tag{7}\label{7}
$$&lt;/code&gt; by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{4}$&lt;/code&gt; evaluated at the mean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mu_t$&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$
\begin{aligned}
\mathbf{g}_{\mu_t} \approx \nabla_{\mu_t} \ell( \mu),\,\,\,\,
 \mathbf{g}_{\Sigma_t} \approx \frac{1}{2} \big[ \nabla_{\mu_t}^2 \ell( \mu) - \mathbf{S}_t\big].
 \end{aligned}\tag{8}\label{8}
$$&lt;/code&gt; where $\Sigma=\mathbf{S}^{-1}$ is the covariance.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Group-structural invariance: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;

      &lt;p&gt;Recall that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B} \in {\cal{B}_{\text{up}}}(k)$&lt;/code&gt;. 
The update in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{7}$&lt;/code&gt; is invariant under any (group) transform &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{R}^T \in  {\cal{B}_{\text{up}}}(k)$&lt;/code&gt; of  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{5}$&lt;/code&gt;  such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\min_{y \in \mathcal{R}^p}  \ell(\mathbf{R} \, \mathbf{y})$&lt;/code&gt;.&lt;/p&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Time complexity: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(k^2 p)$&lt;/code&gt; for triangular structure,&lt;/li&gt;
        &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O( (k_1^2+k_2^2) p)$&lt;/code&gt; for hierarchical structure.&lt;/li&gt;
      &lt;/ul&gt;

      &lt;p&gt;Implementation using Hessian-vector products (HVPs);&lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;Off-diagonal: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$k$&lt;/code&gt; HVPs (triangular), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(k_1+k_2)$&lt;/code&gt; HVPs (hierarchical),&lt;/li&gt;
        &lt;li&gt;Diagonal: compute/approximate diagonal entries of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_{\mu_t}^2 \ell( \mu)$&lt;/code&gt;.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Classical non-convex optimization: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;

      &lt;table&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th style=&quot;text-align: center&quot;&gt;: &amp;lt;font size=&quot;4&quot;&amp;gt; 200-dim non-separable, non-convex functions :|&lt;/th&gt;
            &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-rbfun.png&quot;  width=&quot;90%&quot;/&amp;gt;&lt;/th&gt;
            &lt;th style=&quot;text-align: left&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-dpfun.png&quot;  width=&quot;90%&quot;/&amp;gt;&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td style=&quot;text-align: center&quot;&gt;: &amp;lt;font size=&quot;3&quot;&amp;gt; Performance of our method with group structures (lower-triangular, upper-triangular, upper Heisenberg, lower Heisenberg), Adam, and BFGS &amp;lt;/font&amp;gt; :|&lt;/td&gt;
            &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;

    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h2 id=&quot;structured-adaptive-gradient-methods-for-deep-learning&quot;&gt;Structured Adaptive-gradient Methods for Deep Learning&lt;/h2&gt;
&lt;p&gt;At each NN layer,
consider a  Gaussian family
       &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(\mathbf{w}|\mu,\mathbf{S})$&lt;/code&gt; with a Kronecker product structure, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\tau=(\mu,\mathbf{S})$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Our method gives adaptive-gradient updates with group-structural invariance  by
 approximating &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\nabla_{\mu_t}^2 \ell( \mu)$&lt;/code&gt;  in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{8}$&lt;/code&gt; using the Gauss-Newton.&lt;/p&gt;

&lt;p&gt;The Kronecker product (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}=\mathbf{B}_1 \otimes \mathbf{B}_2$&lt;/code&gt;) of two sparse structured groups (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}
_1$&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{B}_2$&lt;/code&gt;) further reduces the time complexity, where precision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\mathbf{S}=\mathbf{B}\mathbf{B}^T= (\mathbf{B}_1 \mathbf{B}_1^T) \otimes (\mathbf{B}_2 \mathbf{B}_2^T)$&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Time complexity: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(k p)$&lt;/code&gt; for our Kronecker product with triangular groups, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$0&amp;lt;k&amp;lt;p^{1/2}$&lt;/code&gt;&lt;/li&gt;
        &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O( (k_1+k_2) p)$&lt;/code&gt; for our  Kronecker product with hierarchical groups, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$0&amp;lt;k_1+k_2&amp;lt;p^{1/2}$&lt;/code&gt;&lt;/li&gt;
        &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(p)$&lt;/code&gt; for Adam and our diagonal groups&lt;/li&gt;
        &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$O(p^{3/2})$&lt;/code&gt; for KFAC and our Kronecker product with dense groups&lt;/li&gt;
      &lt;/ul&gt;

      &lt;p&gt;Implementation:&lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;Automatically parallelized by Auto-Differentiation&lt;/li&gt;
        &lt;li&gt;No sequential conjugate-gradient (CG) steps&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Image classification problems: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;

      &lt;table&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th style=&quot;text-align: center&quot;&gt;: &amp;lt;font size=&quot;4&quot;&amp;gt; Kronecker product of lower-triangular groups for CNN &amp;lt;/font&amp;gt; :|&lt;/th&gt;
            &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-stl10.png&quot;  width=&quot;90%&quot;/&amp;gt;&lt;/th&gt;
            &lt;th style=&quot;text-align: left&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-cifar10.png&quot;  width=&quot;90%&quot;/&amp;gt;&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td style=&quot;text-align: center&quot;&gt;: &amp;lt;font size=&quot;4&quot;&amp;gt; Comparison between our method with Kronecker product groups and Adam &amp;lt;/font&amp;gt; :|&lt;/td&gt;
            &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;

    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;h2 id=&quot;variational-inference-with-gaussian-mixtures&quot;&gt;Variational Inference with Gaussian Mixtures&lt;/h2&gt;

&lt;p&gt;Our NGD&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;can use structured Gaussian mixtures as flexible variational distributions: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$q(\mathbf{w}|\tau)=\frac{1}{C}\sum_{c=1}^{C}q(\mathbf{w}|\mu_c,\mathbf{S}_c)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;gives efficient stochastic natural-gradient variational methods beyond mean-field/diagonal covariance&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;notice--info&quot;&gt;
  &lt;details&gt;
	&lt;summary&gt;Approximating 80-dim multimodal distributions: (Click to expand)&lt;/summary&gt;
&lt;fieldset class=&quot;field-set&quot;&gt;

      &lt;table&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th style=&quot;text-align: center&quot;&gt;: &amp;lt;font size=&quot;4&quot;&amp;gt; First 8 marginal distributions of Gaussian mixture approximation with upper-triangular structure &amp;lt;/font&amp;gt; :|&lt;/th&gt;
            &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th style=&quot;text-align: center&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-tmm80d-01.png&quot;  width=&quot;90%&quot;/&amp;gt;&lt;/th&gt;
            &lt;th style=&quot;text-align: left&quot;&gt;&amp;lt;img src=&quot;/img/icml2021-tmm80d-02.png&quot;  width=&quot;90%&quot;/&amp;gt;&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td style=&quot;text-align: center&quot;&gt;:  &amp;lt;font size=&quot;4&quot;&amp;gt; Comparison between our approximation (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$k=5$&lt;/code&gt;) and the ground-truth (mixture of t distributions) &amp;lt;/font&amp;gt; :|&lt;/td&gt;
            &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;

    &lt;/fieldset&gt;
&lt;/details&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p class=&quot;bibliography&quot;&gt;&lt;p&gt;&lt;span id=&quot;lin2021tractable&quot;&gt;[1] W. Lin, F. Nielsen, M. E. Khan, &amp;amp; M. Schmidt, &quot;Tractable structured natural gradient descent using local parameterizations,&quot; &lt;i&gt;International Conference on Machine Learning (ICML)&lt;/i&gt; (2021).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;opper2009variational&quot;&gt;[2] M. Opper &amp;amp; C. Archambeau, &quot;The variational Gaussian approximation revisited,&quot; &lt;i&gt;Neural computation&lt;/i&gt; &lt;b&gt;21&lt;/b&gt;:786–792 (2009).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;glasmachers2010exponential&quot;&gt;[3] T. Glasmachers, T. Schaul, S. Yi, D. Wierstra, &amp;amp; J. Schmidhuber, &quot;Exponential natural evolution strategies,&quot; &lt;i&gt;Proceedings of the 12th annual conference on Genetic and evolutionary computation&lt;/i&gt; (2010), pp. 393–400.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2021structured&quot;&gt;[4] W. Lin, F. Nielsen, M. E. Khan, &amp;amp; M. Schmidt, &quot;Structured second-order methods via natural gradient descent,&quot; &lt;i&gt;arXiv preprint arXiv:2107.10884&lt;/i&gt; (2021).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2019stein&quot;&gt;[5] W. Lin, M. E. Khan, &amp;amp; M. Schmidt, &quot;Stein’s Lemma for the Reparameterization Trick with Exponential Family Mixtures,&quot; &lt;i&gt;arXiv preprint arXiv:1910.13398&lt;/i&gt; (2019).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;lin2020handling&quot;&gt;[6] W. Lin, M. Schmidt, &amp;amp; M. E. Khan, &quot;Handling the positive-definite constraint in the bayesian learning rule,&quot; &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (PMLR, 2020), pp. 6116–6126.&lt;/span&gt;&lt;/p&gt;&lt;/p&gt;</content><author><name>Wu Lin,&lt;br&gt;Frank Nielsen,&lt;br&gt;Emtiyaz Khan,&lt;br&gt;Mark Schmidt</name><email>informationgeometryML@gmail.com</email></author><category term="Natural Gradient Descent" /><category term="Information Geometry" /><category term="Matrix Lie Groups" /><category term="Exponential Family" /><summary type="html">More about this work [1]: (Youtube) talk, extended paper, short paper, poster</summary></entry></feed>