<!DOCTYPE html>
<!--
  Minimal Mistakes Jekyll Theme 4.12.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
--><html lang="en" class="no-js">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Part III: Invariance of Natural-Gradients - Information Geometry in Machine Learning</title>
<meta name="description" content="GoalThis blog post focuses on invariant properties of natural-gradients.We will discuss  transformation rules of natural-gradients,  the automatic computation of re-parametrized natural-gradients via the transformation rules,  non-invariance of the Euclidean counterpart.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Information Geometry in Machine Learning">
<meta property="og:title" content="Part III: Invariance of Natural-Gradients">
<meta property="og:url" content="/posts/2021/11/Geomopt03/">


  <meta property="og:description" content="GoalThis blog post focuses on invariant properties of natural-gradients.We will discuss  transformation rules of natural-gradients,  the automatic computation of re-parametrized natural-gradients via the transformation rules,  non-invariance of the Euclidean counterpart.">







  <meta property="article:published_time" content="2021-11-02T00:00:00-07:00">





  

  


<link rel="canonical" href="/posts/2021/11/Geomopt03/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Information Geometry in ML",
      "url": "https://github.com/pages/informationgeometryML/informationgeometryML.github.io",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Information Geometry in Machine Learning Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

<!-- end custom head snippets -->

  </head>

  <body class="layout--single mywide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Information Geometry in Machine Learning</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="/">Home</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/news/">News</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/year-archive/">Blog Posts</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  

  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Part III: Invariance of Natural-Gradients">
    <meta itemprop="description" content="GoalThis blog post focuses on invariant properties of natural-gradients.We will discuss  transformation rules of natural-gradients,  the automatic computation of re-parametrized natural-gradients via the transformation rules,  non-invariance of the Euclidean counterpart.">
    <meta itemprop="datePublished" content="November 02, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Part III: Invariance of Natural-Gradients
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  12 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="goal">Goal</h2>
<p>This blog post focuses on invariant properties of natural-gradients.
We will discuss</p>
<ul>
  <li>transformation rules of natural-gradients,</li>
  <li>the automatic computation of re-parametrized natural-gradients via the transformation rules,</li>
  <li>non-invariance of the Euclidean counterpart.</li>
</ul>

<p>The discussion here is informal and focuses more on intuitions, rather than rigor.</p>

<div class="notice--info">
  <details>
<summary>Click to see how to cite this blog post</summary>
<fieldset class="field-set">

      <div class="language-latex highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>@misc<span class="p">{</span>lin2021NGDblog03,
  title = <span class="p">{</span>Introduction to Natural-gradient Descent: Part III<span class="p">}</span>,
  author = <span class="p">{</span>Lin, Wu and Nielsen, Frank and Khan, Mohammad Emtiyaz and Schmidt, Mark<span class="p">}</span>,
  url = <span class="p">{</span>https://informationgeometryml.github.io/year-archive/<span class="p">}</span>, 
  howpublished = <span class="p">{</span><span class="k">\url</span><span class="p">{</span>https://informationgeometryml.github.io/posts/2021/11/Geomopt03/<span class="p">}}</span>,
  year = <span class="p">{</span>2021<span class="p">}</span>,
  note = <span class="p">{</span>Accessed: 2021-11-02<span class="p">}</span>
<span class="p">}</span>
</code></pre></div>      </div>
    </fieldset>
</details>
</div>

<h1 id="parameter-transformation-and-invariance">Parameter Transformation and Invariance</h1>
<hr>
<p>Recall that a Riemannian gradient is also known as a <strong>natural gradient</strong> since we use the Fisher-Rao metric <code class="language-plaintext highlighter-rouge">$\mathbf{F}$</code> as the Riemannian metric.</p>

<p>In <a href="/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional">Part II</a>, we have shown that a Riemannian gradient is a parametric representation of the tangent direction of a curve in a  manifold.
Since a curve and its tangent direction are geometric obejects, they should be invariant to the choice of parametrization.
In other words, geometric properties should be persevered in any valid coordinate system. This is a coordinate-free argument.</p>

<p>The argument could be abstract for beginners. To be more concrete, we consider the corresponding coordinate-dependent argument: geometric properties should remain unchanged if we perform a (valid) parameter transformation. This coordinate-dependent argument also gives us a rule to compute natrual-gradients under a parameter transformation.</p>

<p>The transformation rules are summarized in the following table</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Type of  gradients</th>
      <th style="text-align: center">transformation rules</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Euclidean gradient (A.K.A. <a href="https://en.wikipedia.org/wiki/One_form">differential 1-form</a>)</td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">$\eqref{6}$</code></td>
    </tr>
    <tr>
      <td style="text-align: left">Euclidean steepest descent direction (normalized Euclidean gradient)</td>
      <td style="text-align: center">Non-invariant</td>
    </tr>
    <tr>
      <td style="text-align: left">Riemannian gradient</td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">$\eqref{7}$</code></td>
    </tr>
    <tr>
      <td style="text-align: left">Riemannian steepest descent direction (normalized Riemannian gradient)</td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">$\eqref{10}$</code></td>
    </tr>
  </tbody>
</table>

<p>The following example illustrates these transformation rules shown in the Table.</p>
<div class="notice--info">
  <details>
<summary>Univariate Gaussian example: (click to expand)</summary>
<fieldset class="field-set">
      <blockquote>
        <p>Consider the following scalar function
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
h_\tau(\tau):= E_{q(w|\tau)} [ w^2 + \log q(w|\tau) ]
= \mu^2 + \frac{1}{s} + \frac{1}{2} \log(s)- \frac{1}{2}(1+\log(2\pi))
\end{aligned}
$$</code>
where  <code class="language-plaintext highlighter-rouge">$q(w|\tau)= \mathcal{N}(w|\mu,s^{-1})$</code> is a Gaussian family with mean <code class="language-plaintext highlighter-rouge">$\mu$</code>, variance <code class="language-plaintext highlighter-rouge">$s^{-1}$</code>, 
  intrinsic parametrization <code class="language-plaintext highlighter-rouge">$\tau=(\mu,s)$</code>, and parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\tau=\{(\mu,s)|\mu \in \mathcal{R},s&gt;0 \}$</code>.</p>

        <p>The FIM of Gaussian $q(w|\tau)$ under parametrization <code class="language-plaintext highlighter-rouge">$\tau$</code> is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{F}_\tau (\tau) := -E_{q(w|\tau)} [ \nabla_\tau^2 \log q(w|\tau) ] 
=
\begin{bmatrix}
s &amp; 0 \\
0 &amp; \frac{1}{2s^2}
\end{bmatrix}
\end{aligned}
$$</code>
We consider a member $\tau_0=(0.5,0.5)$ in the Gaussian family.
The Euclidean gradient is 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{g}_\tau (\tau_0) :=
\nabla_\tau h_\tau(\tau_0) =
\begin{bmatrix}
2 \mu \\
-\frac{1}{s^2} +\frac{1}{2s}
\end{bmatrix}_{\tau=\tau_0}
=\begin{bmatrix}
1 \\ -3
\end{bmatrix}
\end{aligned}
$$</code>
The natural/Riemannian gradient is 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\hat{\mathbf{g}}_\tau (\tau_0) :=
\mathbf{F}_\tau^{-1} (\tau_0) \nabla_\tau h_\tau(\tau_0) =
\begin{bmatrix}
2 \mu s^{-1}  \\
( -\frac{1}{s^2} +\frac{1}{2s} ) (2s^2)
\end{bmatrix}_{\tau=\tau_0}
=\begin{bmatrix}
2 \\ -\frac{3}{2}
\end{bmatrix}
\end{aligned}
$$</code></p>

        <p>Now, consider the following re-parametrization of the function</p>

        <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
h_\lambda(\lambda)= E_{q(w|\lambda)} [ w^2 + \log q(w|\lambda)]
= \mu^2 + v - \frac{1}{2} \log(v) - \frac{1}{2}(1+\log(2\pi))
\end{aligned}
$$</code>
where <code class="language-plaintext highlighter-rouge">$q(w|\lambda)= \mathcal{N}(w|\mu,v)$</code> with mean <code class="language-plaintext highlighter-rouge">$\mu$</code> and variance <code class="language-plaintext highlighter-rouge">$v=s^{-1}$</code>,
  intrinsic parametrization <code class="language-plaintext highlighter-rouge">$\lambda=(\mu,v)$</code>, and parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\lambda=\{(\mu,v)|\mu \in \mathcal{R},v&gt;0 \}$</code>.</p>

        <p>The FIM of Gaussian $q(w|\lambda)$ under parametrization <code class="language-plaintext highlighter-rouge">$\lambda$</code> is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{F}_\lambda (\lambda):= -E_{q(w|\lambda)} [ \nabla_\lambda^2 \log q(w|\lambda) ] 
=
\begin{bmatrix}
\frac{1}{v} &amp; 0 \\
0 &amp; \frac{1}{2v^2}
\end{bmatrix}
\end{aligned}
$$</code></p>

        <p>The Jacobian matrix is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{J} :=\frac{\partial \lambda(\tau)}{\partial \tau} = 
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; -\frac{1}{s^2}
\end{bmatrix}
\end{aligned}
$$</code> where $\lambda(\tau)=(\mu,v)=(\mu,\frac{1}{s})$ and $\tau=(\mu,s)$.</p>

        <p>We can verify that <code class="language-plaintext highlighter-rouge">$\eqref{8}$</code> holds for the FIM.</p>

        <p>Consider the same member $\lambda_0=(0.5,2)$ in the Gaussian family.
The Euclidean gradient is 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{g}_\lambda (\lambda_0) :=
\nabla_\lambda h_\lambda(\lambda_0) =
\begin{bmatrix}
2 \mu \\
1 - \frac{1}{2v}
\end{bmatrix}_{\lambda=\lambda_0}
=\begin{bmatrix}
1 \\ \frac{3}{4}
\end{bmatrix} 
\end{aligned}
$$</code>
We can verify that <code class="language-plaintext highlighter-rouge">$\eqref{6}$</code> holds for the Euclidean gradient.</p>

        <p>The natural/Riemannian gradient is 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\hat{\mathbf{g}}_\lambda (\lambda_0) :=
\mathbf{F}_\lambda^{-1} (\lambda_0) \nabla_\lambda h_\lambda(\lambda_0) =
\begin{bmatrix}
2 \mu  v \\
( 1 - \frac{1}{2v} ) (2v^2)
\end{bmatrix}_{\lambda=\lambda_0}
=\begin{bmatrix}
2 \\ 6
\end{bmatrix}
\end{aligned}
$$</code></p>

        <p>We can verify that <code class="language-plaintext highlighter-rouge">$\eqref{7}$</code> holds for the Riemannian gradient</p>
      </blockquote>
    </fieldset>
</details>
</div>

<div class="notice--success">
  <p>We will show that two key <strong>geometric properties</strong> remains the same under any <strong>intrinsic</strong> parameter transformation.</p>
  <ol>
    <li>Directional derivative</li>
    <li>Length of a Riemannian vector/gradient induced by the Fisher-Rao metric</li>
  </ol>
</div>

<p>Thanks to these properties, we will show that the optimal solution of the <a href="/Geomopt02/#riemannian-steepest-direction">Riemannian steepest direction</a> considered in Part II is invariant under an intrinsic parameter transformation. This is in contrast with the Euclidean steepest direction which is not invaraint under an intrinsic parameter transformation.</p>

<p>In <a href="/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional">Part II</a>, we consider a point $\mathbf{x}_0$ in a manifold $\mathcal{M}$, a (1-dimensional) curve $\gamma(t)$, and a smooth scalar function $h: \mathcal{M} \to \mathcal{R}$.
Given an intrinsic parametrization $\tau$ containing the point, we consider the following parametric representations.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">geometric objects</th>
      <th style="text-align: center">parametric representations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">point <code class="language-plaintext highlighter-rouge">$\mathbf{x}_0$</code>
</td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">$\tau_0$</code></td>
    </tr>
    <tr>
      <td style="text-align: left">curve  <code class="language-plaintext highlighter-rouge">$\gamma(t)$</code>
</td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">$\gamma_\tau(t) $</code></td>
    </tr>
    <tr>
      <td style="text-align: left">function  <code class="language-plaintext highlighter-rouge">$h(x_0)$</code>
</td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">$h_\tau(\tau_0) $</code></td>
    </tr>
  </tbody>
</table>

<h2 id="transformation-rules-for-riemannian-gradients-and-euclidean-gradients">Transformation Rules for Riemannian Gradients and Euclidean Gradients</h2>

<p>Intuitively, the following identity should hold for any two intrinsic parametrizations <code class="language-plaintext highlighter-rouge">$\tau$</code> and <code class="language-plaintext highlighter-rouge">$\lambda$</code>
that represent a common sub-set of points in a manifold.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
h(\gamma(t)) = \underbrace{[h \circ \Psi_\tau^{-1}]}_{h_\tau}( \underbrace{ \Psi_\tau( \gamma(t) ) }_{:= \gamma_\tau(t)  }  ) = \underbrace{[h \circ \Psi_\lambda^{-1}]}_{h_\lambda}( \underbrace{ \Psi_\lambda( \gamma(t) ) }_{:= \gamma_\lambda(t)  }  )
\end{aligned}
$$</code> where we consider $t$ to be fixed and <code class="language-plaintext highlighter-rouge">$\Psi_\tau$</code> is known as the coordinate/parametrization map for parametrization <code class="language-plaintext highlighter-rouge">$\tau$</code> (i.e., <code class="language-plaintext highlighter-rouge">$\Psi_\tau: \mathcal{M} \to \mathcal{R}^{\mathrm{dim}(\mathcal{M})}$</code>).</p>

<p>Technically speaking,  domain <code class="language-plaintext highlighter-rouge">$\mathbf{I}_\tau$</code> of curve <code class="language-plaintext highlighter-rouge">$\gamma_\tau(t)$</code> and  domain <code class="language-plaintext highlighter-rouge">$\mathbf{I}_\lambda$</code> of curve <code class="language-plaintext highlighter-rouge">$\gamma_\lambda(t)$</code> may be different.
For simplicity, we assume both domains are open intervals containing 0.
In other words, <code class="language-plaintext highlighter-rouge">$\gamma_\tau(0)=\tau_0$</code> and <code class="language-plaintext highlighter-rouge">$\gamma_\lambda(0)=\lambda_0$</code>  are parametric representations of the same point <code class="language-plaintext highlighter-rouge">$\mathbf{x}_0$</code>.</p>

<p>From the above expression, we can see that directional derivatives should be the same at $t=0$
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} = \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} 
\end{aligned}\tag{1}\label{1}
$$</code></p>

<p>In <a href="/Geomopt02/#parameterization-dependent-representation">Part II</a>, we have shown that 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &amp;=  [\nabla h_\tau(\mathbf{\tau}_0)  ]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}   \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} &amp; = [\nabla h_\lambda(\mathbf{\lambda}_0) ]^T  \frac{d {\gamma}_\lambda(t) }{d t} \Big|_{t=0}  
\end{aligned}
$$</code> where $\nabla$ is the standard (coordinate) derivative.</p>

<p>Recall that in <a href="/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional">Part II</a>,  we have shown that <code class="language-plaintext highlighter-rouge">$\frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$</code> is a  parametric representation of a tangent vector, which is a Riemannian gradient.
Notice that <code class="language-plaintext highlighter-rouge">$\nabla h_\lambda(\mathbf{\lambda}_0)$</code> is a Euclidean gradient<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<p>We will use the following notations to simplify expressions.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Notations</th>
      <th style="text-align: center">Meanings</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Euclidean gradient <code class="language-plaintext highlighter-rouge">$(g_\tau)_i$</code>
</td>
      <td style="text-align: center">$i$-th entry  under parametrization $\tau$</td>
    </tr>
    <tr>
      <td style="text-align: left">Riemannian gradient <code class="language-plaintext highlighter-rouge">$(\hat{g}_\tau)^j$</code>
</td>
      <td style="text-align: center">$j$-th entry under parametrization $\tau$</td>
    </tr>
    <tr>
      <td style="text-align: left">Parameter <code class="language-plaintext highlighter-rouge">$\tau^j$</code>
</td>
      <td style="text-align: center">$j$-th parameter under parametrization   $\tau$</td>
    </tr>
  </tbody>
</table>

<p>Using these notations, the derivational derivatives then can be re-expressed as</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &amp;= \sum_{i}  (g_\tau)_i  (\hat{g}_\tau)^i  = \mathbf{g}_\tau^T \hat{\mathbf{g}}_\tau \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} &amp; =\sum_{i}  (g_\lambda)_i (\hat{g}_\lambda)^i =  {\mathbf{g}}_\lambda^T \hat{\mathbf{g}}_\lambda
\end{aligned}\tag{2}\label{2}
$$</code>
where  <code class="language-plaintext highlighter-rouge">$\mathbf{g}_\lambda$</code> and <code class="language-plaintext highlighter-rouge">$\mathbf{g}_\tau$</code> are Euclidean gradients (e.g.,  <code class="language-plaintext highlighter-rouge">$\mathbf{g}_\tau=\nabla h_\tau(\tau_0) $</code>)  while  <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_\lambda$</code> and <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_\tau$</code>  are  Riemannian gradients (e.g., <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_\tau=\frac{d \gamma_\tau(0) }{d t}$</code>) .</p>

<p>By <code class="language-plaintext highlighter-rouge">$\eqref{1}$</code> and <code class="language-plaintext highlighter-rouge">$\eqref{2}$</code>, we have the following identity obtained from the <strong>geometric property</strong> of directional derivatives.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
 \mathbf{g}_\tau^T \hat{\mathbf{g}}_\tau  =  \mathbf{g}_\lambda^T \hat{\mathbf{g}}_\lambda 
\end{aligned}\tag{3}\label{3}
$$</code></p>

<p>Now, we discuss the parameter transformation between $\tau$ and $\lambda$, where we can express $\lambda$ in terms of $\tau$ denoted by $\lambda(\tau)$.</p>

<p>By the (standard) chain rule for a Euclidean gradient<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, we has
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
(g_\tau)_i = \sum_{k}  {\color{red} (g_\lambda)_k} \frac{ {\color{red} \partial \lambda^k(\tau) }}{ \partial \tau^i } 
\end{aligned}
\tag{4}\label{4}
$$</code></p>

<p>Let <code class="language-plaintext highlighter-rouge">$J_{ki}:=\frac{\partial \lambda^k(\tau) }{ \partial \tau^i }$</code> denotes the $(k,i)$ entry of the Jacobian matrix. We illustrate our matrix notation in a 2D case as below.</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\begin{matrix}
&amp; \\
\mathbf{J} = 
    \left ( \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    i=1 &amp; i=2 \\ \hline
    J_{11} &amp; J_{12}  \\
   J_{21} &amp; J_{22} \\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
&amp; \\
\left . \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right )
    \begin{matrix}
    k=1 \\ k=2 
    \end{matrix}
\end{matrix}
\end{aligned}\tag{5}\label{5}
$$</code></p>

<p>Eq. <code class="language-plaintext highlighter-rouge">$\eqref{4}$</code> gives us the transformation rule for Eulcidean gradients (denoted by a row vector)  as below in a vector form.</p>

<div class="notice--success">
  <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{g}_\tau^T =  \mathbf{g}_\lambda^T \mathbf{J}
\end{aligned}\tag{6}\label{6},
$$</code></p>

  <p>Note:
<span style="color:blue"><strong>row</strong></span> vector <code class="language-plaintext highlighter-rouge">${\mathbf{g}}_\tau^T$</code> can be computed via a vector-Jacobian product in any standard Auto-Diff toolbox given that <code class="language-plaintext highlighter-rouge">${\mathbf{g}}_\lambda$</code> is pre-computed.</p>
</div>

<p>By Eq <code class="language-plaintext highlighter-rouge">$\eqref{3}$</code>, we obtain the transformation rule for Riemannian gradients  (denoted by a column vector) as below,  where <code class="language-plaintext highlighter-rouge">$\mathbf{Q}:=\mathbf{J}^{-1}$</code> is also a Jacobian matrix and <code class="language-plaintext highlighter-rouge">$Q_{ki}=\frac{\partial \tau^k(\lambda)}{\partial \lambda^i}$</code> is the <code class="language-plaintext highlighter-rouge">$(k,i)$</code> entry of matrix <code class="language-plaintext highlighter-rouge">$\mathbf{Q}$</code>.</p>

<div class="notice--success">
  <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\hat{\mathbf{g}}_\tau= \mathbf{J}^{-1}  \hat{\mathbf{g}}_\lambda 
\end{aligned}\tag{7}\label{7}
$$</code></p>

  <p>Note:
<span style="color:blue"><strong>column</strong></span> vector <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_\tau$</code> can be computed via a (inverse) Jacobian-vector product used in forward-mode differentiation <a class="citation" href="#lin2021tractable">[1]</a> <a class="citation" href="#salimbeni2018natural">[2]</a> given that <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_\lambda$</code> is pre-computed and we can explicitly express <code class="language-plaintext highlighter-rouge">$\tau$</code> in terms of <code class="language-plaintext highlighter-rouge">$\lambda$</code>. As shown in <a class="citation" href="#salimbeni2018natural">[2]</a>, this Jacobian-vector product can be computed by using two 
 vector-Jacobian products in any standard Auto-Diff toolbox.</p>
</div>

<p>The elementwise expression of the transformation rule for Riemannian gradients is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
(g_\tau)^k
 = \sum_{i}  \frac{ \partial \tau^k(\lambda)  }{ {\color{red} \partial  \lambda^i} }  {\color{red} (g_\lambda)^i}
\end{aligned},
$$</code></p>

<p>Note that these transformation rules are valid when the Jacobian matrix <code class="language-plaintext highlighter-rouge">$\mathbf{J}$</code> is square and non-singular.
As we discussed in Part I about <a href="/posts/2021/09/Geomopt01/#intrinsic-parameterizations">intrinsic parameterizations</a>, the transformation map between $\tau$ and $\lambda$ must be bi-jective, which implies the Jacoabian matrix is square. 
Moreover, the map and its inverse map should be smooth, which implies that the Jacobian matrix is well-defined and non-singular.</p>

<h2 id="transformation-rule-for-the-fisher-information-matrix">Transformation Rule for the Fisher Information Matrix</h2>

<p>Now, we discuss a transformation rule for the Fisher information matrix (FIM) as defined at <a href="/posts/2021/09/Geomopt01/#fisher-rao-metric">Part I</a>.</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
 F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log p(w|\tau) \Big) ]
\end{aligned}
$$</code>
Since $ \log p(w|\tau )$ can be considered as a scalar function $h$ defined on the manifold for any valid $w$, we have 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
 \log p(w|\tau_0  ) = h_\tau(\tau_0) = h_\lambda(\lambda_0) =  \log p(w|\lambda_0  )
\end{aligned}
$$</code></p>

<p>Thus, the FIM can be computed as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
 F_{ij}(\tau_0) &amp;= E_{p(w|\tau_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log p(w|\tau_0) \Big) ] \\
&amp;=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log p(w|\tau_0) \Big) ]\\
\end{aligned}
$$</code></p>

<p>Recall that by the standard chain rule, we have 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\partial_{\tau_i} \log p(w|\tau_0 ) = \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \Big( \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)
\end{aligned}
$$</code></p>

<p>Moreover, the Jacobian matrix does not depent on $w$. Therefore, we have
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
 F_{ij}(\tau_0) 
&amp;=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log p (w|\tau_0) \Big) ]\\
&amp;=  E_{p(w|\lambda_0) }  [ \Big( \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \sum_l \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&amp;= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  E_{p(w|\lambda_0) }  [ \Big(  \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&amp;= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } F_{kl}(\lambda_0)
\end{aligned}
$$</code></p>

<p>We can re-express the above expression in a matrix form as below. This is the transformation rule for the FIM.</p>

<div class="notice--success">
  <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{F}_{\tau} (\tau_0) = \underbrace{\mathbf{J}^T}_{  \frac{ \partial \lambda^i(\tau_0) }{ \partial \tau^k }  } \mathbf{F}_{\lambda} (\lambda_0) \underbrace{\mathbf{J}}_{  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  } 
\end{aligned}\tag{8}\label{8}
$$</code></p>
</div>

<p>By using this transformation rule, we can show that another <strong>geometric property</strong>: the length of a Riemannian vector is preserved.</p>

<p>We can see that the length of a Riemannian vector is also invariant.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\| \hat{\mathbf{g}}_\tau \|^2_{F_{\tau_0}} &amp;= [\hat{\mathbf{g}}_\tau]^T \mathbf{F}_{\tau} (\tau_0) \hat{\mathbf{g}}_\tau \\
&amp;= [\mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda]^T \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda \\
&amp;= [\hat{\mathbf{g}}_\lambda]^T [ \mathbf{J}^{-T}  \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} ] \hat{\mathbf{g}}_\lambda \\
&amp;= [\hat{\mathbf{g}}_\lambda]^T  \mathbf{F}_{\lambda} (\lambda_0)  \hat{\mathbf{g}}_\lambda = \| \hat{\mathbf{g}}_\lambda \|^2_{F_{\lambda_0}}
\end{aligned}\tag{9}\label{9}
$$</code></p>

<h1 id="riemannian-steepest-direction-is-invariant">Riemannian Steepest Direction is Invariant</h1>
<hr>
<p>Now, we can show that the optimal solution of <a href="/Geomopt02/#riemannian-steepest-direction">Riemannian steepest direction</a> considered in Part II  under parametrization $\tau$ and $\lambda$ are equivalent since both the length and the directional derivative remain the same.</p>

<p>Denote Euclidean gradients as <code class="language-plaintext highlighter-rouge">$\mathbf{g}_\lambda:= \nabla  f_\lambda(\mathbf{\lambda}_0) $</code> and  <code class="language-plaintext highlighter-rouge">$\mathbf{g}_\tau:= \nabla  f_\tau(\mathbf{\tau}_0)  = \nabla  f_\lambda(\mathbf{\lambda}(\tau_0)) $</code>, which follows the parameter transformation rule in  <code class="language-plaintext highlighter-rouge">$\eqref{6}$</code>.</p>

<p>Now, consider natural/Riemannian gradients as <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_\lambda:= \mathbf{F}_{\lambda}^{-1}(\mathbf{\lambda}_0)  \mathbf{g}_\lambda $</code> and  <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_\tau:= \mathbf{F}_{\tau}^{-1}(\mathbf{\tau}_0) \mathbf{g}_\tau $</code>. These Riemannian gradients follow the parameter transformation rule in <code class="language-plaintext highlighter-rouge">$\eqref{7}$</code> as shown below.</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\hat{\mathbf{g}}_\tau &amp;= \mathbf{F}_{\tau}^{-1}(\mathbf{\tau}_0) \mathbf{g}_\tau \\
&amp;= \big( \mathbf{J}^T  \mathbf{F}_{\lambda} (\mathbf{\lambda}_0) \mathbf{J} \big)^{-1} ( \mathbf{g}^T_\tau )^T &amp; ( \text{by } \eqref{8} )\\
&amp;=  \mathbf{J}^{-1} \mathbf{F}_{\lambda}^{-1} (\mathbf{\lambda}_0) \mathbf{J}^{-T}   ( \mathbf{g}^T_\lambda \mathbf{J} )^T  &amp; ( \text{by } \eqref{6} ) \\
&amp;=  \mathbf{J}^{-1} \mathbf{F}_{\lambda}^{-1} (\mathbf{\lambda}_0) \mathbf{J}^{-T}  (  \mathbf{J}^T  \mathbf{g}_\lambda  ) \\
&amp;=  \mathbf{J}^{-1} \mathbf{F}_{\lambda}^{-1} (\mathbf{\lambda}_0)   \mathbf{g}_\lambda   \\
&amp;=  \mathbf{J}^{-1}   \hat{\mathbf{g}}_\lambda
\end{aligned}
$$</code></p>

<p>Recall that the optimal solution of the Riemannian steepest direction is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{v}_{\lambda}^{(opt)}= -\frac{ \mathbf{F_\lambda}^{-1}(\mathbf{\lambda}_0) \nabla_\lambda f(\mathbf{\lambda}_0) }{\| \mathbf{F_\lambda}^{-1}(\mathbf{\lambda}_0)\nabla_\lambda f(\mathbf{\lambda}_0) \|_{F_{\lambda_0}}} = -\frac{\hat{\mathbf{g}}_\lambda}{\|\hat{\mathbf{g}}_\lambda\|_{ F_{\lambda_0} } } \\
\mathbf{v}_{\tau}^{(opt)}= -\frac{ \mathbf{F_\tau}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0) }{\| \mathbf{F_\tau}^{-1}(\mathbf{\tau}_0)\nabla_\tau f(\mathbf{\tau}_0) \|_{F_{\tau_0}}} = -\frac{\hat{\mathbf{g}}_\tau}{\|\hat{\mathbf{g}}_\tau\|_{ F_{\tau_0} } } 
\end{aligned}
$$</code></p>

<p>We can easily verify the following identities since
<code class="language-plaintext highlighter-rouge">$ \|\hat{\mathbf{g}}_\lambda\|_{ F_{\lambda_0} } =  \|\hat{\mathbf{g}}_\tau\|_{ F_{\tau_0} }  $</code> as shown in <code class="language-plaintext highlighter-rouge">$\eqref{9}$</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{g}_\tau^T   \mathbf{v}^{(opt)}_{\tau}   &amp; =   \mathbf{g}_\lambda^T  \mathbf{v}^{(opt)}_{\lambda} &amp; \,\,\, \text{(invariance of a directional derivative)}  \\
\|  \mathbf{v}^{(opt)}_{\tau} \|^2_{\color{red}{F_{\tau_0}}} &amp; = \|  \mathbf{v}^{(opt)}_{\lambda} \|^2_{\color{red}{F_{\lambda_0}} } &amp; \,\,\, \text{(invariance of the length of the Riemannian steepest direction)}  \\
\mathbf{v}^{(opt)}_{\tau} &amp; = \mathbf{J}^{-1}  \mathbf{v}^{(opt)}_{\lambda} &amp; \,\,\, \text{(transformation rule for the Riemannian steepest direction)}  
\end{aligned}\tag{10}\label{10}.
$$</code> 
In other words, the Riemannian steepest direction (which is indeed  a normalized Riemannian gradient) is also transformed according to the transformation rule for Riemannian gradients in <code class="language-plaintext highlighter-rouge">$\eqref{7}$</code>.</p>

<p>As we will discuss in <a href="/posts/2021/11/Geomopt04/#natural-gradient-descent-is-linearly-invariant">Part IV</a>, this invariance property implies that natural-gradient descent is linearly invariant.</p>

<h1 id="euclidean-steepest-direction-is-not-invariant">Euclidean Steepest Direction is NOT Invariant</h1>
<hr>
<p>Recall that we have shown that a unit Euclidean gradient is the optimal solution of  <a href="/Geomopt02/#euclidean-steepest-direction">Euclidean steepest direction</a> in Part II.</p>

<p>We can show that the (standard) length of a Euclidean gradient is NOT invariant under a parameter transformation due to the Jacobian matrix (i.e., <code class="language-plaintext highlighter-rouge">$\| \mathbf{g}_\tau \| \neq \| \mathbf{g}_\lambda \|$</code>). This is a reason why we use the <a href="/Geomopt02/#weighted-norm-induced-by-the-fisher-rao-metric">weighted inner product</a> to define the length of a gradient vector.</p>

<p>Note that we denote the Euclidean gradients as <code class="language-plaintext highlighter-rouge">$\mathbf{g}_\lambda:= \nabla  f_\lambda(\mathbf{\lambda}_0) $</code> and  <code class="language-plaintext highlighter-rouge">$\mathbf{g}_\tau:= \nabla  f_\tau(\mathbf{\tau}_0)  = \nabla  f_\lambda(\mathbf{\lambda}(\tau_0)) $</code></p>

<p>Recall that the optimal solution of the <a href="/Geomopt02/#euclidean-steepest-direction">Euclidean steepest direction</a> is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{u}_{\lambda}^{(opt)}= -\frac{\nabla_\lambda f_\lambda(\mathbf{\lambda}_0) }{\|\nabla_\lambda f_\lambda(\mathbf{\lambda}_0)  \|} = -\frac{\mathbf{g}_\lambda}{\|\mathbf{g}_\lambda\|} \\
\mathbf{u}_{\tau}^{(opt)}= -\frac{\nabla_\tau f_\tau(\mathbf{\tau}_0) }{\|\nabla_\tau f_{\tau}(\mathbf{\tau}_0) \|} = -\frac{\mathbf{g}_\tau}{\|\mathbf{g}_\tau\|} 
\end{aligned}
$$</code></p>

<p>Unfortunately, the Euclidean steepest direction  does NOT obey the parameter transformation rule for Euclidean gradients in  <code class="language-plaintext highlighter-rouge">$\eqref{6}$</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
(\mathbf{u}_{\tau}^{(opt)})^T \neq (\mathbf{u}_{\lambda}^{(opt)})^T \mathbf{J} 
\end{aligned}
$$</code></p>

<p>Moreover, the optimal value of the Euclidean steepest direction is NOT invariant under a parameter transformation as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{g}_\lambda^T \mathbf{u}^{(opt)}_{\lambda}  = - \|\mathbf{g}_\lambda\| \neq - \|\mathbf{g}_\tau\| = \mathbf{g}_\tau^T \mathbf{u}^{(opt)}_{\tau} 
\end{aligned}
$$</code></p>

<p>In summary, the Euclidean steepest direction (which is a normalized Euclidean gradient) is NOT transformed according to the transformation rule for a Euclidean gradient.
Moreover, Euclidean gradient descent is not invariant under a parameter transformation. We will cover more about this in <a href="/posts/2021/11/Geomopt04/">Part IV</a>.</p>

<hr>
<h1 id="references">References</h1>
<p class="bibliography"></p>
<p><span id="lin2021tractable">[1] W. Lin, F. Nielsen, M. E. Khan, &amp; M. Schmidt, "Tractable structured natural gradient descent using local parameterizations," <i>International Conference on Machine Learning (ICML)</i> (2021).</span></p>
<p><span id="salimbeni2018natural">[2] H. Salimbeni, S. Eleftheriadis, &amp; J. Hensman, "Natural gradients in practice: Non-conjugate variational inference in Gaussian process models," <i>International Conference on Artificial Intelligence and Statistics</i> (PMLR, 2018), pp. 689–697.</span></p>

<h2 id="footnotes">Footnotes:</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In differential geometry, a Euclidean gradient is also known as a coordinate representation of a cotangent vector. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>We assume readers are familar with the transformation rule for Euclidean gradients.  Let  <code class="language-plaintext highlighter-rouge">$\tau(t)=\gamma_\tau(t)$</code> and <code class="language-plaintext highlighter-rouge">$\lambda(t)=\gamma_\lambda(t)$</code>.  In differential geometry, this transformation rule can also be shown by using the following identity: <code class="language-plaintext highlighter-rouge">$\mathbf{J}(\tau)= \frac{ \partial [\Psi_\lambda \circ \Psi_\tau^{-1}](\tau) }{\partial \tau } =  \frac{ \partial  \lambda  }{\partial \tau } $</code>, where we drop the index $t$ and recall that <code class="language-plaintext highlighter-rouge">$\gamma_\tau(t)=\Psi_\tau \circ \gamma(t)$</code> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#information-geometry" class="page__taxonomy-item" rel="tag">Information Geometry</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#natural-gradient-descent" class="page__taxonomy-item" rel="tag">Natural Gradient Descent</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#riemannian-manifold" class="page__taxonomy-item" rel="tag">Riemannian Manifold</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-11-02T00:00:00-07:00">November 02, 2021</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title" data-translate="share_on_label">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Part+III%3A+Invariance+of+Natural-Gradients%20informationgeometryML.github.io%2Fposts%2F2021%2F11%2FGeomopt03%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=informationgeometryML.github.io%2Fposts%2F2021%2F11%2FGeomopt03%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=informationgeometryML.github.io%2Fposts%2F2021%2F11%2FGeomopt03%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>



     

  <script src="https://utteranc.es/client.js" repo="informationgeometryML/informationgeometryML.github.io" issue-term="url" label="blog-comments" theme="github-light" crossorigin="anonymous" async>
  </script>





</section>


      
  <nav class="pagination">
    
      <a href="/Geomopt02/" class="pagination--pager" title="Geomopt02
">Previous</a>
    
    
      <a href="/posts/2021/11/Geomopt04/" class="pagination--pager" title="Part IV: Natural Gradient Descent and its Extension—Riemannian Gradient Descent
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/12/Geomopt06/" rel="permalink">Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (Part VI is incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/12/Geomopt05/" rel="permalink">Part V: Efficient Natural-gradient Methods for Exponential Family
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  19 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/11/Geomopt04/" rel="permalink">Part IV: Natural Gradient Descent and its Extension—Riemannian Gradient Descent
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  19 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/Geomopt02/" rel="permalink">Geomopt02
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  16 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">–
title: ‘Part II: Derivation of Natural-gradients’
date: 2021-10-04
permalink: /posts/2021/10/Geomopt02/
tags:

  Natural Gradient Descent
  Information Geo...</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow</strong></li>
    
    
    
    
      <li><a href="https://github.com/informationgeometryML"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">© 2022 Information Geometry in ML. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.2.0/js/all.js"></script>












  
    <script src="/assets/js/custom.js"></script>
  
    <script src="/assets/js/math-code.js"></script>
  



  </body>
</html>
