<!DOCTYPE html>
<!--
  Minimal Mistakes Jekyll Theme 4.12.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
--><html lang="en" class="no-js">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Part IV: Natural and Riemannian  Gradient Descent - Information Geometry in Machine Learning</title>
<meta name="description" content="Warning: working in Progress (incomplete)">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Information Geometry in Machine Learning">
<meta property="og:title" content="Part IV: Natural and Riemannian  Gradient Descent">
<meta property="og:url" content="/posts/2021/11/Geomopt04/">


  <meta property="og:description" content="Warning: working in Progress (incomplete)">







  <meta property="article:published_time" content="2021-11-15T00:00:00-08:00">





  

  


<link rel="canonical" href="/posts/2021/11/Geomopt04/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Information Geometry in ML",
      "url": "https://github.com/pages/informationgeometryML/informationgeometryML.github.io",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Information Geometry in Machine Learning Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML" async></script>


<!-- end custom head snippets -->

  </head>

  <body class="layout--single mywide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Information Geometry in Machine Learning</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="/">Home</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/news/">News</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/year-archive/">Blog Posts</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  

  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Part IV: Natural and Riemannian  Gradient Descent">
    <meta itemprop="description" content="Warning: working in Progress (incomplete)">
    <meta itemprop="datePublished" content="November 15, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Part IV: Natural and Riemannian  Gradient Descent
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  18 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>Warning: working in Progress (incomplete)</p>

<h2 id="goal">Goal</h2>
<p>This blog post should help readers to understand natural-gradient descent and Riemannian gradient descent.
We also discuss some invariance property of natural-gradient descent, Riemannian gradient descent, and Newton’s method.</p>

<p>We will give an informal introduction with a focus on high level of ideas.</p>

<h1 id="two-kinds-of-spaces">Two kinds of Spaces</h1>
<hr>
<p>We will discuss (Riemannian) gradient spaces and parameter spaces for gradient-based updates.</p>

<p>As we disucssed in <a href="/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional">Part II</a>, the parameter space $\Omega_\tau$ and the tangent space denoted by <code class="language-plaintext highlighter-rouge">$T\mathcal{M}_{\tau_0}$</code> at point $\tau_0$ are two distinct spaces. 
Given  <a href="/posts/2021/09/Geomopt01/#intrinsic-parameterizations">intrinsic parametrization</a>  <code class="language-plaintext highlighter-rouge">$\tau$</code>,  the tangent space is a (complete) vector space and <code class="language-plaintext highlighter-rouge">$T\mathcal{M}_{\tau_0}=\mathcal{R}^K$</code> while the parameter space $\Omega_\tau$ is like a local vector space in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>, where $K$ is the dimension of the manifold. In other words, <code class="language-plaintext highlighter-rouge">$\Omega_\tau$</code> is often an open (proper) subset of <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>.</p>

<p>In manifold cases, we have to explicitly distinguish the difference between the representation of a point (parameter) and a vector (Riemannian gradient).  These two spaces are different in many aspects such as
the domain and the distance.
Mathematically
speaking,  a (Riemannian) gradient space is much simpler and nicer than a  parameter space.</p>

<p><img src="/img/sphere.png" width="500"></p>

<h1 id="natural-gradient-descent-in-an-intrinsic-parameter-space">Natural-gradient Descent in an Intrinsic Parameter Space</h1>
<hr>

<p>Using intrinsic parametrization $\tau$, we can perform a natural-gradient update known as natural-gradient descent (NGD) <a class="citation" href="#amari1998natural">[1]</a> when we use the Fisher-Rao metric.</p>

<div class="notice--success">
  <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{NGD:} &amp;\,\,\,\,\, 
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{1}\label{1}
$$</code></p>
</div>
<p>where <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_{\tau_k}$</code> is a natural gradient evaluated at point <code class="language-plaintext highlighter-rouge">$\tau_{k}$</code> and $\alpha&gt;0$ is a step-size.</p>

<p>This update in Eq. <code class="language-plaintext highlighter-rouge">$\eqref{1}$</code> is inspired by the standard vector update in the <strong>gradient space</strong>.
By choosing an intrinsic parametrization, this update is also valid in the <strong>parameter space</strong> as long as the step-size is small
enough.</p>

<p>The update in the parameter space is valid since the parameter space $\Omega_\tau$ has a local vector-space structure thanks to the use of an intrinsic parametrization.
However, when $\Omega_\tau$ is a proper subset of $T\mathcal{M}_{\tau_k}$ (i.e., <code class="language-plaintext highlighter-rouge">$\Omega_\tau \neq T\mathcal{M}_{\tau_k} $</code>), the update in Eq. <code class="language-plaintext highlighter-rouge">$\eqref{1}$</code> is valid only when the step-size $\alpha$ is small enough so that  <code class="language-plaintext highlighter-rouge">$\tau_{k+1} \in \Omega_\tau$</code>.</p>

<div class="notice--danger">
  <p><strong>Warning</strong>:</p>

  <p>Using a small step-size could be an issue since it could greatly slow down the progression of natural-gradient
descent in practice.</p>
</div>

<div class="notice--info">
  <details>
<summary>Example of NGD in a constrained space: (click to expand)</summary>
<fieldset class="field-set">
      <blockquote>

        <p>Consider a univariate Gaussian family <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(w |\mu,v) \Big| \mu \in \mathcal{R}, v&gt;0 \}$</code> with mean <code class="language-plaintext highlighter-rouge">$\mu$</code>, variance <code class="language-plaintext highlighter-rouge">$v$</code>, and intrinsic parameter <code class="language-plaintext highlighter-rouge">$\tau = (\mu,v) $</code>. <br></p>

        <p>We have to properly select the step-size $\alpha$ for natural-gradient descent in  <code class="language-plaintext highlighter-rouge">$\eqref{1}$</code> due to the positivity constraint in $\sigma$.</p>

        <p>In multivariate Gaussian cases, we have to handle a positive-definite constraint.</p>
      </blockquote>
    </fieldset>
</details>
</div>

<h1 id="natural-gradient-descent-is-linearly-invariant">Natural-gradient Descent is Linearly Invariant</h1>
<hr>

<p>Recall that in <a href="/posts/2021/11/Geomopt03/#parameter-transform-and-invariance">Part III</a>, we have shown that natural gradients are invariant under any intrinsic parameter transformation.
The parameter transformation can be non-linear.</p>

<p>It is natural to expect that natural-gradient descent has a similar property. Unfortunately, natural-gradient descent is only invariant under an intrinsic <strong>linear</strong> transformation. Note that Newton’s method is also linearly invariant while Euclidean gradient descent is not.</p>

<p>Let’s consider the following (scalar) optimization problem on a smooth manifold $\mathcal{M}$ with the Fisher-Rao metric <code class="language-plaintext highlighter-rouge">$F$</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{x \in \mathcal{M}} h(x)
\end{aligned}\tag{2}\label{2}
$$</code></p>

<p>Note that $\mathcal{M}$ in general does not have a vector-space structure. 
We has to <strong>artificially</strong> choose an intrinsic parameterization $\tau$ so that the parameter space $\Omega_\tau$ at least has a local  vector-space structure.
The problem in <code class="language-plaintext highlighter-rouge">$\eqref{2}$</code> can be re-expressed as below.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{\tau \in \Omega_\tau} h_\tau(\tau)
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$h_\tau$</code> is the parameter representation of the smooth function <code class="language-plaintext highlighter-rouge">$h$</code>.</p>

<p>Natural gradient descent in this parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\tau$</code> is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{3}\label{3}
$$</code> where  <code class="language-plaintext highlighter-rouge">$\mathbf{F}_\lambda$</code> is the FIM, natural gradient is <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_{\tau_k} := [\mathbf{F}_\tau(\tau_k) ]^{-1} \nabla_\tau h_\tau(\tau_k)$</code> and the step-size <code class="language-plaintext highlighter-rouge">$\alpha$</code> is small enough so that  <code class="language-plaintext highlighter-rouge">$\tau_{k+1} \in \Omega_\tau$</code>.</p>

<p>Consider another intrinsic parameterization $\lambda$ so that <code class="language-plaintext highlighter-rouge">$\lambda=\mathbf{U} \tau$</code>, where <code class="language-plaintext highlighter-rouge">$\mathbf{U}$</code> is a constant (square) invertible matrix. 
For simplicity,  we further assume <code class="language-plaintext highlighter-rouge">$\Omega_\lambda=\{ \mathbf{U}\tau |\tau \in\Omega_\tau \} $</code>, where <code class="language-plaintext highlighter-rouge">$\Omega_\lambda$</code> is the parameter space of <code class="language-plaintext highlighter-rouge">$\lambda$</code>.</p>

<p>Natural gradient descent in this parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\lambda$</code> is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} -  \alpha  \hat{\mathbf{g}}_{\lambda_k} 
\end{aligned}\tag{4}\label{4}
$$</code> where <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_{\lambda_k} := [\mathbf{F}_\lambda(\lambda_k) ]^{-1} \nabla_\lambda h_\lambda(\lambda_k)$</code>  and the cost function is <code class="language-plaintext highlighter-rouge">$h_\lambda(\lambda) = h_\tau(\tau(\lambda))$</code>.</p>

<p>Recall that we have the <a href="/posts/2021/11/Geomopt03/#transformation-rules-for-natural-gradients-and-euclidean-gradients">transformation rule</a> for natural gradients as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\hat{\mathbf{g}}_\tau= \mathbf{Q}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$</code> where $Q_{ji}=\frac{\partial \tau^j(\lambda)}{\partial \lambda^i}$.</p>

<p>We can verify that $\mathbf{Q} = \mathbf{U}^{-1}$. Notice that $\tau_0 = \mathbf{U}^{-1} \lambda_0$ by construction.
The update in  <code class="language-plaintext highlighter-rouge">$\eqref{3}$</code> at iteration $k=1$ then can be re-expressed as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{1} \leftarrow \tau_{0} -  \alpha  \hat{\mathbf{g}}_{\tau_0} = \mathbf{U}^{-1} \lambda_0 -  \alpha  \mathbf{U}^{-1}  \hat{\mathbf{g}}_{\lambda_0} = \mathbf{U}^{-1} \lambda_1
\end{aligned}
$$</code></p>

<p>When <code class="language-plaintext highlighter-rouge">$\alpha$</code> is small enough, we have  <code class="language-plaintext highlighter-rouge">$\tau_1 \in \Omega_\tau$</code> and <code class="language-plaintext highlighter-rouge">$\lambda_1 \in \Omega_\lambda$</code>.
It is easy to show that <code class="language-plaintext highlighter-rouge">$\tau_k = \mathbf{U}^{-1} \lambda_k$</code> by induction.
Therefore, updates in <code class="language-plaintext highlighter-rouge">$\eqref{3}$</code> and <code class="language-plaintext highlighter-rouge">$\eqref{4}$</code> are equivalent.</p>

<h1 id="riemannian-gradient-descent-and-its-non-linear-invariance">Riemannian Gradient Descent and its (Non-linear) Invariance</h1>
<hr>
<p>Now we discuss a gradient-based method that is invariant to any intrinsic parameter transformation.</p>

<p>As mentioned before, a manifold in general does not have a vector-space structure. 
We has to <strong>artificially</strong> choose an intrinsic parameterization <code class="language-plaintext highlighter-rouge">$\tau$</code>, which gives rise to
a parametrization-dependence.
Therefore, it will be ideal if  a 
gradient-based method does not dependent on the choice of intrinsic parametrizations.</p>

<p>We will first introduce the concept of a (one-dimensional) geodesic $\gamma(t)$, which is the “shortest curve” on a manifold with a Riemannian metric (i.e., the Fisher-Rao metric).
Recall that in  <a href="/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional">Part II</a> we only define a distance between two Riemannian gradients evaluated at the same point. We can use the length of a geodesic to define the distance between two points on the manifold. 
In statistics, the distance induced by a geodesic with the Fisher-Rao metric is known as the Rao distance <a class="citation" href="#atkinson1981rao">[2]</a>. This is
known as the boundary value problem (BVP) of a geodesic. The boundary conditions are  specified by  a starting point and an end
point in the manifold. To solve this problem, we often solve an easier problem, which is known as the initial value
problem (IVP) of a geodesic. The initial conditions are  specified by a starting point and an initial Riemannian gradient/velocity.</p>

<p>We will only consider the IVP of a geodesic for simplicity. 
Consider an intrinsic parametrization $\tau$, where $\gamma_\tau(t)$ is the parameter representation of the geodesic.
To specify a geodesic, we need to provide a starting point $\tau_0$ on the manifold and a Riemannian gradient $\mathbf{v}_{\tau_0}$ evluated at point $\tau_0$.
The geodesic is the solution of a system of second-order non-linear ordinary differential equations (ODE) with the following initial conditions.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\gamma_\tau(0) = \tau_0; \,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$</code> where the geodesic is uniquely determined by the initial conditions and  <code class="language-plaintext highlighter-rouge">$\mathbf{I}_\tau$</code> is the domain of the parametric geodesic curve. We assume 0 is contained in the domain for simplicity.</p>

<p>For a <a href="https://en.wikipedia.org/wiki/Geodesic_manifold">geodesically complete</a> manifold, the domain of the geodesic is the whole space  <code class="language-plaintext highlighter-rouge">$\mathbf{I}_{\tau}
=\mathcal{R}$</code>. We will only consider this case in this post.</p>

<p>To avoid writing down the differential equations of a geodesic<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> (i.e., <a href="https://en.wikipedia.org/wiki/Levi-Civita_connection#Christoffel_symbols">Christoffel symbols</a> or <a href="https://en.wikipedia.org/wiki/Connection_form#Example:_the_Levi-Civita_connection">connection 1-form in a section</a>), 
researchers refer to it as the manifold exponential map.
Given an intrinsic parametrization $\tau$,
the  map at point $\tau_0$  is defined as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{Exp}_{\tau_0}\colon  T\mathcal{M}_{\tau_0} &amp; \mapsto \mathcal{M}\\
\mathbf{v}_{\tau_0} &amp; \mapsto \gamma_\tau(1) \,\,\,\, \textrm{s.t.} \,\,\,\,\,\, \gamma_\tau(0) = \tau_0;\,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$</code>  where <code class="language-plaintext highlighter-rouge">$t$</code> is fixed to be 1 and <code class="language-plaintext highlighter-rouge">$\tau_0$</code> denotes the initial point under  parametrization <code class="language-plaintext highlighter-rouge">$\tau$</code>.</p>

<div class="notice--danger">
  <p><strong>Warning</strong>:</p>

  <p>We use 
a parametric representation of a geodesic curve to define the exponential map.
Thus, the form of the map does depend on the choice of parametrizations.</p>
</div>

<p>Now, we can define a Riemannian gradient method.
Under intrinsic parametrization <code class="language-plaintext highlighter-rouge">$\tau$</code> of a manifold, (exact) Riemannian gradient descent (RGD) is defined as</p>
<div class="notice--success">
  <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{RGD:} &amp;\,\,\,\,\, 
\tau_{k+1} \leftarrow \mathrm{Exp}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} ) 
\end{aligned}
$$</code></p>
</div>
<p>where <code class="language-plaintext highlighter-rouge">$\tau_{k+1}$</code> always stays in the manifold thanks to the exponential map since <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_{\tau_k}$</code> is in the domain of the exponential map.</p>

<p>The invariance of this update is due to the uniqueness of the geodesic and the invariance of <a href="https://en.wikipedia.org/wiki/Solving_the_geodesic_equations">the Euler-Lagrange equation</a>. We will not discuss this further in this post to avoid complicated derivations.</p>

<p>Although Riemannian gradient descent is nice, the exponential map or the geodesic often has high computational cost and does not admit a closed-form expression.</p>

<h1 id="many-faces-of-natural-gradient-descent">Many faces of Natural-gradient Descent</h1>
<hr>
<h2 id="natural-gradient-descent-as-inexact-riemannian-gradient-descent">Natural-gradient Descent as Inexact Riemannian Gradient Descent</h2>

<p>Natural-gradient descent can be derived from a first-order (linear) approximation of the geodesic, which implies that natural-gradient descent is indeed an inexact Riemannian gradient update.
Natural-gradient descent is linearly invariant due to the approximation.</p>

<p>Consider a first-order Taylor approximation at $t=0$ of the geodesic shown below.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\gamma_\tau(t) \approx  \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (t-0)  
\end{aligned}
$$</code></p>

<div class="notice--danger">
  <p><strong>Warning</strong>:</p>

  <p>This approximation does not guarantee that the approximated geodesic stays on the manifold for all $t \neq 0$.</p>
</div>

<p>Recall that the exponential map is defined via the geodesic <code class="language-plaintext highlighter-rouge">$\gamma_\tau(1)$</code>.
We can use this approximation of the geodesic to define a new map (A.K.A. the Euclidean retraction map) as shown below.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{Ret}_{\tau_0}(\mathbf{v}_{\tau_0}) := \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (1-0) =\tau_0 + \mathbf{v}_{\tau_0}
\end{aligned}
$$</code></p>

<p>Therefore, an inexact Riemannian gradient update with this new map is defined as 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{k+1} \leftarrow \mathrm{Ret}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} )  = \tau_k  - \alpha  \hat{\mathbf{g}}_{\tau_k}
\end{aligned}
$$</code> which recovers natural-gradient descent.</p>

<h2 id="natural-gradient-descent-as-unconstrained-proximal-gradient-descent">Natural-gradient Descent as Unconstrained Proximal-gradient Descent</h2>

<p>In this section, we will make an additional  assumption:</p>

<div class="notice--success">
  <p><strong>Additional assumption</strong>:</p>

  <p>The parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\tau=\mathcal{R}^K$</code> is unconstrained.</p>
</div>

<p>As we mentioned before, the <strong>distances</strong> in the gradient space and the parameter space are defined differently. 
In the <a href="#riemannian-gradient-descent-and-its-non-linear-invariance">previous section</a>, we use the geodesic to define the distance between two points in a parameter space.</p>

<p>We could also use other “distances” denoted by <code class="language-plaintext highlighter-rouge">$\mathrm{D}(.,.)$</code> (i.e., Kullback–Leibler divergence) <a class="citation" href="#khan2016faster">[3]</a> to define the length between two points in a parameter space.</p>

<p>In the following section, we assume <code class="language-plaintext highlighter-rouge">$p(w|\mathbf{y})$</code> and <code class="language-plaintext highlighter-rouge">$p(w|\tau_k)$</code> are two members  in a parameteric distribution family <code class="language-plaintext highlighter-rouge">$p(w|\tau)$</code> indexed by  <code class="language-plaintext highlighter-rouge">$\mathbf{y}$</code> and <code class="language-plaintext highlighter-rouge">$\tau_k$</code>, respectively.</p>

<p>We define a class of f-divergence in this case. 
Note that a f-divergence can be defined in a more general setting.</p>

<h3 id="csiszar-f-divergence">Csiszar f-divergence</h3>
<hr>

<p>Let <code class="language-plaintext highlighter-rouge">$f:(0,+\infty)\mapsto \mathcal{R}$</code> be a  smooth scalar function satisfying all the following conditions.</p>
<ul>
  <li>convex in its domain</li>
  <li><code class="language-plaintext highlighter-rouge">$f(1)=0$</code></li>
  <li><code class="language-plaintext highlighter-rouge">$\ddot{f}(1)=1$</code></li>
</ul>

<p>A f-divergence for a parametric family <code class="language-plaintext highlighter-rouge">$p(w|\tau)$</code> is defined as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{D}_f(\mathbf{y},\tau_k) := \int f(\frac{p(w|\mathbf{y})}{p(w|\tau_k)}) p(w|\tau_k) dw.
\end{aligned}
$$</code>
Thanks to Jensen’s inequality,
a f-divergence is always non-negative as<br>
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{D}_f(\mathbf{y},\tau_k) \geq  f( \int \frac{p(w|\mathbf{y})}{p(w|\tau_k)} p(w|\tau_k) dw )  = f(\int p(w|\mathbf{y})dw) =f(1) =0 .
\end{aligned}
$$</code></p>

<p>The KL divergence is a f-divergence where $f(t)=-\log(t)$.</p>

<h3 id="proximal-gradient-descent">Proximal-gradient descent</h3>
<hr>
<p>Given such a “distance” <code class="language-plaintext highlighter-rouge">$\mathrm{D}(\mathbf{y},\tau_k)$</code>, we could perform an unconstrained proximal-gradient update as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{k+1} \leftarrow \arg\min_{\mathbf{y} \in  \mathcal{R}^K  } \{ \langle \mathbf{g}_{\tau_k}, \mathbf{y}\rangle   + \frac{1}{\alpha} \mathrm{D}(\mathbf{y},\tau_k) \}
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathbf{g}_{\tau_k}$</code> is a Eulcidean gradient and the parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\tau$</code> is unconstrained.</p>

<div class="notice--success">
  <p><strong>Claim</strong>:</p>

  <p>The secord-order Taylor approximation of any f-divergence <code class="language-plaintext highlighter-rouge">$\mathrm{D}_f(\mathbf{y},\tau_k)$</code> at <code class="language-plaintext highlighter-rouge">$\mathbf{y}=\tau_k$</code> is  <code class="language-plaintext highlighter-rouge">$\frac{1}{2}(\mathbf{y}-\tau_k)^T \mathbf{F}_\tau(\tau_k) (\mathbf{y}-\tau_k)$</code></p>
</div>

<div class="notice--info">
  <details>
<summary>Proof of the claim: (click to expand)</summary>
<fieldset class="field-set">
      <p>Proof:</p>

      <p>We will show that the second-order Talor approximation denoted by <code class="language-plaintext highlighter-rouge">$\mathrm{D}(\mathbf{y},\tau_k)$</code>  can be expressed as below.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{D}(\mathbf{y},\tau_k) := \underbrace{\mathrm{D}_f(\tau_k,\tau_k)}_{=0} + (\mathbf{y}-\tau_k)^T \underbrace{ [\nabla_y  \mathrm{D}_f(\mathbf{y},\tau_k) \big|_{y=\tau_k}]}_{=0} +\frac{1}{2} (\mathbf{y}-\tau_k)^T \underbrace{ [\nabla_y^2  \mathrm{D}_f(\mathbf{y},\tau_k)\big|_{y=\tau_k}]}_{ =\mathbf{F}_\tau(\tau_k) } (\mathbf{y}-\tau_k)
\end{aligned}
$$</code></p>

      <p>For the zero-order term, we have
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{D}_f(\tau_k,\tau_k) &amp; = \int f(\frac{p(w|{\tau_k})}{p(w|\tau_k)}) p(w|\tau_k) dw 
= \int \underbrace{f(1)}_{=0} p(w|\tau_k) dw =0.
\end{aligned}
$$</code></p>

      <p>For the first-order term, we have
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\nabla_y \mathrm{D}_f(\mathbf{y},\tau_k) &amp;  =
\int \nabla_y f(\frac{p(w|\mathbf{y})}{p(w|\tau_k)}) p(w|\tau_k) dw \\
&amp;=\int  \dot{f}\big(\frac{p(w|\mathbf{y})}{p(w|\tau_k)}\big) \frac{\nabla_y p(w|\mathbf{y})}{p(w|\tau_k)} p(w|\tau_k) dw \\
&amp;=\int  \dot{f}\big(\frac{p(w|\mathbf{y})}{p(w|\tau_k)}\big) \nabla_y p(w|\mathbf{y}) dw.
\end{aligned}
$$</code></p>

      <p>Note that when $y=\tau_k$, we can simplify the expression since 
<code class="language-plaintext highlighter-rouge">$\dot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big)=\dot{f}(1)$</code> is a constant.
Therefore,
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\nabla_y \mathrm{D}_f(\mathbf{y},\tau_k) \big|_{y=\tau_k}  
=\int  \dot{f}(1) \nabla_y p(w|\mathbf{y}) \big|_{y=\tau_k} dw =  
\dot{f}(1)  \nabla_y \underbrace{\Big[ \int   p(w|\mathbf{y}) dw \Big]}_{=1} \Big|_{y=\tau_k} = 0.
\end{aligned}
$$</code></p>

      <p>For the second-order term, we have
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\nabla_{y}^2 \mathrm{D}_f(y,\tau_k) &amp; = 
\int \nabla_{y}^2 f\big(\frac{p(w|y)}{p(w|\tau_k)}\big) p(w|\tau_k) dw  \\
&amp;= \int \nabla_{y} [\dot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big) \frac{\nabla_y p(w|y)}{p(w|\tau_k)} ] p(w|\tau_k) dw \\
&amp;= \int \Big[\ddot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big) \frac{\nabla_y p(w|y)}{p(w|\tau_k)} \frac{\nabla_y^T p(w|y)}{p(w|\tau_k)}+\dot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big) \frac{\nabla_y^2 p(w|y)}{p(w|\tau_k)} \Big] p(w|\tau_k) dw 
\end{aligned}
$$</code>
Note that when $y=\tau_k$, we can simplify the expression since <code class="language-plaintext highlighter-rouge">$\ddot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big)=\ddot{f}(1)=1$</code> and 
<code class="language-plaintext highlighter-rouge">$\dot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big)=\dot{f}(1)$</code> is a constant.</p>

      <p>Therefore,
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\nabla_{y}^2 \mathrm{D}_f(y,\tau_k) \Big|_{y=\tau_k} 
&amp;= \int \Big[\ddot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big) \frac{\nabla_y p(w|y)}{p(w|\tau_k)} \frac{\nabla_y^T p(w|y)}{p(w|\tau_k)} +\dot{f}\big(\frac{p(w|y)}{p(w|\tau_k)}\big) \frac{\nabla_y^2 p(w|y)}{p(w|\tau_k)} \Big] p(w|\tau_k) dw \Big|_{y=\tau_k} \\
&amp;=\underbrace{\int  \frac{\nabla_y p(w|y)}{p(w|\tau_k)} \frac{\nabla_y^T p(w|y)}{p(w|\tau_k)} p(w|\tau_k) dw  \Big|_{y=\tau_k}}_{\text{Term I}} + \underbrace{\int \dot{f}(1) \frac{\nabla_y^2 p(w|y)}{p(w|\tau_k)}  p(w|\tau_k) dw  \Big|_{y=\tau_k}}_{\text{Term II}}
\end{aligned}
$$</code></p>

      <p>Term II is zero since
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\int \dot{f}(1) \frac{\nabla_y^2 p(w|y)}{p(w|\tau_k)}  p(w|\tau_k) dw \Big|_{y=\tau_k}= \dot{f}(1) \int  \nabla_y^2 p(w|y) dw \Big|_{y=\tau_k}= \dot{f}(1) \nabla_y^2 \underbrace{\Big[\int   p(w|y) dw \Big] }_{=1} \Big|_{y=\tau_k}=0
\end{aligned}
$$</code></p>

      <p>Term I is the FIM since
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\int  \underbrace{ \frac{\nabla_y p(w|y)}{p(w|\tau_k)}}_{= \nabla_y \log p(w|y)}  \frac{\nabla_y^T p(w|y)}{p(w|\tau_k)} p(w|\tau_k) dw  \Big|_{y=\tau_k} &amp;=
E_{p(w|\tau_k)} \Big[ \nabla_y \log p(w|y)   \nabla_y^T \log p(w|y) \Big] \Big|_{y=\tau_k} \\
&amp;=
E_{p(w|\tau_k)} \Big[ \nabla_\tau \log p(w|\tau_k)   \nabla_\tau^T \log p(w|\tau_k) \Big] \\
&amp;= \mathbf{F}_\tau(\tau_k)
\end{aligned}
$$</code></p>

    </fieldset>
</details>
</div>

<p>By the claim, when <code class="language-plaintext highlighter-rouge">$\mathrm{D}(\mathbf{y},\tau_k)$</code> is the second-order Taylor approximation of a
f-divergence <code class="language-plaintext highlighter-rouge">$\mathrm{D}_f(\mathbf{y},\tau_k)$</code>  at <code class="language-plaintext highlighter-rouge">$\mathbf{y}=\tau_k$</code>,
the unconstrained proximal-gradient update can be re-expressed as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{k+1} \leftarrow \arg\min_{y \in  \mathcal{R}^K  } \{ \langle \mathbf{g}_{\tau_k}, \mathbf{y} \rangle   + \frac{1}{2\alpha} (\mathbf{y}-\tau_k)^T \mathbf{F}_\tau(\tau_k) (\mathbf{y}-\tau_k) \}
\end{aligned}
$$</code></p>

<p>It is easy to see that we can obtain the following natural-gradient update from the above expression.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \underbrace{ \mathbf{F}^{-1}_\tau(\tau_k) \mathbf{g}_{\tau_k}}_{ =\hat{\mathbf{g}}_{\tau_k} }
\end{aligned}
$$</code></p>

<div class="notice--success">
  <p>Note</p>

  <p>By using this Taylor approximation,
we essentially use the distance defined in a (Riemannian) gradient space to approximate the distance in a parameter space.</p>
</div>

<p>In <a href="/posts/2021/12/Geomopt05/#natural-gradient-descent-as-unconstrained-mirror-descent">Part V</a>   , we will show that  <code class="language-plaintext highlighter-rouge">$\mathrm{D}(y,\tau_k)$</code> can also be an exact KL divergence when $p(w)$ is an exponential family.
Under a particular parametrization,  natural-gradient descent also can be viewed as (unconstrained) mirror descent.</p>

<div class="notice--danger">
  <p><strong>Warning</strong>:</p>

  <p>The connection bewteen natural-gradient descent and proximal-gradient/mirror descent could break down in constrained cases. We will cover more about this point in <a href="/posts/2021/12/Geomopt06/">Part VI</a>.</p>
</div>

<h1 id="natural-gradient-descent-in-non-intrinsic-parameter-spaces">Natural-gradient Descent in Non-intrinsic Parameter Spaces</h1>
<hr>
<p>As mentioned in <a href="/posts/2021/09/Geomopt01/#intrinsic-parameterizations">Part I</a>, an intrinsic parametrization creates a nice parameter space (e.g., a local vector space structure) and guarantees a non-singular FIM.
We now discuss issues when it comes to  natural-gradient descent over non-intrinsic parametrizations including overparameterization.</p>

<ol>
  <li>
    <p>We may not have a local vector space structure in a non-intrinsic parameter space. Therefore, natural-gradient descent in this parameter space is pointless since the updated parameter will leave the parameter space. Indeed, the FIM could also be ill-defined in such cases. We will illustrate this by examples.</p>

    <div class="notice--info">
      <details>
 <summary>Bernoulli Example: Invalid NGD (click to expand)</summary>
 <fieldset class="field-set">
          <blockquote>

            <p>Consider Bernoulli family  $ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0&gt;0, \pi_1&gt;0, \pi_0+\pi_1=1  \}$ with parameter $\tau = (\pi_0,\pi_1)$.</p>

            <p>As we shown in <a href="/posts/2021/09/Geomopt01/#caveats-of-the-fisher-matrix-computation">Part I</a>, the FIM is ill-defined due to this eqaulity constraint.</p>

            <p>Moreover, the NGD update will violate the eqaulity constraint.</p>
          </blockquote>
        </fieldset>
 </details>
    </div>

    <div class="notice--info">
      <details>
 <summary>Von Mises–Fisher Example: Invalid NGD  (click to expand)</summary>
 <fieldset class="field-set">
          <blockquote>

            <p>Consider $2$-dimensional <a href="https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution">Von Mises–Fisher family</a>  $ \{p(\mathbf{w}|\tau):= C(\kappa) \exp(\kappa (w_1\mu_1+w_2\mu_2) ) \Big| \kappa&gt;0, \mu_1^2+\mu_2^2 =1  \}$ with parameter $\tau = (\kappa,\mu_1,\mu_2)$, where $ C(\kappa)$ is the normalization constant, <code class="language-plaintext highlighter-rouge">$\kappa$</code> is a positive scalar, $\mathbf{w}=(w_1,w_2)$ is a random unit vector defined in a circle,
 and <code class="language-plaintext highlighter-rouge">$\mathbf{\mu}=(\mu_1,\mu_2)$</code> is also a unit vector.</p>

            <p>We can show that the FIM is ill-defined under this parametrization due to this eqaulity constraint.</p>
          </blockquote>
        </fieldset>
 </details>
    </div>
  </li>
  <li>The FIM is singular in a non-intrinsic space. In theory, Moore–Penrose inverse could be used to compute natural-gradients so that natural-gradient descent is linearly invariant in this case. However, Moore–Penrose inverse often has to use the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> (SVD) and  destroies structures of the FIM.  In practice, the iteration cost of Moore–Penrose inverse is very high as illustrated in the following example.
    <div class="notice--info">
      <details>
 <summary>Example: High iteration cost (click to expand)</summary>
 <fieldset class="field-set">
          <blockquote>

            <p>Consider a $d$-dimensional Gaussian mixture family <code class="language-plaintext highlighter-rouge">$ \{ \frac{1}{C} \sum_{k=1}^{C} \mathcal{N}(w |\mathbf{\mu}_k,  \mathbf{\Sigma}_k)  \Big|  \mathbf{\mu}_k \in \mathcal{R}^d,  \mathbf{\Sigma}_k  \succ \mathbf{0}  \}$</code> with <code class="language-plaintext highlighter-rouge">$\tau = \{ \mu_k,\Sigma_k\}_{k=1}^{C} $</code>.</p>

            <p>If we  use the following initialization such that all $C$ components have the same mean <code class="language-plaintext highlighter-rouge">$\mu_0$</code> and the same covariance <code class="language-plaintext highlighter-rouge">$\mathbf{\Sigma}_0$</code>, this family becomes a Gaussian family.
In this case,
<code class="language-plaintext highlighter-rouge">$\tau$</code> is a non-intrinsic parameterization for a Gaussian family. Note that the FIM is singular under
 parametrization $\tau$.
The iteration cost of natural-gradient descent in this parameter space $\Omega_\tau$ will be $O(C^3 d^6)$ if  Moore–Penrose inverse is employed.</p>

            <p>Now, consider an equivalent Gaussian family <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(w |\mathbf{\mu},  \mathbf{\Sigma})  \Big|  \mathbf{\mu} \in \mathcal{R}^d,  \mathbf{\Sigma}  \succ \mathbf{0}  \}$</code> with <code class="language-plaintext highlighter-rouge">$\lambda =( \mu,\mathrm{vech}(\mathbf{\Sigma}) ) $</code>, where $\lambda$ is an intrinsic parameterization of the Gaussian family and initialized by 
mean $\mu_0$ and  covariance $\mathbf{\Sigma}_0$.</p>

            <p>As we will show in <a href="/posts/2021/12/Geomopt05/">Part V</a>, the iteration cost of natural-gradient descent in this parameter space $\Omega_\lambda$ will be $O(d^3)$ if we exploit structures of the exact non-singular FIM.</p>
          </blockquote>
        </fieldset>
 </details>
    </div>
  </li>
  <li>It is tempting to approximate the singular FIM by an emprical FIM with a scalar damping term and use Woodbury matrix identity to reduce the iteration cost of computing natural-gradients. However, sample-based emprical approximations could be problematic <a class="citation" href="#kunstner2019limitations">[4]</a>.
Moreover, damping introduces an additional tuning hyper-parameter and destories the linear invariance property of  natural-gradient descent. Such an approximation should be used with caution.</li>
</ol>

<h1 id="euclidean-gradient-descent-is-not-linearly-invariant">Euclidean Gradient Descent is NOT (Linearly) Invariant</h1>
<hr>
<p>For simplicity, consider an unconstrained optimization problem.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{\tau \in \mathcal{R}^K } h_\tau(\tau)
\end{aligned}
$$</code></p>

<p>Euclidean gradient descent (GD) in parametrization <code class="language-plaintext highlighter-rouge">$\tau$</code> is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha {\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{5}\label{5}
$$</code> where <code class="language-plaintext highlighter-rouge">${\mathbf{g}}_{\tau_k} := \nabla_\tau h_\tau(\tau_k)$</code> is a Euclidean gradient.</p>

<p>Consider a reparametrization  <code class="language-plaintext highlighter-rouge">$\lambda$</code> so that <code class="language-plaintext highlighter-rouge">$\lambda=\mathbf{U} \tau$</code>, where <code class="language-plaintext highlighter-rouge">$\mathbf{U}$</code> is a constant (square) invertible matrix. 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{\lambda \in \mathcal{R}^K } h_\lambda(\lambda):= h_\tau( \mathbf{U}^{-1} \lambda)
\end{aligned}
$$</code></p>

<p>The Euclidean gradient descent (GD) in parametrization <code class="language-plaintext highlighter-rouge">$\lambda$</code> is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} - \alpha {\mathbf{g}}_{\lambda_k} 
\end{aligned}\tag{6}\label{6}
$$</code> where <code class="language-plaintext highlighter-rouge">${\mathbf{g}}_{\lambda_k} := \nabla_\lambda h_\lambda(\lambda_k)$</code> is a Euclidean gradient.</p>

<p>Note that Euclidean gradients follow the <a href="/posts/2021/11/Geomopt03/#transformation-rules-for-natural-gradients-and-euclidean-gradients">transformation rule</a>  as</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{g}_\tau^T =  \mathbf{g}_\lambda^T \mathbf{J}
\end{aligned}
$$</code> where  <code class="language-plaintext highlighter-rouge">$J_{ki}:=\frac{\partial \lambda^k(\tau) }{ \partial \tau^i }$</code></p>

<p>We can verify that <code class="language-plaintext highlighter-rouge">$\mathbf{J}=\mathbf{U}$</code> and <code class="language-plaintext highlighter-rouge">$\mathbf{g}_\tau = \mathbf{U}^T \mathbf{g}_\lambda $</code>.</p>

<p>Notice that <code class="language-plaintext highlighter-rouge">$\tau_0 = \mathbf{U}^{-1} \lambda_0$</code> by construction.
The update in  <code class="language-plaintext highlighter-rouge">$\eqref{5}$</code> at iteration $k=1$ then can be re-expressed as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{1} \leftarrow \tau_{0} -  \alpha  {\mathbf{g}}_{\tau_0} = \mathbf{U}^{-1} \lambda_0 -  \alpha  \mathbf{U}^{T}  {\mathbf{g}}_{\lambda_0} \neq \mathbf{U}^{-1} \lambda_1
\end{aligned}
$$</code></p>

<p>It is easy to see that
updates in <code class="language-plaintext highlighter-rouge">$\eqref{5}$</code> and <code class="language-plaintext highlighter-rouge">$\eqref{6}$</code> are NOT equivalent.
Therefore,  Euclidean gradient descent is not invariant.</p>

<h1 id="newtons-method-is-linearly-invariant">Newton’s Method is Linearly Invariant</h1>
<hr>
<p>For simplicity, consider an unconstrained convex optimization problem.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{\tau \in \mathcal{R}^K } h_\tau(\tau)
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$h_\tau(\tau)$</code> is strongly convex and twice continuously differentiable w.r.t. <code class="language-plaintext highlighter-rouge">$\tau$</code>.</p>

<p>Newton’s method under parametrization <code class="language-plaintext highlighter-rouge">$\tau$</code> is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \mathbf{H}^{-1}_\tau(\tau_k) {\mathbf{g}}_{\tau_k} 
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">${\mathbf{g}}_{\tau_k} := \nabla_\tau h_\tau(\tau_k)$</code> is a Euclidean gradient and <code class="language-plaintext highlighter-rouge">$\mathbf{H}_\tau(\tau_k):=\nabla_\tau^2 h_\tau(\tau_k)$</code> is the Hessian.</p>

<p>Consider a reparametrization  <code class="language-plaintext highlighter-rouge">$\lambda$</code> so that <code class="language-plaintext highlighter-rouge">$\lambda=\mathbf{U} \tau$</code>, where <code class="language-plaintext highlighter-rouge">$\mathbf{U}$</code> is a constant (square) invertible matrix. 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{\lambda \in \mathcal{R}^K } h_\lambda(\lambda):= h_\tau( \mathbf{U}^{-1} \lambda)
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$h_\lambda(\lambda)$</code> is also strongly convex w.r.t. <code class="language-plaintext highlighter-rouge">$\lambda$</code> due to  <code class="language-plaintext highlighter-rouge">$\eqref{7}$</code>.</p>

<p>Newton’s method under parametrization <code class="language-plaintext highlighter-rouge">$\lambda$</code> is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} - \alpha \mathbf{H}^{-1}_\lambda(\lambda_k) {\mathbf{g}}_{\lambda_k} 
\end{aligned} 
$$</code> where <code class="language-plaintext highlighter-rouge">${\mathbf{g}}_{\lambda_k} := \nabla_\lambda h_\lambda(\lambda_k)$</code> is a Euclidean gradient and
<code class="language-plaintext highlighter-rouge">$\mathbf{H}_\tau(\lambda_k):=\nabla_\lambda^2 h_\lambda(\lambda_k)$</code> is the Hessian.</p>

<p>As we discussed in the previous section, 
Euclidean gradients follow the <a href="/posts/2021/11/Geomopt03/#transformation-rules-for-natural-gradients-and-euclidean-gradients">transformation rule</a>  as <code class="language-plaintext highlighter-rouge">$\mathbf{g}_\tau^T =  \mathbf{g}_\lambda^T \mathbf{J}$</code>, where
<code class="language-plaintext highlighter-rouge">$\mathbf{J}=\mathbf{U}$</code>.</p>

<p>Surprisingly, for a linear transformation, the Hessian follows the <a href="/posts/2021/11/Geomopt03/#transformation-rule-for-the-fisher-information-matrix">transformation rule</a>  like the FIM as</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{H}_{\tau} (\tau_k) &amp;= \nabla_\tau ( \mathbf{g}_{\tau_k} ) \\
&amp;=\nabla_\tau ( \mathbf{J}^T \mathbf{g}_{\lambda_k} ) \\
&amp;=\mathbf{J}^T\nabla_\tau (  \mathbf{g}_{\lambda_k} ) + \underbrace{[\nabla_\tau \mathbf{J}^T ]}_{=0} \mathbf{g}_{\lambda_k}  \,\,\,\,\text{(In linear cases, } \mathbf{J} = \mathbf{U} \text{ is a
constant)}   \\
&amp;=\mathbf{J}^T [ \nabla_\lambda (  \mathbf{g}_{\lambda_k} ) ] \mathbf{J} \\ 
&amp;=\mathbf{J}^T \mathbf{H}_{\lambda} (\lambda_k)\mathbf{J} 
\end{aligned}\tag{7}\label{7}
$$</code></p>

<p>Therefore, the direction in Newton’s method denoted by <code class="language-plaintext highlighter-rouge">$\tilde{\mathbf{g}}_{\tau_k} := \mathbf{H}^{-1}_\tau(\tau_k) \mathbf{g}_{\tau_k}$</code> is transformed like natural-gradients in <strong>linear</strong> cases as</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tilde{\mathbf{g}}_{\tau_k} &amp;:= \mathbf{H}^{-1}_\tau(\tau_k) \mathbf{g}_{\tau_k} \\
&amp;= [ \mathbf{J}^T \mathbf{H}_{\lambda} (\lambda_k)\mathbf{J} ]^{-1} \mathbf{g}_{\tau_k} \\
&amp;=  \mathbf{J}^{-1} \mathbf{H}^{-1}_{\lambda} (\lambda_k)\mathbf{J}^{-T} [ \mathbf{J}^{T}\mathbf{g}_{\lambda_k} ] \\
&amp;=  \mathbf{J}^{-1} \mathbf{H}^{-1}_{\lambda} (\lambda_k) \mathbf{g}_{\lambda_k}  \\
&amp;=  \mathbf{J}^{-1}  \tilde{\mathbf{g}}_{\lambda_k}  \\
&amp;=  \mathbf{Q}  \tilde{\mathbf{g}}_{\lambda_k}  \\
\end{aligned} 
$$</code> where by the definition we have <code class="language-plaintext highlighter-rouge">$\mathbf{Q}= \mathbf{J}^{-1}$</code>.</p>

<p>The consequence is that Newton’s method like natural-gradient descent is linearly invariant.</p>

<h2 id="the-hessian-is-not-a-valid-manifold-metric">The Hessian is not a valid manifold metric</h2>

<p>The Hessian <code class="language-plaintext highlighter-rouge">$\mathbf{H}_\tau(\tau_k)=\nabla_\tau^2 h_\tau(\tau_k)$</code>  in general is not a valid manifold metric since it does not follow the transformation
rule of a metric in non-linear cases.</p>

<p>Contrastingly, the FIM is a valid manifold metric. Recall that the FIM can also be computed
as
<code class="language-plaintext highlighter-rouge">$\mathbf{F}_\tau(\tau) = E_{p(w|\tau)}\big[ -\nabla_\tau^2 \log p(w|\tau) \big]$</code>.</p>

<div class="notice--success">
  <p><strong>Claim</strong>:</p>

  <p>The FIM follows the transformation
rule even in non-linear cases.</p>
</div>

<div class="notice--info">
  <details>
<summary>
Proof of the claim (click to expand)
</summary>
<fieldset class="field-set">
      <blockquote>
        <p>Proof</p>

        <p>Given a non-linear intrinsic reparametrization <code class="language-plaintext highlighter-rouge">$\lambda$</code>, recall that the Jacobian matrix <code class="language-plaintext highlighter-rouge">$\mathbf{J}(\tau_k)$</code> in <code class="language-plaintext highlighter-rouge">$\eqref{7}$</code> is no longer a constant matrix but a square and non-singular matrix.
In this case, the FIM still follows the <a href="/posts/2021/11/Geomopt03/#transformation-rule-for-the-fisher-information-matrix">transformation rule</a> thanks to <a href="https://en.wikipedia.org/wiki/Score_(statistics)#Mean">the expectation of the score function</a>.</p>

        <p><code class="language-plaintext highlighter-rouge">$$
\require{cancel}
\begin{aligned}
&amp; \mathbf{F}_\tau(\tau_k)\\
=&amp; E_{p(w|\tau_k)}\big[ -\nabla_\tau^2 \log p(w|\tau_k) \big]  \\
=&amp; E_{p(w|\lambda_k)}\big[ -\nabla_\tau^2 \log p(w|\tau_k) \big]  \\
=&amp; E_{p(w|\lambda_k)}\big[ -\nabla_\tau [ \mathbf{J}^T(\tau_k) \nabla_\lambda \log p(w|\lambda_k)]  \big]  \\
=&amp; -    \mathbf{J}^T(\tau_k)   E_{p(w|\lambda_k)}\big[  \nabla_\tau  \nabla_\lambda \log p(w|\lambda_k) \big]   - [\nabla_\tau \mathbf{J}^T(\tau_k)  ]  \underbrace{  \cancelto{=0}{E_{p(w|\lambda_k)}\big[  \nabla_\lambda \log p(w|\lambda_k) \big]}  }_{ \text{ (the expectation of the score is zero)}  }   \\
=&amp; -    \mathbf{J}^T(\tau_k)   E_{p(w|\lambda_k)}\big[  [\nabla_\lambda^2 \log p(w|\lambda_k) ] \mathbf{J }(\tau_k) \big] \\
=&amp; -    \mathbf{J}^T(\tau_k)    E_{p(w|\lambda_k)}\big[  \nabla_\lambda^2 \log p(w|\lambda_k) \big] \mathbf{J}(\tau_k) \\
=&amp;\mathbf{J }^T(\tau_k)  \mathbf{F}_\lambda(\lambda_k) \mathbf{J}(\tau_k)  
\end{aligned}
$$</code></p>
      </blockquote>
    </fieldset>
</details>
</div>

<p>We will discuss in
<a href="/posts/2021/12/Geomopt05/#minimal-parametrizations-of-exponential-family">Part V</a>, for a special parametrization <code class="language-plaintext highlighter-rouge">$\tau$</code> (known as a natural parametrization) of <a href="/posts/2021/12/Geomopt05/#exponential-family">exponential family</a>, the FIM under this parametrization can be computed  as
<code class="language-plaintext highlighter-rouge">$\mathbf{F}_\tau(\tau) = \nabla_\tau^2 A_\tau(\tau)$</code>, where <code class="language-plaintext highlighter-rouge">$A_\tau(\tau)$</code> is a strictly convex function.</p>

<p>In the literature, the exponential family with a natural parametrization is known as a Hessian manifold <a class="citation" href="#shima2007geometry">[5]</a>, where the FIM
under this kind of parametrization is called a Hessian metric.
However, a non-linear reparametrization will lead to a non-natural parametrization.</p>

<hr>
<h1 id="references">References</h1>
<p class="bibliography"></p>
<p><span id="amari1998natural">[1] S.-I. Amari, "Natural gradient works efficiently in learning," <i>Neural computation</i> <b>10</b>:251–276 (1998).</span></p>
<p><span id="atkinson1981rao">[2] C. Atkinson &amp; A. F. S. Mitchell, "Rao’s distance measure," <i>Sankhyā: The Indian Journal of Statistics, Series A</i> 345–365 (1981).</span></p>
<p><span id="khan2016faster">[3] M. E. Khan, R. Babanezhad, W. Lin, M. Schmidt, &amp; M. Sugiyama, "Faster stochastic variational inference using Proximal-Gradient methods with general divergence functions," <i>Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence</i> (2016), pp. 319–328.</span></p>
<p><span id="kunstner2019limitations">[4] F. Kunstner, P. Hennig, &amp; L. Balles, "Limitations of the empirical Fisher approximation for natural gradient descent," <i>Advances in Neural Information Processing Systems</i> <b>32</b>:4156–4167 (2019).</span></p>
<p><span id="shima2007geometry">[5] H. Shima, <i>The geometry of Hessian structures</i> (World Scientific, 2007).</span></p>

<h2 id="footnotes">Footnotes:</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In Riemannian geometry, a geodesic is induced by the Levi-Civita connection. This connection is known as the metric compatiable parallel transport. Christoffel symbols are used to represent the connection in a coordinate/parametrization system. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#information-geometry" class="page__taxonomy-item" rel="tag">Information Geometry</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#natural-gradient-descent" class="page__taxonomy-item" rel="tag">Natural Gradient Descent</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#riemannian-manifold" class="page__taxonomy-item" rel="tag">Riemannian Manifold</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-11-15T00:00:00-08:00">November 15, 2021</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title" data-translate="share_on_label">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Part+IV%3A+Natural+and+Riemannian++Gradient+Descent%20informationgeometryML.github.io%2Fposts%2F2021%2F11%2FGeomopt04%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=informationgeometryML.github.io%2Fposts%2F2021%2F11%2FGeomopt04%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=informationgeometryML.github.io%2Fposts%2F2021%2F11%2FGeomopt04%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>



     

  <script src="https://utteranc.es/client.js" repo="informationgeometryML/informationgeometryML.github.io" issue-term="url" label="blog-comments" theme="github-light" crossorigin="anonymous" async>
  </script>





</section>


      
  <nav class="pagination">
    
      <a href="/posts/2021/11/Geomopt03/" class="pagination--pager" title="Part III: Invariance of Natural-Gradients
">Previous</a>
    
    
      <a href="/posts/2021/12/Geomopt05/" class="pagination--pager" title="Part V: Efficient Natural-gradient Methods for Exponential Family
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/12/Geomopt06/" rel="permalink">Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/12/Geomopt05/" rel="permalink">Part V: Efficient Natural-gradient Methods for Exponential Family
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  18 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/11/Geomopt03/" rel="permalink">Part III: Invariance of Natural-Gradients
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Goal
This blog post should help readers to understand the invariance of natural-gradients.
We will also discuss why the Euclidean steepest direction is NOT i...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/10/Geomopt02/" rel="permalink">Part II: Natural-Gradients Evaluated at one Point
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  13 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Goal
This blog post should help readers to understand natural-gradients, which are known as Riemannian gradients with the Fisher-Rao metric.
The main propose...</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow</strong></li>
    
    
    
    
      <li><a href="https://github.com/informationgeometryML"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">© 2022 Information Geometry in ML. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.2.0/js/all.js"></script>












  
    <script src="/assets/js/custom.js"></script>
  
    <script src="/assets/js/translations.js"></script>
  
    <script src="/assets/js/math-code.js"></script>
  



  </body>
</html>
