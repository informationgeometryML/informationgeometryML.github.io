<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.12.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Part IV: Natural and Riemannian  Gradient Descent - Information Geometry in Machine Learning</title>
<meta name="description" content="Warning: working in Progress (incomplete)">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Information Geometry in Machine Learning">
<meta property="og:title" content="Part IV: Natural and Riemannian  Gradient Descent">
<meta property="og:url" content="/posts/2021/11/Geomopt04/">


  <meta property="og:description" content="Warning: working in Progress (incomplete)">







  <meta property="article:published_time" content="2021-11-15T00:00:00-08:00">





  

  


<link rel="canonical" href="/posts/2021/11/Geomopt04/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Information Geometry in ML",
      "url": "https://github.com/pages/informationgeometryML/informationgeometryML.github.io",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Information Geometry in Machine Learning Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>


<!-- end custom head snippets -->

  </head>

  <body class="layout--single mywide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Information Geometry in Machine Learning</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="/" >Home</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/news/" >News</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/year-archive/" >Blog Posts</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  

  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Part IV: Natural and Riemannian  Gradient Descent">
    <meta itemprop="description" content="Warning: working in Progress (incomplete)">
    <meta itemprop="datePublished" content="November 15, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Part IV: Natural and Riemannian  Gradient Descent
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  10 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>Warning: working in Progress (incomplete)</p>

<h2 id="goal">Goal</h2>
<p>This blog post should help readers to understand natural-gradient descent and Riemannian gradient descent.</p>

<p>We will give an informal introduction with a focus on high level of ideas.</p>

<h1 id="two-kinds-of-spaces">Two kinds of Spaces</h1>
<hr />
<p>As we disucssed in <a href="/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional">Part II</a>, the parameter space $\Omega_\tau$ and the tangent space denoted by <code class="language-plaintext highlighter-rouge">$T\mathcal{M}_{\tau_0}$</code> at point $\tau_0$ are different spaces. Recall that the tangent space is a vector space and <code class="language-plaintext highlighter-rouge">$T\mathcal{M}_{\tau_0}=\mathcal{R}^K$</code> while the parameter space $\Omega_\tau$ is like a local vector space in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>, where $K$ is the dimension of the manifold. Moreover, $\Omega_\tau \subset T\mathcal{M}_{\tau_0}$ since  $\tau$ is an <a href="/posts/2021/09/Geomopt01/#intrinsic-parameterizations">intrinsic parametrization</a>.</p>

<p>In a manifold case, we have to explicitly distinguish the difference between the representation of a point/parameter and a (Riemannian) vector/gradient.
The following figure illustrates the difference between the <strong>domain</strong> of these two spaces. Moreover, the <strong>norm/distance</strong> in each of these two spaces is defined differently.</p>

<p><img src="/img/sphere.png" width="500" /></p>

<h1 id="natural-gradient-descent-in-an-intrinsic-parameter-space">Natural-gradient Descent in an Intrinsic Parameter Space</h1>
<hr />

<p>Using intrinstic parametrization $\tau$, we can perform a natural-gradient update known as natural-gradient descent. 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{1}\label{1}
$$</code> where <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_{\tau_k}$</code> is a natural/Riemannian gradient evaluated at point <code class="language-plaintext highlighter-rouge">$\tau_{k}$</code> and $\alpha&gt;0$ is a step-size.</p>

<p>The update in Eq. <code class="language-plaintext highlighter-rouge">$\eqref{1}$</code> is valid since the parameter space $\Omega_\tau$  has a local vector-space structure thanks to the use of an intrinsic parametrization.
However, when $\Omega_\tau$ is a proper subset of $T\mathcal{M}_{\tau_k}$ (i.e., <code class="language-plaintext highlighter-rouge">$\Omega_\tau \neq T\mathcal{M}_{\tau_k} $</code>), the update in Eq. <code class="language-plaintext highlighter-rouge">$\eqref{1}$</code> is valid only when the step-size $\alpha$ is small enough so that  <code class="language-plaintext highlighter-rouge">$\tau_{k+1} \in \Omega_\tau$</code>.</p>

<blockquote>
  <p>Example:</p>

  <p>Consider a 1-dimensional  Gaussian family.
We specify an intrinsic parameterization $\mathbf{\tau}$  as <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&gt;0 \}$</code> with <code class="language-plaintext highlighter-rouge">$\tau = (\mu,\sigma) $</code>. <br /></p>

  <p>We have to properly select the step-size $\alpha$ for natural-gradient descent in  <code class="language-plaintext highlighter-rouge">$\eqref{1}$</code> due to the positivity constraint in $\sigma$.</p>

  <p>In multivariate Gaussian cases, we have to handle a positive-definite constraint.</p>
</blockquote>

<h1 id="natural-gradient-descent-is-linearly-invariant">Natural-gradient Descent is Linearly Invariant</h1>
<hr />

<p>Recall that in <a href="/posts/2021/11/Geomopt03/#parameter-transform-and-invariance">Part III</a>, we have shown that natural-gradients are invaraint under any intrinsic parameter transformation.
The parameter transformation can be non-linear.</p>

<p>It is natural to expect that natural-gradient descent has a similar property. However, natural-gradient descent is only invariant under  an intrinsic <strong>linear</strong> transformation. Note that Newton’s method is also linearly invariant while Euclidean gradient descent is not.</p>

<p><span style="color:red"><strong>Warning</strong></span>: Be aware of the difference of the invaraince property between natural-gradient and natural-gradient descent.</p>

<p>Let’s consider the following (scalar) optimization problem on a manifold $\mathcal{M}$ with the Fisher-Rao metric $F$.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{x \in \mathcal{M}} h(x)
\end{aligned}\tag{2}\label{2}
$$</code></p>

<p>Note that $\mathcal{M}$ in general does not have a vector-space structure. 
We consider an intrinstic parameterization $\tau$ so that the parameter space $\Omega_\tau$ at least has a local  vector-space structure.
The problem in <code class="language-plaintext highlighter-rouge">$\eqref{2}$</code> can be re-expressed as below.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{\tau \in \Omega_\tau} h_\tau(\tau)
\end{aligned}
$$</code> where $h_\tau$ is the parameter representation of scalar smooth function $h$.</p>

<p>Natural gradient descent in this parameter space $\Omega_\tau$ is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{3}\label{3}
$$</code> where <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_{\tau_k} := [\mathbf{F}_\tau(\tau_k) ]^{-1} \nabla_\tau h_\tau(\tau_k)$</code> and the step-size $\alpha$ is small enough so that  $\tau_{k+1} \in \Omega_\tau$.</p>

<p>Consider another intrinstic parameterization $\lambda$ so that $\lambda=\mathbf{U} \tau$, where $\mathbf{U}$ is a constant (square) invertible matrix. 
When $\lambda$ is a valid parameterization, we know that <code class="language-plaintext highlighter-rouge">$\{ \mathbf{U}\tau |\tau \in\Omega_\tau \} 	\cap \Omega_\lambda \neq \emptyset$</code>.
For simplicity,  we further assume <code class="language-plaintext highlighter-rouge">$\{ \mathbf{U}\tau |\tau \in\Omega_\tau \} = \Omega_\lambda$</code>, where $\Omega_\lambda$ is the parameter space of $\lambda$. In general, we could use a smaller parameter space either $\Omega_\lambda$ or $\Omega_\tau$ so that this additional assumption holds.</p>

<p>Natural gradient descent in this parameter space $\Omega_\lambda$ is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} -  \alpha  \hat{\mathbf{g}}_{\lambda_k} 
\end{aligned}\tag{4}\label{4}
$$</code> where <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_{\lambda_k} := [\mathbf{F}_\lambda(\lambda_k) ]^{-1} \nabla_\lambda h_\lambda(\lambda_k)$</code></p>

<p>Recall that we have the <a href="/posts/2021/11/Geomopt03/#parameter-transformation-and-invariance">transformation rule</a> for natural gradients as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\hat{\mathbf{g}}_\tau= \mathbf{Q}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$</code> where $Q_{ji}=\frac{\partial \tau^j(\lambda)}{\partial \lambda^i}$.</p>

<p>We can verify that $\mathbf{Q} = \mathbf{U}^{-1}$. Notice that $\tau_0 = \mathbf{U}^{-1} \lambda_0$ by construction.
The update in  <code class="language-plaintext highlighter-rouge">$\eqref{3}$</code> at iteration $k=1$ then can be re-expressed as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{1} \leftarrow \tau_{0} -  \alpha  \hat{\mathbf{g}}_{\tau_0} = \mathbf{U}^{-1} \lambda_0 -  \alpha  \mathbf{U}^{-1}  \hat{\mathbf{g}}_{\lambda_0} 
\end{aligned}
$$</code></p>

<p>Therefore, it is easy to show that $\tau_k = \mathbf{U}^{-1} \lambda_k$ by induction. Updates in <code class="language-plaintext highlighter-rouge">$\eqref{3}$</code> and <code class="language-plaintext highlighter-rouge">$\eqref{4}$</code> are equivalent when $t$ is small enough.</p>

<h1 id="euclidean-gradient-descent-is-not-linearly-invariant">Euclidean Gradient Descent is NOT (Linearly) Invariant</h1>
<p>to do:
add an exapmle</p>

<h1 id="riemannian-gradient-descent-and-its-non-linear-invariance">Riemannian Gradient Descent and its (Non-linear) Invariance</h1>

<p>Now we discuss a gradient-based method that is invariant to any intrinsic parameter transformation.
We will first introduce the concept of a (one-dimensional) geodesic $\gamma(t)$, which is the “shortest curve” on a manifold with a Riemannian metric (e.g., the Fisher-Rao metric).
Recall that in  <a href="/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional">Part II</a> we only define a distance between two Riemannian gradients evaluated at the same point. We can use the length of a geodesic to define the distance between two points on the manifold.</p>

<p>Consider an intrinsic parametrization $\tau$, where $\gamma_\tau(t)$ is the parameter representation of the geodesic.
To specify a geodesic, we need to provide a starting point $\tau_0$ on the manifold and a Riemannian gradient $\mathbf{v}_{\tau_0}$ evluated at point $\tau_0$.
The geodeisc is the solution of a system of second-order non-linear ordinary differential equations (ODE) with the following initial conditions.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\gamma_\tau(0) = \tau_0; \,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$</code> where the geodesic is determined by the initial conditions and the domain <code class="language-plaintext highlighter-rouge">$\mathbf{I}_\tau$</code> of the geodesic contains 0 and 1.</p>

<p>We will use the following map to define Riemannian gradient descent without specifying complicated  differential equations (e.g., Christoffel symbols) in the geodesic.
We define a manifold expoential map at point $\tau_0$ for a manifold  via the geodesic as 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{Exp}_{\tau_0}\colon  T\mathcal{M}_{\tau_0} &amp; \mapsto \mathcal{M}\\
\mathbf{v}_{\tau_0} &amp; \mapsto \gamma_\tau(1) \,\,\,\, \textrm{s.t.} \,\,\,\,\,\, \gamma_\tau(0) = \tau_0;\,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$</code> Technically, we should require  manifold <code class="language-plaintext highlighter-rouge">$\mathcal{M}$</code> to be geodesically complete so that the domain of the expoential map is the whole tangent space. 
Equiavalently, <code class="language-plaintext highlighter-rouge">$\mathbf{I}_\tau $</code> is the whole <code class="language-plaintext highlighter-rouge">$\mathcal{R}^1$</code> space in such cases.</p>

<p>Under intrinsic parametrization $\tau$, (exact) Riemannian gradient descent is defined as 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{k+1} \leftarrow \mathrm{Exp}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} ) 
\end{aligned}
$$</code></p>

<p>The invariance of this update is due to the uniqueness of ODE and transformation rules for natural-gradients, Fisher information matrix, and Christoffel symbols. We will not discuss this further in this post to avoid complicated derivations. 
Although Riemannian gradient descent is nice, the expoential map or the geodesic often does not have a closed form expression.</p>

<h1 id="natural-gradient-descent-as-inexact-riemannian-gradient-descent">Natural-gradient Descent as Inexact Riemannian Gradient Descent</h1>

<p>Natural-gradient descent can be viewed as a first-order (linear) approximation of the geodesic, which implies that natural-gradient descent is indeed an inexact Riemannian gradient update.
Natural-gradient descent only is linearly invariant due to the approximation.</p>

<p>Consider a first-order Taylor approximation at $t=0$ of the geodesic shown below.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\gamma_\tau(t) \approx  \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (t-0)  
\end{aligned}
$$</code> Note that this approximation does not guarantee that the approximated geodesic stays on the manifold for all $t \neq 0$.</p>

<p>Recall that the  expoential map  is defined via the geodesic  <code class="language-plaintext highlighter-rouge">$\gamma_\tau(1)$</code>.
We can similarly define an approximated  expoential map (A.K.A. the Euclidean retraction map)  as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{Ret}_{\tau_0}(\mathbf{v}_{\tau_0}) := \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (1-0) =\tau_0 + \mathbf{v}_{\tau_0}
\end{aligned}
$$</code></p>

<p>Therefore, the inexact Riemannian gradient update is defined as 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{k+1} \leftarrow \mathrm{Ret}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} )  = \tau_k  - \alpha  \hat{\mathbf{g}}_{\tau_k}
\end{aligned}
$$</code> which recovers natural-gradient descent.</p>

<h1 id="issues-of-natural-gradient-descent-in-non-intrinsic-parameter-spaces">Issues of Natural-gradient Descent in Non-intrinsic Parameter Spaces</h1>
<hr />
<p>As mentioned in <a href="/posts/2021/09/Geomopt01/#intrinsic-parameterizations">Part I</a>, an intrinsic parametrization creates a nice parameter space (e.g., a local vector space structure) and guarantees a non-singular FIM.
We now discuss issues when it comes to  natural-gradient descent over non-intrinsic parametrizations including overparameterization.</p>

<p>The first issue of using a non-intrinsic parametrization is that we may not have a local vector space structure in the parameter space. Therefore, natural-gradient descent in this parameter space is pointless since the updated parameter will leave the parameter space. Indeed, the FIM is not well-defined in such cases. We will illustrated by examples.</p>

<blockquote>
  <p>Example: Bernoulli family:</p>

  <p>Consider Bernoulli family  $ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0&gt;0, \pi_1&gt;0, \pi_0+\pi_1=1  \}$ with parameter $\tau = (\pi_0,\pi_1)$.
As we shown in <a href="/posts/2021/09/Geomopt01/#caveats-of-the-fisher-matrix-computation">Part I</a>, the FIM is ill-defined due to this eqaulity constraint.</p>
</blockquote>

<blockquote>
  <p>Example: Von Mises–Fisher family:</p>

  <p>Consider $2$-dimensional Von Mises–Fisher  family  $ \{  C(\kappa) \exp(\kappa (w_1\mu_1+w_2\mu_2) ) \Big| \kappa&gt;0, \mu_1^2+\mu_2^2 =1  \}$ with parameter $\tau = (\kappa,\mu_1,\mu_2)$, where $ C(\kappa)$ is the normalization constant, $\mathbf{w}=(w_1,w_2)$ is a random unit vector defined in a circle,
$\kappa$ is a positive scalar, and <code class="language-plaintext highlighter-rouge">$\mathbf{\mu}=(\mu_1,\mu_2)$</code> is also a unit vector.
We can show that the FIM is ill-defined under this parametrization due to this eqaulity constraint.</p>
</blockquote>

<p>The second issue is that we have to deal with a singular FIM. In theory, Moore–Penrose inverse could be used to compute natural-gradients so that natural-gradient descent is linearly invariant in this case. However, Moore–Penrose inverse often has to use the singular value decomposition (SVD) and the singular FIM often destroy structures in a non-singular FIM.  In practice, the iteration cost of Moore–Penrose inverse is very high as illustrated in the following example. Moreover, natural-gradient descent in non-intrinsic parameter spaces could easily violate parameter constraints.</p>

<blockquote>
  <p>Example:  (parameter constraint violation)</p>

  <p>Consider a univarate  Gaussian family with zero mean <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(w |0,  \sigma_1 \sigma_2) \Big|  \sigma_1&gt;0, \sigma_2 &gt; 0  \}$</code> with <code class="language-plaintext highlighter-rouge">$\tau = (\sigma_1,\sigma_2) $</code>, where <code class="language-plaintext highlighter-rouge">$ \sigma_1 \sigma_2$</code> is the varaince of this Gaussian family.</p>

  <p>Natural-gradient descent in this parameter space $\Omega_\tau$ could easily violate the parameter constraints when $\sigma_1$ is very closed to $0$ even when the variance $\sigma_1 \sigma_2 $ is big enough.</p>

  <p>Let’s consider an intrinsic parameterization of this family as <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(w |0,  \sigma) \Big|  \sigma &gt; 0  \}$</code> with <code class="language-plaintext highlighter-rouge">$\lambda = \sigma $</code>, where <code class="language-plaintext highlighter-rouge">$ \sigma$</code> is the varaince of this Gaussian family.
Natural-gradient descent in this parameter space $\Omega_\lambda$ less likely violate the parameter constraints when the variance $\sigma_1 \sigma_2 $ is big enough.</p>
</blockquote>

<blockquote>
  <p>Example:  (high iteation cost)</p>

  <p>Consider a $d$-dimensional Gaussian mixture family <code class="language-plaintext highlighter-rouge">$ \{ \frac{1}{C} \sum_{k=1}^{C} \mathcal{N}(w |\mathbf{\mu}_k,  \mathbf{\Sigma}_k)  \Big|  \mathbf{\mu}_k \in \mathcal{R}^d,  \mathbf{\Sigma}_k  \succ \mathbf{0}  \}$</code> with <code class="language-plaintext highlighter-rouge">$\tau = \{ \mu_k,\Sigma_k\}_{k=1}^{C} $</code>.
If we  use the following initialization such that all $K$ components have the same mean $\mu_0$ and the same variance $\Sigma_0$, this family becomes a Gaussian family. In this case, the FIM of this mixture is singular.</p>

  <p>The iteration cost of natural-gradient descent in this parameter space $\Omega_\tau$ will be $O(C^3 d^6)$ if  Moore–Penrose inverse is employed.</p>

  <p>Now, consider the equivalent Gaussian family <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(w |\mathbf{\mu}_0,  \mathbf{\Sigma}_0)  \Big|  \mathbf{\mu}_0 \in \mathcal{R}^d,  \mathbf{\Sigma}_0  \succ \mathbf{0}  \}$</code> with <code class="language-plaintext highlighter-rouge">$\lambda =( \mu_0,\Sigma_0 ) $</code>, where $\lambda$ is an intrinsic parameterization of the Gaussian family.</p>

  <p>As we will shown in Part V, the iteration cost of natural-gradient descent in this parameter space $\Omega_\lambda$ will be $O(d^3)$ if we exploit structures of the exact non-singular FIM.</p>
</blockquote>

<p>To reduce the iteration cost of computing natural-gradients, it is tempting to use an emprical FIM with a scalar damping term to handle the singularity of the FIM. However, damping introduces an additional tuning hyper-parameter and destories the linear invariance property of  natural-gradient descent.</p>

<h1 id="natural-gradient-descent-as-an-proximal-gradient-method">Natural-gradient Descent as an Proximal Gradient Method</h1>
<p>As we mentioned before, the <strong>distances</strong> in  the gradient space and the parameter space are defined differently. 
In the <a href="#riemannian-gradient-descent-and-its-non-linear-invariance">previous section</a>, we use the geodesic to define the distance between two points in a parameter space.</p>

<p>We could also use other “distances” denoted by <code class="language-plaintext highlighter-rouge">$\mathrm{D}(.,.)$</code> (e.g., Kullback–Leibler divergence or f-divergence) to define the length between two points in a parameter space.
Given such of definition of “distance”, we could obtain new gradient-based updates via the proximal gradient framework as shown below.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\tau_{k+1} = \arg\min_{y \in  \Omega_\tau  } \{ \langle \mathbf{g}_{\tau_k}, y\rangle   + \mathrm{D}(y,\tau_k) \}
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathbf{g}_{\tau_k}$</code> is a Eulcidean gradient.</p>

<p>When $\mathrm{D}(y,\tau_k)$ is a secord-order Taylor approximation of the KL divergence <code class="language-plaintext highlighter-rouge">$\mathrm{KL} [q(w|\tau_k) || q(w|y)]$</code> (in general, any f-divergence)  at $\tau_k$,  proximal gradient descent also recovers natural-gradient method when the parameter space $\Omega_\tau=\mathcal{R}^K$ is unconstrainted.</p>

<p>In Part V, we will show that proximal gradient descent recovers natural-gradient descent for expoential family where $\mathrm{D}(y,\tau_k)$ can also be the exact KL divergence.</p>

<p>In the expoential family cases, we will show in Part V that natural-gradient method becomes mirror descent under the natural-parameterization $\tau$ as long as the parameter space $\Omega_\tau=\mathcal{R}^K$ is unconstrainted.</p>


        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#information-geometry" class="page__taxonomy-item" rel="tag">Information Geometry</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#natural-gradient-descent" class="page__taxonomy-item" rel="tag">Natural Gradient Descent</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#riemannian-manifold" class="page__taxonomy-item" rel="tag">Riemannian Manifold</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-11-15T00:00:00-08:00">November 15, 2021</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title" data-translate="share_on_label">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Part+IV%3A+Natural+and+Riemannian++Gradient+Descent%20informationgeometryML.github.io%2Fposts%2F2021%2F11%2FGeomopt04%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=informationgeometryML.github.io%2Fposts%2F2021%2F11%2FGeomopt04%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=informationgeometryML.github.io%2Fposts%2F2021%2F11%2FGeomopt04%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>



     

  <script src="https://utteranc.es/client.js"
    repo=informationgeometryML/informationgeometryML.github.io
    issue-term=url
    label=blog-comments
    theme=github-light
    crossorigin= "anonymous"
    async>
  </script>





</section>


      
  <nav class="pagination">
    
      <a href="/posts/2021/11/Geomopt03/" class="pagination--pager" title="Part III: Invariance of Natural-Gradients
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/11/Geomopt03/" rel="permalink">Part III: Invariance of Natural-Gradients
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  8 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/10/Geomopt02/" rel="permalink">Part II: Natural-Gradients Evaluted at one Point
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Goal
This blog post should help readers to understand natural-gradients, which are known as Riemannian gradients with the Fisher-Rao metric.
The main propose...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/09/Geomopt01/" rel="permalink">Part I: Manifolds with the Fisher-Rao Metric
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  12 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Goal
This blog post focuses on the Fisher-Rao metric, also known as the Fisher information matrix (FIM). The purpose is to show that,

  The FIM plays an ess...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/07/ICML/" rel="permalink">Structured Natural Gradient Descent (ICML 2021)
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  8 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">About this work [1]: (Youtube) talk, ICML paper, workshop paper,
poster

</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow</strong></li>
    
    
    
    
      <li><a href="https://github.com/informationgeometryML"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Information Geometry in ML. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.2.0/js/all.js"></script>












  
    <script src="/assets/js/custom.js"></script>
  
    <script src="/assets/js/translations.js"></script>
  
    <script src="/assets/js/math-code.js"></script>
  



  </body>
</html>

