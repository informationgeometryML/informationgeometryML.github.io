<!DOCTYPE html>
<!--
  Minimal Mistakes Jekyll Theme 4.12.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
--><html lang="en" class="no-js">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Part I: Smooth Manifolds with the Fisher-Rao Metric - Information Geometry in Machine Learning</title>
<meta name="description" content="GoalThis blog post focuses on the Fisher-Rao metric, which gives rise to the Fisher information matrix (FIM). We will introduce the following  useful concepts to ensure non-singular FIMs:  Regularity conditions and intrinsic parameterizations of a distribution  Dimensionality of a smooth manifold">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Information Geometry in Machine Learning">
<meta property="og:title" content="Part I: Smooth Manifolds with the Fisher-Rao Metric">
<meta property="og:url" content="/posts/2021/09/Geomopt01/">


  <meta property="og:description" content="GoalThis blog post focuses on the Fisher-Rao metric, which gives rise to the Fisher information matrix (FIM). We will introduce the following  useful concepts to ensure non-singular FIMs:  Regularity conditions and intrinsic parameterizations of a distribution  Dimensionality of a smooth manifold">







  <meta property="article:published_time" content="2021-09-06T00:00:00-07:00">





  

  


<link rel="canonical" href="/posts/2021/09/Geomopt01/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Information Geometry in ML",
      "url": "https://github.com/pages/informationgeometryML/informationgeometryML.github.io",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Information Geometry in Machine Learning Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

<!-- end custom head snippets -->

  </head>

  <body class="layout--single mywide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Information Geometry in Machine Learning</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="/">Home</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/news/">News</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/year-archive/">Blog Posts</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  

  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Part I: Smooth Manifolds with the Fisher-Rao Metric">
    <meta itemprop="description" content="GoalThis blog post focuses on the Fisher-Rao metric, which gives rise to the Fisher information matrix (FIM). We will introduce the following  useful concepts to ensure non-singular FIMs:  Regularity conditions and intrinsic parameterizations of a distribution  Dimensionality of a smooth manifold">
    <meta itemprop="datePublished" content="September 06, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Part I: Smooth Manifolds with the Fisher-Rao Metric
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  18 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="goal">Goal</h2>
<p>This blog post focuses on the Fisher-Rao metric, which gives rise to the Fisher information matrix (FIM). We will introduce the following  useful concepts to ensure non-singular FIMs:</p>
<ul>
  <li>Regularity conditions and intrinsic parameterizations of a distribution</li>
  <li>Dimensionality of a smooth manifold</li>
</ul>

<p>The discussion here is informal and focuses on more on intuitions, rather than rigor.</p>

<div class="notice--info">
  <details>
<summary>Click to see how to cite this blog post</summary>
<fieldset class="field-set">

      <div class="language-latex highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>@misc<span class="p">{</span>lin2021NGDblog01,
  title = <span class="p">{</span>Introduction to Natural-gradient Descent: Part I<span class="p">}</span>,
  author = <span class="p">{</span>Lin, Wu and Nielsen, Frank and Khan, Mohammad Emtiyaz and Schmidt, Mark<span class="p">}</span>,
  url = <span class="p">{</span>https://informationgeometryml.github.io/year-archive/<span class="p">}</span>, 
  howpublished = <span class="p">{</span><span class="k">\url</span><span class="p">{</span>https://informationgeometryml.github.io/posts/2021/09/Geomopt01/<span class="p">}}</span>,
  year = <span class="p">{</span>2021<span class="p">}</span>,
  note = <span class="p">{</span>Accessed: 2021-09-06<span class="p">}</span>
<span class="p">}</span>
</code></pre></div>      </div>
    </fieldset>
</details>
</div>

<h1 id="motivation">Motivation</h1>
<hr>
<p>The goal of this blog is to introduce geometric structures associated with probability distribution. Why should we care about such geometric structures?
By exploiting the structures, we can</p>
<ul>
  <li>design efficient and simple algorithms <a class="citation" href="#amari1998natural">[1]</a>
</li>
  <li>design robust methods that are less sensitive to re-parametrization <a class="citation" href="#lin2021tractable">[2]</a>
</li>
  <li>understand the behavior of models/algorithms using tools from differential geometry, information geometry, and invariant theory <a class="citation" href="#liang2019fisher">[3]</a>
</li>
</ul>

<p>These benefits are relevant for the majority of machine learning methods, all of which make use of probability distributions of various kinds.</p>

<p>Below, we give some common examples from the literature. A reader familiar with such examples can skip this part.</p>

<blockquote>
  <p><strong>Empirical Risk Minimization</strong> (frequentist estimation):</p>

  <p>Given N input-output pairs <code class="language-plaintext highlighter-rouge">$(x_i,y_i)$</code>,  the least-square loss can be viewed as a finite-sample approximation of the expectation w.r.t. a probability distribution (data generating distribution),
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{\tau}  \frac{1}{2n} \sum_{i=1}^{n}   (y_i-x_i^T\tau)^2 
 &amp;= - \frac{1}{n}  \sum_{i=1}^{n}  \log \mathcal{N}(y_i | x_i^T\tau,1) + \text{constant}\\
&amp; \approx  E_{ \color{red}  { p(x,y | \tau) } } [ - \log  p(x,y | \tau) ]
\end{aligned} \tag{1}\label{1}
$$</code>
where <code class="language-plaintext highlighter-rouge">$ p(x,y | \tau) = \mathcal{N}(y | x^T\tau,1) p(x) $</code> is assumed to be the data-generating distribution. Here, <code class="language-plaintext highlighter-rouge">$ \mathcal{N} (y | m, v) $</code> denotes a normal distribution over <code class="language-plaintext highlighter-rouge">$ y $</code> with mean <code class="language-plaintext highlighter-rouge">$ m $</code> and variance <code class="language-plaintext highlighter-rouge">$ v $</code>.</p>

  <p>Well-known algorithms such as  <a href="https://en.wikipedia.org/wiki/Scoring_algorithm#Fisher_scoring"><strong>Fisher scoring</strong></a>  and <strong>(empirical) natural-gradient descent</strong> <a class="citation" href="#martens2020new">[4]</a> are commonly used methods that exploit the geometric structure of <code class="language-plaintext highlighter-rouge">$p(x,y | \tau)$</code>. These are examples of algorithms derived from a frequentist perspective, which can also be generalized to neural networks <a class="citation" href="#martens2020new">[4]</a>.</p>
</blockquote>

<blockquote>
  <p><strong>Variational Inference</strong> (Bayesian estimation):</p>

  <p>Given a prior <code class="language-plaintext highlighter-rouge">$ p(z) $</code> and a likelihood <code class="language-plaintext highlighter-rouge">$ p(\mathcal{D} | z ) $</code> over a latent vector <code class="language-plaintext highlighter-rouge">$z$</code> and known data <code class="language-plaintext highlighter-rouge">$ \mathcal{D} $</code>, we can approximate the exact posterior <code class="language-plaintext highlighter-rouge">$ p( z | \mathcal{D} ) =\frac{p(z,\mathcal{D})}{p(\mathcal{D})} $</code> by optimizing a variational objective with respect to  an approximated distribution <code class="language-plaintext highlighter-rouge">$ q(z | \tau) $</code>:
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{\tau} \mathrm{KL} [ { q(z | \tau) || p( z | \mathcal{D} ) }   ] = E_{ \color{red}  {q(z | \tau)} } [ \log q(z | \tau) - \log p( z , \mathcal{D} )    ]  + \text{constant} 
\end{aligned} \tag{2}\label{2}
$$</code>
where <code class="language-plaintext highlighter-rouge">$ \mathrm{KL} [ q(z) ||  p(z) ] :=  E_{ {q(z)} } [ \log \big(\frac{q(z)}{p(z)}\big) ]$</code> is the Kullback–Leibler divergence.</p>

  <p>The <strong>natural-gradient variational inference</strong> <a class="citation" href="#khan2017conjugate">[5]</a> is an algorithm that speeds up the inference by exploiting the geometry of <code class="language-plaintext highlighter-rouge">$q(z|\tau)$</code> induced by the Fisher-Rao metric.
This approach is derived from a Bayesian  perspective, and can also be generalized to neural networks <a class="citation" href="#lin2021structured">[6]</a> and Bayesian neural networks <a class="citation" href="#osawa2019practical">[7]</a>.</p>
</blockquote>

<blockquote>
  <p><strong>Evolution Strategies and Policy-Gradient Methods</strong> (Global optimization):</p>

  <p>Global optimization methods often use a search distribution, denoted by <code class="language-plaintext highlighter-rouge">$ \pi(a | \tau ) $</code>, to find the global maximum of an objective <code class="language-plaintext highlighter-rouge">$h(a)$</code> by solving a problem of the following form:
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{\tau}  E_{ \color{red}  {\pi(a | \tau)} } [ h(a)  ]
\end{aligned} \tag{3}\label{3}
$$</code>
Samples from the search distribution are evaluated through a “fitness” function <code class="language-plaintext highlighter-rouge">$ h(a) $</code>, and guide the optimization towards better optima.</p>

  <p>The <strong>natural evolution strategies</strong> <a class="citation" href="#wierstra2014natural">[8]</a> is an algorithm that speeds up the search process by exploiting the geometry of <code class="language-plaintext highlighter-rouge">$\pi(a|\tau)$</code>.
In the context of reinforcement learning,  <code class="language-plaintext highlighter-rouge">$ \pi(a | \tau ) $</code> is known as the policy distribution to generate actions and the natural evolution strategies is known as the <strong>natural policy gradient</strong> method <a class="citation" href="#kakade2001natural">[9]</a>.</p>
</blockquote>

<p>In all of the examples above, the objective function is expressed in terms of an expectation w.r.t. a distribution in red, parameterized with the parameter <code class="language-plaintext highlighter-rouge">$ \tau $</code>. 
The geometric structure of a distribution <code class="language-plaintext highlighter-rouge">$ p(w|\tau) $</code> for the quantity <code class="language-plaintext highlighter-rouge">$ w $</code> can be exploited to improve the learning algorithms. The table below summarizes the three examples. 
More applications of similar nature are discussed in <a class="citation" href="#le2007topmoumoute">[10]</a> and <a class="citation" href="#duan2020ngboost">[11]</a>.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Example       </th>
      <th style="text-align: center">meaning of <code class="language-plaintext highlighter-rouge">$w$</code>        </th>
      <th style="text-align: right">distribution <code class="language-plaintext highlighter-rouge">$ p(w|\tau) $</code>
</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Empirical Risk Minimization</td>
      <td style="text-align: center">observation <code class="language-plaintext highlighter-rouge">$(x,y)$</code>
</td>
      <td style="text-align: right"><code class="language-plaintext highlighter-rouge">$p(x,y|\tau)$</code></td>
    </tr>
    <tr>
      <td style="text-align: left">Variational Inference</td>
      <td style="text-align: center">latent variable $z$</td>
      <td style="text-align: right"><code class="language-plaintext highlighter-rouge">$q(z|\tau)$</code></td>
    </tr>
    <tr>
      <td style="text-align: left">Evolution Strategies</td>
      <td style="text-align: center">decision variable $a$</td>
      <td style="text-align: right"><code class="language-plaintext highlighter-rouge">$\pi(a|\tau)$</code></td>
    </tr>
  </tbody>
</table>

<div class="notice--success">
  <p><strong>Note</strong>:</p>

  <p>In general, we may have to compute or estimate the inverse of the FIM. However, in many useful machine learning applications, algorithms such as <a class="citation" href="#lin2021tractable">[2]</a>  <a class="citation" href="#martens2020new">[4]</a> <a class="citation" href="#khan2017conjugate">[5]</a> <a class="citation" href="#osawa2019practical">[7]</a>  <a class="citation" href="#wierstra2014natural">[8]</a>  <a class="citation" href="#kakade2001natural">[9]</a>  <a class="citation" href="#le2007topmoumoute">[10]</a> can be efficiently implemented without
explicitly computing the inverse of the FIM.</p>

  <p>We discuss this in other posts. See<br>
<a href="/posts/2021/12/Geomopt05/#efficient-ngd-for-multivariate-gaussians">Part V</a>
and 
<a href="/posts/2021/07/ICML/">our ICML work</a>.</p>
</div>

<p>In the rest of the post, we will mainly focus on the geometric structure of (finite-dimensional) parametric families, for example, a univariate Gaussian family.
The following figure illustrates four distributions in the Gaussian family denoted by
<code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&gt;0 \}$</code>, where <code class="language-plaintext highlighter-rouge">$ p(w | \tau ): = \mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $</code>  and <code class="language-plaintext highlighter-rouge">$\tau :=(\mu,\sigma) $</code>. We will later see that this family is a 2-dimensional manifold in the parameter space.</p>

<p><img src="/img/gauss1d.png" alt="Figure 2" title="Source:Wikipedia"></p>

<h1 id="intrinsic-parameterizations">Intrinsic Parameterizations</h1>
<hr>
<p>We start by discussing a special type of parameterizations, we call intrinsic parameterizations, which are useful to obtain non-singular FIMs.
An arbitrary parameterization may not always be appropriate for a smooth manifold <a class="citation" href="#tu2011introduction">[12]</a>. Rather, the parameterization should be such that the manifold is locally like a <em>flat</em> vector space, for example, how the curved Earth surface looks flat to us, locally.
We will refer to such flat vector space as a <em>local</em> vector-space structure (denote it by <code class="language-plaintext highlighter-rouge">$ E $</code>).</p>

<div class="notice--success">
  <p>Local <strong>vector-space structure</strong>:</p>

  <p>It supports local <strong>vector additions</strong>,  local <strong>real scalar products</strong>, and their algebraic laws (i.e., the distributive law). (see 
<a href="/posts/2021/10/Geomopt02/#riemannian-gradients-as-tangent-vectors-optional">Part II</a>
for the details.)</p>
</div>

<p>Intrinsic parameterizations<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> are those that satisfy the following two conditions:</p>
<ul>
  <li>We require that the parameter space of $\tau$, denoted by $\Omega_\tau$, be an <strong>open</strong> set in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>, where <code class="language-plaintext highlighter-rouge">$K$</code> is the number of entries of a parameter array. Intuitively, this ensures a local vector-space structure throughout the parameter space, which then ensures that a small, local perturbation <code class="language-plaintext highlighter-rouge">$ E $</code> at each point stays within $\Omega_\tau$.</li>
  <li>We also require that <code class="language-plaintext highlighter-rouge">$ E $</code>  <strong>uniquely</strong> and <strong>smoothly</strong> represents  points in a manifold. The condition ensures arbitrary (smooth) parameter transformations should still represent the same sub-set. In other words, we require that
    <ul>
      <li>there exists a <strong>bi-jective</strong> map among such two parameterizations if these parameterizations represent a common sub-set of points in the manifold.</li>
      <li>this map and it inverse map are both <strong>smooth</strong>.</li>
    </ul>

    <p>In differential geometry, this  requirement is known as a diffeomorphism, which is a formal but more abstract definition of this requirement.</p>
  </li>
</ul>

<p>Intrinsic parameterizations satisfy the above two conditions, and lead to non-singular FIMs, as we will see soon.</p>

<p>We will now discuss a simple case of a manifold, a unit circle in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^2$</code>, and give an example of an intrinsic parameterization and three non-intrinsic ones due to different reasons such as non-smoothness, non-openness, and non-uniqueness.</p>

<p><img src="/img/circle.png" title="Source:Wikipedia" width="300"></p>

<blockquote>
  <p>Parameterization 1 (an intrinsic parameterization):</p>

  <p>A (local) parametrization at $ (0,1) $ highlighted in red for the circle is
<code class="language-plaintext highlighter-rouge">$\{ (t,\sqrt{1-t^2}) | -h&lt;t&lt;h \} $</code>, where $h=0.1$. We use <strong>one</strong> (scalar) parameter in this parametrization. The manifold is (locally) “flat” since we can always find a small <strong>1-dimensional</strong> perturbation $E$ in the <strong>1-dimensional</strong> parameter space  <code class="language-plaintext highlighter-rouge">$\Omega_t=\{t|-h&lt;t&lt;h \} $</code>. Therefore, this is an intrinsic parameterization.</p>

  <p><img src="/img/1d-perturbation.png" title="Fig" width="300"></p>

  <p>We can similarly define a (local) parametrization at each point of the circle. In fact, we can use  finite (local) parameterizations to represent the whole circle as shown below.</p>

  <p><img src="/img/charts.png" title="Source:Wikipedia" width="200"></p>
</blockquote>

<p>Now, we discuss invalid cases, where not all conditions are satisfied.</p>

<blockquote>
  <p>Parameterization 2 (a non-intrinsic parameterization due to non-smoothness):</p>

  <p>Let’s define a map <code class="language-plaintext highlighter-rouge">$f : [0,2\pi) \rightarrow \mathcal{S}^1 $</code> such that <code class="language-plaintext highlighter-rouge">$f(\theta) = (\sin \theta, \cos \theta ) $</code>, where we use $\mathcal{S}^1$ to denote the circle.</p>

  <p>A (global) parametrization of the circle is <code class="language-plaintext highlighter-rouge">$\{ f(\theta) | \theta \in [0,2\pi)  \}$</code>, where we use one (scalar) parameter.</p>

  <p>This map $f$ is bijective and smooth. However, the parameter space is not open in $\mathcal{R}$, and its inverse map $f^{-1}$ is <strong>not</strong> continunous at point $(0,1) \in  \mathcal{S}^1$. Therefore, this parametrization is not intrinsic.
In fact, there does not exist a (single) <strong>global</strong> and intrinsic parametrization to represent the whole circle.</p>
</blockquote>

<p>Smoothness of the inverse map is essential when it comes to reparametrization (A.K.A. parameter transformation). The smoothness, along with the inverse map, gives us a way to generate new intrinsic parameterizations. Essentially, in such case, the Jacobian matrix (to change between the parameterizations) is non-singular everywhere, and we can use the chain rule and inverse function theorem to jump between different intrinsic parameterizations.
We will discuss this in  <a href="/posts/2021/11/Geomopt03/#parameter-transformation-and-invariance">Part III</a>.</p>

<blockquote>
  <p>Parametrization 3 (a non-intrinsic parameterization due to non-openness):</p>

  <p>The circle does <strong>not</strong> look like a flat space under the following parametrization
<code class="language-plaintext highlighter-rouge">$\{ (x,y) | x^2+y^2=1, x,y \in \mathcal{R} \} $</code>. The number of entries in this parameter array is 2.</p>

  <p>The reason is that we cannot find a small <strong>2-dimensional</strong> perturbation $E$ in the <strong>2-dimensional</strong> parameter space <code class="language-plaintext highlighter-rouge">$\Omega_{\tau}=\{(x,y)|x^2+y^2=1 \} $</code> due to the constraint $x^2+y^2=1$. In other words, $\Omega_\tau$ is not open in $\mathcal{R}^2$.</p>

  <p><img src="/img/2d-perturbation.png" title="Fig" width="300"></p>
</blockquote>

<blockquote>
  <p>Parametrization 4 (a non-intrinsic parameterization due to non-uniqueness):</p>

  <p>Let’s consider the following non-intrinsic parametrization $\tau$ of the circle: <code class="language-plaintext highlighter-rouge">$\{ (\frac{x}{\sqrt{x^2+y^2}}, \frac{y}{\sqrt{x^2+y^2}}) | x^2+y^2 \neq 0, x,y \in \mathcal{R}  \}$</code>, where <code class="language-plaintext highlighter-rouge">$\tau=(x,y)$</code>. The parameter space $\Omega_\tau$ is open in $\mathcal{R}^2$.</p>

  <p>This parametrization is not intrinsic since it does not uniquely represent a point in the circle. It is obvious to see that  $\tau_1=(x_1,y_1)$ and $\alpha \tau_1=(\alpha x_1,\alpha y_1)$ both represent the same point in the circle when scalar $\alpha&gt;0$.</p>

</blockquote>

<h1 id="intrinsic-parameterizations-for-parametric-families">Intrinsic Parameterizations for Parametric families</h1>
<hr>
<p>The examples in the previous section clearly show the importance of parameterization, and that it should be chosen carefully. Now, we discuss how to choose such a parameterization for a given parametric family.</p>

<p>Roughly speaking, a parameterization <code class="language-plaintext highlighter-rouge">$ \tau $</code> for a family of distribution <code class="language-plaintext highlighter-rouge">$ p(w|\tau) $</code> is intrinsic if <code class="language-plaintext highlighter-rouge">$\log
p(w|\tau) $</code> is both smooth and unique w.r.t. <code class="language-plaintext highlighter-rouge">$ \tau $</code> in its parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\tau$</code>.
Below is the formal condition.</p>

<div class="notice--success">
  <p><strong>Regularity Condition</strong>:</p>

  <p>For any <code class="language-plaintext highlighter-rouge">$\tau \in \Omega_\tau$</code>,  the set of partial derivatives 
<code class="language-plaintext highlighter-rouge">$ \{ \partial_{\tau_i} \log p(w|\tau) \} $</code> is smooth w.r.t. <code class="language-plaintext highlighter-rouge">$\tau$</code> and is a set of linearly independent functions of <code class="language-plaintext highlighter-rouge">$w$</code>.</p>

  <p>In other words, <code class="language-plaintext highlighter-rouge">$\sum_i c_i [ \partial_{\tau_i} \log p(w|\tau)] = 0 $</code> holds only when constant <code class="language-plaintext highlighter-rouge">$c_i$</code> is zero and the value of <code class="language-plaintext highlighter-rouge">$c_i$</code> does not depend on  <code class="language-plaintext highlighter-rouge">$w$</code>.</p>
</div>

<p>Note that, due to the definition of the partial derivatives, this regularity condition implicitly assumes that the parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\tau$</code> is an open set in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>, where K is the number of entries in parameter array <code class="language-plaintext highlighter-rouge">$\tau$</code>.
In other words, the openness requirement is not explicit and hidden within the regularity condition.
We will discuss more about this at <a href="#caveats-of-the-fisher-matrix-computation">here</a>.</p>

<p>The following examples illustrate the regularity condition.</p>

<blockquote>
  <p>Example 1 (regularity condition for an intrinsic parameterization):</p>

  <p>We will write the regularity condition at a point for an intrinsic parameterization. Consider a 1-dimensional Gaussian family <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(w |\mu,v) \Big| \mu \in \mathcal{R}, v&gt;0 \}$</code> with mean <code class="language-plaintext highlighter-rouge">$\mu$</code>, variance <code class="language-plaintext highlighter-rouge">$v$</code>, and parametrization <code class="language-plaintext highlighter-rouge">$\tau = (\mu,v) $</code>.
The partial derivatives are the following,
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\partial_{\mu} \log \mathcal{N}(w |\mu,v) = \frac{w-\mu}{v}, \,\,\, \partial_{v} \log \mathcal{N}(w |\mu,v) = \frac{ (w-\mu)^2 }{2 v^2} - \frac{1}{2 v} 
\end{aligned}
$$</code> 
It is easy to see that these partial derivatives are smooth w.r.t. $\tau=(\mu,v)$ in its parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\tau=\{(\mu,v)|\mu\in \mathcal{R}, v&gt;0\}$</code>.</p>

  <p>Consider the partial derivatives at a point $(\mu=0, v=1)$,</p>

  <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\partial_{\mu} \log \mathcal{N}(w |\mu,v) \Big|_{\mu=0,v=1}= w, \,\,\, \partial_{v} \log \mathcal{N}(w |\mu,v) \Big|_{\mu=0,v=1} = \frac{ w^2 -1 }{2}
\end{aligned}
$$</code>
For this point, the regularity condition will be <code class="language-plaintext highlighter-rouge">$ c_1 w + c_2  (\frac{ w^2 -1 }{2}) = 0$</code>.
For this to hold for all $w$, it is necessarily that <code class="language-plaintext highlighter-rouge">$c_1=c_2=0$</code>, which implies linear independence.</p>

  <p>A formal proof can be built to show that this holds for any <code class="language-plaintext highlighter-rouge">$\mu \in \mathcal{R}$</code> and <code class="language-plaintext highlighter-rouge">$v &gt;0$</code>.</p>
</blockquote>

<blockquote>
  <p>Example 2 (regularity condition for a non-intrinsic parameterization):</p>

  <p>By using a counterexample, we will show that the regularity condition fails for a non-intrinsic parameterization. Consider a Bernoulli family <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0&gt;0, \pi_1&gt;0  \}$</code> with parameter <code class="language-plaintext highlighter-rouge">$\tau = (\pi_0,\pi_1) $</code>, where function <code class="language-plaintext highlighter-rouge">$ \mathcal{I}(\cdot) $</code> is the indicator function.
The partial derivatives are</p>

  <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\partial_{\pi_0} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{\pi_1}{(\pi_0+\pi_1)^2}   
\end{aligned}
$$</code>
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\partial_{\pi_1} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} 
\end{aligned}
$$</code>
Note that when <code class="language-plaintext highlighter-rouge">$c_0 = \pi_0 \neq 0 $</code> and <code class="language-plaintext highlighter-rouge">$c_1= \pi_1 \neq 0$</code>, we have <code class="language-plaintext highlighter-rouge">$c_0 \frac{\pi_1}{(\pi_0+\pi_1)^2}  + c_1 \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} = 0$</code>. 
Therefore, the partial derivatives are linearly dependent.</p>
</blockquote>

<p>In a similar fashion, we will also see (soon) that the regularity condition is also not satisfied for the following parameterization: <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0&gt;0, \pi_1&gt;0, \pi_0+\pi_1=1  \}$</code> with parameter <code class="language-plaintext highlighter-rouge">$\tau = (\pi_0,\pi_1)$</code>. The main reason is that the parameter space is not open in $\mathcal{R}^2$.</p>

<p>On the other hand, the condition holds for the following parameterization: <code class="language-plaintext highlighter-rouge">$\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) (1-\pi_0)  \Big| 0&lt;\pi_0&lt;1  \}$</code> with parameter <code class="language-plaintext highlighter-rouge">$\tau = \pi_0$</code>.</p>

<h1 id="fisher-rao-metric">Fisher-Rao Metric</h1>
<hr>

<p>Given an intrinsic parameterization, the Fisher-Rao metric is defined as follows,
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log p(w|\tau) \Big) ].
\end{aligned}
$$</code></p>

<p>We can also express the metric in a matrix form as</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{F}(\tau) := E_{ p(w|\tau) }  [ \Big( \nabla_{\tau} \log p(w|\tau ) \Big)  \Big(\nabla_{\tau} \log p(w|\tau) \Big)^T ],
\end{aligned}
$$</code>
where $K$ is the number of entries of parameter array $\tau$ and 
<code class="language-plaintext highlighter-rouge">$ \nabla_{\tau} \log p(w|\tau ) := [ \partial_{\tau_1} \log p(w|\tau ), \cdots, \partial_{\tau_K} \log p(w|\tau ) ]^T  $</code> is a column vector. The matrix form is also known as the <strong>Fisher information matrix</strong> (FIM). Obviously, the form of the FIM depends on the choice of parameterizations. In many cases,  we could also compute the FIM as
<code class="language-plaintext highlighter-rouge">$ \mathbf{F}(\tau) := - E_{p(w|\tau) }  [  \nabla_{\tau}^2 \log p(w|\tau )  ]$</code>.
The regularity condition guarantees that the FIM is non-singular if the matrix exists, that is, the expectation in the definition exists.</p>

<p>In what follows, we will assume the metric to be well-defined, which makes the Fisher-Rao metric a valid Riemannian metric <a class="citation" href="#lee2018introduction">[13]</a> since the corresponding FIM is positive definite everywhere in its intrinsic parameter space.</p>

<h1 id="caveats-of-the-fisher-matrix-computation">Caveats of the Fisher matrix computation</h1>
<hr>
<p>There are some caveats when it comes to the Fisher matrix computation. In particular, the regularity condition should be satisfied.
It is possible to define the FIM under a non-intrinsic parameterization. However, the FIM often is singular or ill-defined under a non-intrinsic  parameterization as shown below.</p>

<blockquote>
  <p>Example 1 (Ill-defined FIM):</p>

  <p>Consider Bernoulli family  <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0&gt;0, \pi_1&gt;0, \pi_0+\pi_1=1  \}$</code> with non-intrinsic parameter <code class="language-plaintext highlighter-rouge">$\tau = (\pi_0,\pi_1)$</code>.
The following computation is not correct. Do you make similar mistakes like this?</p>

  <p>Let <code class="language-plaintext highlighter-rouge">$  p(w|\tau ) =  \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1$</code>, where <code class="language-plaintext highlighter-rouge">$\tau = (\pi_0,\pi_1)$</code>. The derivative is
<code class="language-plaintext highlighter-rouge">$$  \begin{aligned} \nabla_{\tau} \log p(w|\tau ) = \frac{1}{p(w|\tau )} [\mathcal{I}(w=0) ,  \mathcal{I}(w=1)]^T  \end{aligned} \tag{4}\label{4}$$</code>
Thus, by Eq. <code class="language-plaintext highlighter-rouge">$\eqref{4}$</code>, the FIM under this  parameterization is</p>

  <p><code class="language-plaintext highlighter-rouge">$$  \begin{aligned} F(\tau) = E_{p(w|\tau) } [ \frac{1}{p^2(w|\tau)} \begin{bmatrix} \mathcal{I}^2(w=0) &amp;  \mathcal{I}(w=1)  \mathcal{I}(w=0) \\  \mathcal{I}(w=0)  \mathcal{I}(w=1) &amp;  \mathcal{I}^2(w=1) \end{bmatrix} ]   = \begin{bmatrix} \frac{1}{\pi_0} &amp;  0 \\ 0 &amp;  \frac{1}{\pi_1} \end{bmatrix} \end{aligned}$$</code></p>
  <div class="notice--danger">
    <p>This computation is not correct. Do you know why?</p>
  </div>

  <div class="notice--info">
    <details>
<summary>Reason: (Click to expand)</summary>
<fieldset class="field-set">
        <p>The key reason is that the parameter space is not open in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^2$</code> due to the equality constraint <code class="language-plaintext highlighter-rouge">$ \pi_0+\pi_1=1 $</code>. Thus, Eq. <code class="language-plaintext highlighter-rouge">$\eqref{4}$</code> is <strong>incorrect</strong>.</p>

        <p>By definition, a Bernoulli distribution is valid only when the constraint holds. 
Thus, the constraint <code class="language-plaintext highlighter-rouge">$ \pi_0+\pi_1=1 $</code> must be satisfied when we compute the Fisher matrix since the computation involves computing the expectation w.r.t. this distribution.</p>

        <p>Note that the gradient is defined as <code class="language-plaintext highlighter-rouge">$ \nabla_{\tau} \log p(w|\tau ) := [ \partial_{\pi_0} \log p(w|\tau ), \partial_{\pi_1} \log p(w|\tau ) ]^T $</code>.</p>

        <p>Unfortunately, these partial derivatives do not exist. By the definition of  partial derivative <code class="language-plaintext highlighter-rouge">$ \partial_{\pi_0} \log p(w|\tau )$</code>, we fix <code class="language-plaintext highlighter-rouge">$\pi_1$</code> and allow <code class="language-plaintext highlighter-rouge">$\pi_0$</code> to change.
However, given that <code class="language-plaintext highlighter-rouge">$\pi_1$</code> is fixed and <code class="language-plaintext highlighter-rouge">$ \pi_0 $</code> is fully determined by <code class="language-plaintext highlighter-rouge">$\pi_1$</code> due to the equality constraint <code class="language-plaintext highlighter-rouge">$ \pi_0+\pi_1=1 $</code>. 
Therefore, <code class="language-plaintext highlighter-rouge">$  \partial_{\pi_0} \log p(w|\tau ) $</code> is not well-defined.
In other words, the above Fisher matrix computation is not correct since <code class="language-plaintext highlighter-rouge">$ \nabla_{\tau} \log p(w|\tau ) $</code> does not exist.</p>
      </fieldset>
</details>
  </div>
</blockquote>

<blockquote>
  <p>Example 2  (Singular FIM):</p>

  <p>Consider Bernoulli family <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0&gt;0, \pi_1&gt;0  \}$</code> with  non-intrinsic parameter <code class="language-plaintext highlighter-rouge">$\tau = (\pi_0,\pi_1) $</code>.</p>

  <p>Note that a Bernoulli distribution in the family is not uniquely represented by this parametrization. It is obvious to
see that $\tau_1 = (1,1)$ and $\tau_2=(2,2)$ both represent the same Bernoulli distribution.</p>

  <p>The FIM under this  parameterization is singular as shown below.</p>

  <p>Let <code class="language-plaintext highlighter-rouge">$  p(w|\tau ) =  \mathcal{I}(w=0)  \frac{\pi_0}{\pi_0+\pi_1}+ \mathcal{I}(w=1)  \frac{\pi_1}{\pi_0+\pi_1}$</code>, where <code class="language-plaintext highlighter-rouge">$\tau = (\pi_0,\pi_1)$</code>. The derivative is</p>

  <p><code class="language-plaintext highlighter-rouge">$$ 
\begin{aligned}
\nabla_{\tau} \log p(w|\tau ) = \frac{ \mathcal{I}(w=0) - \mathcal{I}(w=1)  }{p(w|\tau )} [\frac{\pi_1}{(\pi_0+\pi_1)^2} , \frac{-\pi_0}{(\pi_0+\pi_1)^2}]^T 
\end{aligned}
$$</code></p>

  <p>Thus, the FIM under this  parameterization is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned} 
F(\tau) = E_{p(w|\tau) } [ \frac{( \mathcal{I}(w=0) - \mathcal{I}(w=1))^2 }{p^2(w|\tau)} \begin{bmatrix} \frac{\pi_1^2}{(\pi_0+\pi_1)^4}  &amp; \frac{-\pi_0 \pi_1}{(\pi_0+\pi_1)^4} \\ \frac{-\pi_0 \pi_1}{(\pi_0+\pi_1)^4}  &amp;  \frac{\pi_0^2}{(\pi_0+\pi_1)^4} \end{bmatrix} ]   =\frac{1}{(\pi_0+\pi_1)^2} \begin{bmatrix} \frac{\pi_1}{\pi_0} &amp;  -1 \\ -1 &amp;  \frac{\pi_0}{\pi_1} \end{bmatrix}
\end{aligned}
$$</code>
where this FIM is singular since the matrix determinant is zero as shown below. <code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{det}\big( \begin{bmatrix} \frac{\pi_1}{\pi_0} &amp;  -1 \\ -1 &amp;  \frac{\pi_0}{\pi_1} \end{bmatrix}\big) = 0.
\end{aligned}
$$</code></p>
</blockquote>

<p>Now, we give an example to show that the FIM of a Bernoulli family can be non-singular when we use an intrinsic parameterization.</p>

<blockquote>
  <p>Example 3  (Non-singular FIM):</p>

  <p>Consider Bernoulli family <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi)  \Big| 0&lt;\pi&lt;1 \}$</code> with  <strong>intrinsic</strong> parameter <code class="language-plaintext highlighter-rouge">$\tau = \pi $</code>.</p>

  <p>The FIM under this parameterization is non-singular as shown below.</p>

  <p>Let <code class="language-plaintext highlighter-rouge">$  p(w|\tau ) =  \mathcal{I}(w=0) \pi+ \mathcal{I}(w=1) (1-\pi)$</code>, where <code class="language-plaintext highlighter-rouge">$\tau = \pi$</code>. The derivative is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned} \nabla_{\tau} \log p(w|\tau ) = \frac{\mathcal{I}(w=0) - \mathcal{I}(w=1)}{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) }  \end{aligned}
$$</code></p>

  <p>Thus, the FIM under this  parameterization is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
F(\tau) &amp;= E_{p(w|\tau) } [ \frac{ (\mathcal{I}(w=0) - \mathcal{I}(w=1) )^2 }{ (\mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) )^2 }  ]\\
&amp; = \pi \frac{1^2}{ \pi^2} + (1-\pi) \frac{(-1)^2}{(1-\pi)^2} \\
&amp;= \frac{1}{\pi } + \frac{1}{1-\pi} = \frac{1}{\pi(1-\pi)}&gt; 0
\end{aligned}
$$</code></p>

</blockquote>

<h1 id="dimensionality-of-a-manifold">Dimensionality of a manifold</h1>
<hr>
<p>We can define the dimension of a manifold by using the  degrees of freedom of an intrinsic parametrization.
Due to the theorem of <a href="https://ncatlab.org/nlab/show/topological+invariance+of+dimension">toplological invariance of dimension</a>, any intrinsic parametrization of a manifold has the same degrees of freedom <a class="citation" href="#tu2011introduction">[12]</a>.
This also gives us a tool to  identify non-manifold cases.
We now illustrate this by examples.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">unit circle</th>
      <th style="text-align: center">open unit ball</th>
      <th style="text-align: center">closed unit ball</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/img/circle-org.png" alt="Source:Wikipedia" width="200"></td>
      <td style="text-align: center"><img src="/img/open-ball.png" alt="Source:Wikipedia" width="200"></td>
      <td style="text-align: center"><img src="/img/closed-ball.png" alt="Source:Wikipedia" width="200"></td>
    </tr>
    <tr>
      <td style="text-align: center">1-dim manifold</td>
      <td style="text-align: center">2-dim manifold</td>
      <td style="text-align: center">non-manifold, which is indeed a manifold with (closed) boundary</td>
    </tr>
  </tbody>
</table>

<p>As we shown in <a href="#intrinsic-parameterizations">the previous section</a>, a unit circle is a 1-dimensional manifold.  We can similarly show that an open unit ball is a 2-dimensional manifold.</p>

<p>However, a closed
unit ball is NOT a manifold since its interior is an open unit ball and its boundary is a unit circle.
The  circle and the open unit ball do not have the same dimensionality.</p>

<p>For statistical  manifolds, 
 consider the following examples. We will discuss more about them in 
 <a href="/posts/2021/10/Geomopt02/#riemannian-steepest-direction">Part II</a>.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">1-dim Gaussian with zero mean</th>
      <th style="text-align: center">$d$-dim Gaussian with zero mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">
<code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(w |0,s^{-1}) \Big|  s&gt;0 \}$</code> with precision <code class="language-plaintext highlighter-rouge">$s$</code> <br> under intrinsic parameterization <code class="language-plaintext highlighter-rouge">$\tau = s $</code>
</td>
      <td style="text-align: center">
<code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(\mathbf{w} |\mathbf{0},\mathbf{S}^{-1}) \Big| \mathrm{MatH}(\tau) = \mathbf{S}   \succ \mathbf{0} \}$</code> with precision <code class="language-plaintext highlighter-rouge">$\mathbf{S}$</code>  <br> under intrinsic parameterization <code class="language-plaintext highlighter-rouge">$\tau = \mathrm{vech}(\mathbf{S})$</code>.</td>
    </tr>
    <tr>
      <td style="text-align: center">1-dim statistical manifold</td>
      <td style="text-align: center">
<code class="language-plaintext highlighter-rouge">$\frac{d(d+1)}{2}$</code>-dim statistical  manifold</td>
    </tr>
  </tbody>
</table>

<p>We  use 
the <a href="https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization">half-vectorization map</a> <code class="language-plaintext highlighter-rouge">$\mathrm{vech}()$</code>.
The map <code class="language-plaintext highlighter-rouge">$\mathrm{vech}(\mathbf{S})$</code>  returns  a <code class="language-plaintext highlighter-rouge">$\frac{d(d + 1)}{2}$</code>-dim array obtained by vectorizing only the lower triangular part of a (symmetric) <code class="language-plaintext highlighter-rouge">$d$</code>-by-<code class="language-plaintext highlighter-rouge">$d$</code> matrix <code class="language-plaintext highlighter-rouge">$\mathbf{S}$</code>.
The map <code class="language-plaintext highlighter-rouge">$\mathrm{MatH}()$</code> is the inverse map of  <code class="language-plaintext highlighter-rouge">$\mathrm{vech}()$</code>.</p>

<div class="notice--info">
  <details>
<summary>Illustration of these two maps (click to expand)</summary>
<fieldset class="field-set">
      <blockquote>
        <p>Consider the following symmetric 2-by-2 matrix</p>

        <p><code class="language-plaintext highlighter-rouge">$$
\mathbf{S} = 
\begin{aligned}
\begin{bmatrix} 2 &amp;  -1 \\  -1  &amp; 3  \end{bmatrix}
\end{aligned}
$$</code>
The output of map <code class="language-plaintext highlighter-rouge">$\mathrm{vech}(\mathbf{S})$</code> is
<code class="language-plaintext highlighter-rouge">$$
\mathbf{v}:=\mathrm{vech}(\mathbf{S}) = 
\begin{aligned}
\begin{bmatrix} 2  \\  -1  \\ 3  \end{bmatrix}
\end{aligned}
$$</code></p>

        <p>The output of  map <code class="language-plaintext highlighter-rouge">$\mathrm{MatH}(\mathbf{v})$</code> is
<code class="language-plaintext highlighter-rouge">$$
\mathrm{MatH}(\mathbf{v}) = 
\begin{aligned}
\begin{bmatrix} 2 &amp;  -1 \\  -1  &amp; 3  \end{bmatrix}
\end{aligned}
$$</code></p>
      </blockquote>
    </fieldset>
</details>
</div>

<hr>
<h1 id="references">References</h1>
<p class="bibliography"></p>
<p><span id="amari1998natural">[1] S.-I. Amari, "Natural gradient works efficiently in learning," <i>Neural computation</i> <b>10</b>:251–276 (1998).</span></p>
<p><span id="lin2021tractable">[2] W. Lin, F. Nielsen, M. E. Khan, &amp; M. Schmidt, "Tractable structured natural gradient descent using local parameterizations," <i>International Conference on Machine Learning (ICML)</i> (2021).</span></p>
<p><span id="liang2019fisher">[3] T. Liang, T. Poggio, A. Rakhlin, &amp; J. Stokes, "Fisher-rao metric, geometry, and complexity of neural networks," <i>The 22nd International Conference on Artificial Intelligence and Statistics</i> (PMLR, 2019), pp. 888–896.</span></p>
<p><span id="martens2020new">[4] J. Martens, "New Insights and Perspectives on the Natural Gradient Method," <i>Journal of Machine Learning Research</i> <b>21</b>:1–76 (2020).</span></p>
<p><span id="khan2017conjugate">[5] M. Khan &amp; W. Lin, "Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models," <i>Artificial Intelligence and Statistics</i> (PMLR, 2017), pp. 878–887.</span></p>
<p><span id="lin2021structured">[6] W. Lin, F. Nielsen, M. E. Khan, &amp; M. Schmidt, "Structured second-order methods via natural gradient descent," <i>arXiv preprint arXiv:2107.10884</i> (2021).</span></p>
<p><span id="osawa2019practical">[7] K. Osawa, S. Swaroop, A. Jain, R. Eschenhagen, R. E. Turner, R. Yokota, &amp; M. E. Khan, "Practical deep learning with Bayesian principles," <i>Proceedings of the 33rd International Conference on Neural Information Processing Systems</i> (2019), pp. 4287–4299.</span></p>
<p><span id="wierstra2014natural">[8] D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters, &amp; J. Schmidhuber, "Natural evolution strategies," <i>The Journal of Machine Learning Research</i> <b>15</b>:949–980 (2014).</span></p>
<p><span id="kakade2001natural">[9] S. M. Kakade, "A natural policy gradient," <i>Advances in neural information processing systems</i> <b>14</b> (2001).</span></p>
<p><span id="le2007topmoumoute">[10] N. Le Roux, P.-A. Manzagol, &amp; Y. Bengio, "Topmoumoute Online Natural Gradient Algorithm.," <i>NIPS</i> (Citeseer, 2007), pp. 849–856.</span></p>
<p><span id="duan2020ngboost">[11] T. Duan, A. Anand, D. Y. Ding, K. K. Thai, S. Basu, A. Ng, &amp; A. Schuler, "Ngboost: Natural gradient boosting for probabilistic prediction," <i>International Conference on Machine Learning</i> (PMLR, 2020), pp. 2690–2700.</span></p>
<p><span id="tu2011introduction">[12] L. W. Tu, "An introduction to manifolds. Second," <i>New York, US: Springer</i> (2011).</span></p>
<p><span id="lee2018introduction">[13] J. M. Lee, <i>Introduction to Riemannian manifolds</i> (Springer, 2018).</span></p>

<h2 id="footnotes">Footnotes:</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In differential geometry, an intrinsic parametrization is known as a coordinate chart. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#information-geometry" class="page__taxonomy-item" rel="tag">Information Geometry</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#natural-gradient-descent" class="page__taxonomy-item" rel="tag">Natural Gradient Descent</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#riemannian-manifold" class="page__taxonomy-item" rel="tag">Riemannian Manifold</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-09-06T00:00:00-07:00">September 06, 2021</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title" data-translate="share_on_label">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Part+I%3A+Smooth+Manifolds+with+the+Fisher-Rao+Metric%20informationgeometryML.github.io%2Fposts%2F2021%2F09%2FGeomopt01%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=informationgeometryML.github.io%2Fposts%2F2021%2F09%2FGeomopt01%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=informationgeometryML.github.io%2Fposts%2F2021%2F09%2FGeomopt01%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>



     

  <script src="https://utteranc.es/client.js" repo="informationgeometryML/informationgeometryML.github.io" issue-term="url" label="blog-comments" theme="github-light" crossorigin="anonymous" async>
  </script>





</section>


      
  <nav class="pagination">
    
      <a href="/posts/2021/07/ICML/" class="pagination--pager" title="Structured Natural Gradient Descent (ICML 2021)
">Previous</a>
    
    
      <a href="/posts/2021/10/Geomopt02/" class="pagination--pager" title="Part II: Derivation of Natural-gradients
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/12/Geomopt06/" rel="permalink">Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (Part VI is incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/12/Geomopt05/" rel="permalink">Part V: Efficient Natural-gradient Methods for Exponential Family
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  19 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/11/Geomopt04/" rel="permalink">Part IV: Natural and Riemannian  Gradient Descent
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  19 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/11/Geomopt03/" rel="permalink">Part III: Invariance of Natural-Gradients
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  13 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Goal
This blog post focuses on invariant properties of natural-gradients.
We will discuss

  transformation rules of natural-gradients,
  the automatic compu...</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow</strong></li>
    
    
    
    
      <li><a href="https://github.com/informationgeometryML"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">© 2022 Information Geometry in ML. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.2.0/js/all.js"></script>












  
    <script src="/assets/js/custom.js"></script>
  
    <script src="/assets/js/math-code.js"></script>
  



  </body>
</html>
