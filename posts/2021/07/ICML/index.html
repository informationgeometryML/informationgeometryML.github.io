<!DOCTYPE html>
<!--
  Minimal Mistakes Jekyll Theme 4.12.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
--><html lang="en" class="no-js">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Structured Natural Gradient Descent (ICML 2021) - Information Geometry in Machine Learning</title>
<meta name="description" content="More about this work [1]: Long talk,  (Youtube) talk, extended paper, short paper,poster">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Information Geometry in Machine Learning">
<meta property="og:title" content="Structured Natural Gradient Descent (ICML 2021)">
<meta property="og:url" content="/posts/2021/07/ICML/">


  <meta property="og:description" content="More about this work [1]: Long talk,  (Youtube) talk, extended paper, short paper,poster">







  <meta property="article:published_time" content="2021-07-05T00:00:00-07:00">





  

  


<link rel="canonical" href="/posts/2021/07/ICML/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Information Geometry in ML",
      "url": "https://github.com/pages/informationgeometryML/informationgeometryML.github.io",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Information Geometry in Machine Learning Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

<!-- end custom head snippets -->

  </head>

  <body class="layout--single mywide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Information Geometry in Machine Learning</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="/">Home</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/news/">News</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/year-archive/">Blog Posts</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  

  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Structured Natural Gradient Descent (ICML 2021)">
    <meta itemprop="description" content="More about this work [1]: Long talk,  (Youtube) talk, extended paper, short paper,poster">
    <meta itemprop="datePublished" content="July 05, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Structured Natural Gradient Descent (ICML 2021)
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  12 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>More about this work <a class="citation" href="#lin2021tractable">[1]</a>: <a href="https://download.dsf.tuhh.de/ig4ds22/videos/IG4DS-WuLin.mp4">Long talk</a>,  <a href="https://www.youtube.com/watch?v=vEY1ZxDJX8o&t=11s">(Youtube) talk</a>, <a href="https://arxiv.org/abs/2102.07405">extended paper</a>, <a href="https://arxiv.org/abs/2107.10884">short paper</a>,
<a href="/img/poster.pdf">poster</a></p>

<h1 id="introduction">Introduction</h1>

<h2 id="motivation">Motivation</h2>

<p>Many problems in optimization, search, and inference can be solved via natural-gradient descent (NGD)</p>

<p>Structures play an essential role in</p>
<ul>
  <li>Preconditioners of first-order and second-order optimization, gradient-free search.</li>
  <li>Covariance matrices of variational Gaussian inference <a class="citation" href="#opper2009variational">[2]</a>
</li>
</ul>

<p>Natural-gradient descent on structured parameter spaces is computationally challenging.</p>

<p>Limitations of existing NGD methods:</p>
<ul>
  <li>Limited structures due to the complicated Fisher information matrix (FIM)</li>
  <li>Ad-hoc approximations for handling the singular FIM and cost reductions</li>
  <li>Inefficient and complicated natural-gradient computation</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Existing approach for rank-one covariance</th>
      <th style="text-align: center">Our NGD for rank-one covariance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/img/icml2021-fig02.png" width="465"></td>
      <td style="text-align: center"><img src="/img/icml2021-fig04.png" width="495"></td>
    </tr>
  </tbody>
</table>

<h2 id="our-contributions">Our Contributions</h2>

<p>We propose a flexible and efficient NGD method to incorporate structures via matrix Lie groups.</p>

<p>Our NGD method</p>
<ul>
  <li>generalizes the exponential natural evolutionary strategy <a class="citation" href="#glasmachers2010exponential">[3]</a>
</li>
  <li>recovers existing  Newton-like algorithms</li>
  <li>yields new structured 2nd-order methods and adaptive-gradient methods with group-structural invariance <a class="citation" href="#lin2021structured">[4]</a>
</li>
  <li>gives new NGD updates to learn structured covariances of Gaussian, Wishart and their mixtures</li>
  <li>is a systematic approach to incorporate a range of structures</li>
</ul>

<p><a href="#applications">Applications</a> of our method:</p>
<ul>
  <li>deep learning (structured adaptive-gradient),</li>
  <li>non-convex optimization (structured 2nd-order),</li>
  <li>evolution strategies (structured gradient-free),</li>
  <li>variational mixture of Gaussians (Monte Carlo gradients for structured covariance).</li>
</ul>

<hr>
<h1 id="ngd-for-optimization-inference-and-search">NGD for Optimization, Inference, and Search</h1>

<p>A unified  view for problems in optimization, inference, and search
as optimization over  (variational) parametric family <code class="language-plaintext highlighter-rouge">$q(w|\tau)$</code>:
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
   \min_{ \tau \in \Omega_\tau } \mathcal{L}(\tau):= \mathrm{E}_{q(\text{w}| \tau )} \big[ \ell(\mathbf{w}) \big] + \gamma \mathrm{E}_{q(\text{w} |\tau )} \big[ \log q(w|\tau) \big] 
\end{aligned} \tag{1}\label{1}
$$</code>
where <code class="language-plaintext highlighter-rouge">$\mathbf{w}$</code> is the decision variable,  <code class="language-plaintext highlighter-rouge">$\ell(\mathbf{w})$</code> is a loss function, <code class="language-plaintext highlighter-rouge">$\Omega_\tau$</code> is the parameter space of <code class="language-plaintext highlighter-rouge">$q$</code>, and <code class="language-plaintext highlighter-rouge">$\gamma\ge 0$</code> is a constant.</p>

<p>Using gradient descent and natural-gradient descent to solve <code class="language-plaintext highlighter-rouge">$\eqref{1}$</code>:
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\textrm{GD: } &amp;\tau_{t+1} \leftarrow \tau_t - \alpha \nabla_{\tau_t} \mathcal{L}(\tau) \\
\textrm{Standard NGD: } &amp; \tau_{t+1} \leftarrow \tau_t - \beta\,\, \big[ \mathbf{F}_{\tau} (\tau_t) \big]^{-1} \nabla_{\tau_t} \mathcal{L}(\tau)
\end{aligned} 
$$</code>
where <code class="language-plaintext highlighter-rouge">$\mathbf{F}_{\tau} (\tau_t)$</code> is the FIM of distribution <code class="language-plaintext highlighter-rouge">$q(w|\tau)$</code> at <code class="language-plaintext highlighter-rouge">$\tau=\tau_t$</code>.
For an introduction to natural-gradient methods, see this <a href="/posts/2021/09/Geomopt01/">blog</a>.</p>

<p>Advantages of NGD:</p>
<ul>
  <li>recovers a Newton-like update for Gaussian family <code class="language-plaintext highlighter-rouge">$q(\mathbf{w}|\mu,\mathbf{S})$</code> with parameter <code class="language-plaintext highlighter-rouge">$\tau=(\mu,\mathbf{S})$</code>, mean <code class="language-plaintext highlighter-rouge">$\mu$</code>, and precision <code class="language-plaintext highlighter-rouge">$\mathbf{S}$</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mu_{t+1}  &amp; \leftarrow \mu_t - \beta \mathbf{S}_{t}^{-1} E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w \ell( \mathbf{w}) \big] } \\
\mathbf{S}_{t+1}  &amp; \leftarrow (1-\beta \gamma)\mathbf{S}_t + \beta  E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) \big] }
\end{aligned} \tag{2}\label{2}
$$</code>
</li>
  <li>is less sensitive to parameter transformations  than GD</li>
  <li>converges faster than GD</li>
</ul>

<p><img src="/img/icml2021-fig01.png" width="400"></p>

<p>Challenges of standard NGD:</p>
<ul>
  <li>NGD could violate parameterization constraints (e.g., <code class="language-plaintext highlighter-rouge">$\mathbf{S}_{t+1}$</code> in <code class="language-plaintext highlighter-rouge">$\eqref{2}$</code> may not be positive-definite)</li>
  <li>Singular Fisher information matrix (FIM) <code class="language-plaintext highlighter-rouge">$\mathbf{F}_{\tau}(\tau)$</code> of <code class="language-plaintext highlighter-rouge">$q(w|\tau)$</code>
</li>
  <li>Limited precision/covariance structures</li>
  <li>Ad-hoc approximations for cost reductions</li>
  <li>Complicated and inefficient natural-gradient computation</li>
</ul>

<hr>
<h1 id="ngd-using-local-parameterizations">NGD using Local Parameterizations</h1>

<p>Our method performs NGD updates in local parameter <code class="language-plaintext highlighter-rouge">$\eta$</code> while maintaining structures via matrix groups in auxiliary parameter <code class="language-plaintext highlighter-rouge">$\lambda$</code>. This decoupling enables a <span style="color:red">tractable</span> update that exploits the structures in auxiliary parameter spaces.</p>

<table>
  <tbody>
    <tr>
      <td><img src="/img/icml2021-fig03_new.png" width="450"></td>
      <td>When <code class="language-plaintext highlighter-rouge">$\tau$</code> space has a local vector-space structure, <br> standard NGD in $\tau$ space is a special case of our NGD,  <br> where we choose <code class="language-plaintext highlighter-rouge">$\psi$</code> to be the identity map and <code class="language-plaintext highlighter-rouge">$\phi_{\lambda_t}$</code> to be a linear map.</td>
    </tr>
  </tbody>
</table>

<p>We consider the following three kinds of parameterizations.</p>
<ul>
  <li>Global (original) parameterization <code class="language-plaintext highlighter-rouge">$\tau$</code> for <code class="language-plaintext highlighter-rouge">$q(w|\tau)$</code>
</li>
  <li>New auxiliary parameterization <code class="language-plaintext highlighter-rouge">$\lambda$</code>  with a surjective map: <code class="language-plaintext highlighter-rouge">$\tau= \psi(\lambda)$</code>
</li>
  <li>Local parameterization <code class="language-plaintext highlighter-rouge">$\eta$</code> for <code class="language-plaintext highlighter-rouge">$\lambda$</code> at a current value <code class="language-plaintext highlighter-rouge">$\lambda_t$</code> with a local map:
<code class="language-plaintext highlighter-rouge">$\lambda = \phi_{\lambda_t} (\eta)$</code>,<br> where  <code class="language-plaintext highlighter-rouge">$\phi_{\lambda_t}$</code> is <span style="color:red"> tight </span> at <code class="language-plaintext highlighter-rouge">$\lambda_t$</code>: $\lambda_t \equiv \phi_{\lambda_t} (\eta_0)$, and  we assume <code class="language-plaintext highlighter-rouge">$\eta_0 =\mathbf{0}$</code> to be a relative origin.</li>
</ul>

<div class="notice--success">
  <fieldset class="field-set">
    <legend class="leg-title"><span style="color:red">Our NGD:</span></legend>
    <p><code class="language-plaintext highlighter-rouge">$$ 
\begin{aligned} 
\lambda_{t+1} &amp; \leftarrow  \phi_{\lambda_t} \big( \overbrace{\eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} }^{ \text{NGD in local space} } \big) \,\,\,\, \textrm{(structure-preserving update in auxiliary space)}\\
\tau_{t+1} &amp; \leftarrow \psi\big( \lambda_{t+1} \big) \,\,\,\, \textrm{(constraint-satisfaction update in global space)}
\end{aligned} 
$$</code></p>
  </fieldset>
</div>
<p>where <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_{\eta_0}^{(t)}$</code> is
 the natural-gradient <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_{\eta_0}^{(t)}$</code> at <code class="language-plaintext highlighter-rouge">$\eta_0$</code> tied to <code class="language-plaintext highlighter-rouge">$\lambda_t$</code>, which is computed by the chain rule,
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned} 
        \hat{\mathbf{g}}_{\eta_0}^{(t)} &amp;=  {\color{green}\mathbf{F}_{\eta}(\eta_0)^{-1} } 
         \,\, \big[ \nabla_{\eta_0} \big[ \psi \circ \phi_{\lambda_t} (\eta) \big]
        \nabla_{\tau_t}\mathcal{L}(\tau) \big]  
\end{aligned} 
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathbf{F}_{\eta}(\eta_0)$</code>  is the (exact) FIM for  <code class="language-plaintext highlighter-rouge">$\eta_0$</code> tied to  <code class="language-plaintext highlighter-rouge">$\lambda_t$</code>. 
Our method allows us to choose map <code class="language-plaintext highlighter-rouge">$\psi \circ \phi_{\lambda_t}$</code> so that
the FIM <code class="language-plaintext highlighter-rouge">$\mathbf{F}_{\eta}(\eta_0)$</code> is easy to inverse at <code class="language-plaintext highlighter-rouge">$\eta_0$</code>, which enables tractable natural-gradient
computation.</p>

<hr>
<h1 id="gaussian-example-with-full-precision">Gaussian Example with Full Precision</h1>

<div class="notice--success">
  <p>Notations:</p>
  <ul>
    <li>
<code class="language-plaintext highlighter-rouge">$\mathrm{GL}^{p\times p}$</code>: Invertible Matrices (General Linear Group),</li>
    <li>
<code class="language-plaintext highlighter-rouge">$\mathcal{D}^{p\times p}$</code>: Diagonal Matrices,</li>
    <li>
<code class="language-plaintext highlighter-rouge">$\mathcal{D}_{++}^{p\times p}$</code>: Diagonal and invertible Matrices (Diagonal Matrix Group),</li>
    <li>
<code class="language-plaintext highlighter-rouge">$\mathcal{S}_{++}^{p\times p}$</code>: (Symmetric) positive-definite Matrices,</li>
    <li>
<code class="language-plaintext highlighter-rouge">$\mathcal{S}^{p\times p}$</code>: Symmetric Matrices.</li>
  </ul>
</div>

<p>Consider a Gaussian family <code class="language-plaintext highlighter-rouge">$q(w|\mu,\mathbf{S})$</code> with mean $\mu$ and precision $\mathbf{S}=\Sigma^{-1}$.</p>

<p>The global, auxiliary, and local parameterizations are:
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
        \tau &amp;= \Big\{\mu \in \mathcal{R}^p, \mathbf{S} \in \mathcal{S}_{++}^{p\times p} \Big\},  &amp; \mathbf{S}: \text{positive-definite matrix} \\
        \lambda &amp; = \Big\{ \mu \in  \mathcal{R}^p , \mathbf{B} \in\mathrm{GL}^{p\times p} \Big\}, &amp;\mathbf{B}: \text{ (closed, connected) matrix Lie group member}\\
        \eta &amp;= \Big\{ \delta\in  \mathcal{R}^p, \mathbf{M} \in\mathcal{S}^{p\times p}  \Big\}, &amp; \mathbf{M}: \text{ member in a sub-space of Lie algebra}
\end{aligned}
$$</code></p>

<p>Define <code class="language-plaintext highlighter-rouge">$\mathbf{h}(\mathbf{M}):=\mathbf{I}+\mathbf{M}+\frac{1}{2} \mathbf{M}^2$</code>. 
Maps <code class="language-plaintext highlighter-rouge">$\psi$</code> and  <code class="language-plaintext highlighter-rouge">$\phi_{\lambda_t}$</code> are :
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
        \Big\{ \begin{array}{c} \mu \\ \mathbf{S} \end{array} \Big\} = \psi(\lambda) &amp; := \Big \{ \begin{array}{c} \mu \\ \mathbf{B}\mathbf{B}^\top \end{array} \Big \}, \\
       \Big \{ \begin{array}{c} \mu \\ \mathbf{B} \end{array} \Big \} = \phi_{\lambda_t}(\eta) &amp; := \Big \{ \begin{array}{c} \mu_t + \mathbf{B}_t^{-T} \delta \\ \mathbf{B}_t \mathbf{h} (\mathbf{M}) \end{array} \Big \}.
\end{aligned} \tag{3}\label{3}
$$</code></p>

<div class="notice--info">

  <p>We propose using Lie-group retraction map <code class="language-plaintext highlighter-rouge">$\mathbf{h}()$</code> to</p>
  <ul>
    <li>keep natural-gradient computation tractable</li>
    <li>maintain numerical stability</li>
    <li>enable lower iteration cost compared to the matrix exponential map suggested in <a class="citation" href="#glasmachers2010exponential">[3]</a>
</li>
  </ul>
</div>

<p>Our NGD update in <code class="language-plaintext highlighter-rouge">$  \lambda $</code> space is shown below, where we assume $\eta_0=\mathbf{0}$.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
  \Big\{ \begin{array}{c} \mu_{t+1} \\ \mathbf{B}_{t+1} \end{array} \Big\} = \lambda_{t+1} = 
  \phi_{\lambda_t} \big( \eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} \big)
  =\Big\{ \begin{array}{c}  \mu_t - \beta \mathbf{B}_{t}^{-T} \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ \mathbf{B}_t  \mathbf{h}\big(\beta \mathbf{B}_t^{-1}\mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \end{array} \Big\} 
 \end{aligned}
$$</code>
where <span style="color:red"><strong>tractable</strong></span> natural-gradient  <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{g}}_{\eta_0}^{(t)}$</code>  at <code class="language-plaintext highlighter-rouge">$\eta_0=\{\delta_0, \mathbf{M}_0\}$</code> tied to <code class="language-plaintext highlighter-rouge">$\lambda_t=\{\mu_t,\mathbf{B}_t\}$</code> is</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
    \hat{\mathbf{g}}_{\eta_0}^{(t)} =
 \Big(  \begin{array}{c} \hat{\mathbf{g}}_{\delta_0}^{(t)}\\ \mathrm{vec}(  \hat{\mathbf{g}}_{M_0}^{(t)})\end{array}   \Big)
= \underbrace{  {\color{green} \Big(\begin{array}{cc} \mathbf{I}_p &amp; 0 \\ 0 &amp; 2 \mathbf{I}_{p^2} \end{array} \Big)^{-1}}  }_{ \text{inverse of the exact FIM } } \Big[\begin{array}{c}  \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ \mathrm{vec}( -2\mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T}) \end{array} \Big] \,\,\,\,&amp; (\text{tractable: easy to inverse FIM at  } \eta_0)
\end{aligned}
$$</code></p>

<p>Note that <code class="language-plaintext highlighter-rouge">$\mathbf{g}_\mu$</code> and <code class="language-plaintext highlighter-rouge">$\mathbf{g}_{\Sigma}$</code> are Euclidean gradients of <code class="language-plaintext highlighter-rouge">$\eqref{1}$</code> computed via Stein’s lemma <a class="citation" href="#opper2009variational">[2]</a> <a class="citation" href="#lin2019stein">[5]</a> :
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{g}_\mu = \nabla_{\mu}\mathcal{L}(\tau) = E_{q}{ \big[ \nabla_w \ell( \mathbf{w} ) \big] }, \,\,\,\,\,
\mathbf{g}_{\Sigma}  = \nabla_{S^{-1}}\mathcal{L}(\tau)
 = \frac{1}{2}  E_{q}{ \big[ \nabla_w^2 \ell( \mathbf{w}) \big] } - \frac{\gamma}{2} \mathbf{S} 
 \end{aligned} \tag{4}\label{4}
$$</code></p>

<p>Our update on <code class="language-plaintext highlighter-rouge">$\mathbf{S}_{t+1}=\mathbf{B}_{t+1}\mathbf{B}_{t+1}^T$</code> and <code class="language-plaintext highlighter-rouge">$\mu_{t+1}$</code> is like update of <code class="language-plaintext highlighter-rouge">$\eqref{2}$</code> as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
&amp; \mu_{t+1}   = \mu_t - \beta \mathbf{S}_{t}^{-1} E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w \ell( \mathbf{w} ) \big] } \\
&amp;\mathbf{S}_{t+1} =   \underbrace{ \overbrace{(1-\beta \gamma)\mathbf{S}_t + \beta  E_{q(w|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) \big] }}^{\text{standard NGD on $\mathbf{S}$ }} + { \color{red} \frac{\beta^2}{2} \mathbf{G}_t \mathbf{S}_t^{-1}\mathbf{G}_t}
}_{\color{red}{\text{ RGD with retraction}}}+ O(\beta^3)
 \end{aligned}
$$</code> where $\mathbf{B}$ is a <span style="color:red"><strong>dense</strong></span> matrix in matrix group <code class="language-plaintext highlighter-rouge">$\mathrm{GL}^{p\times p}$</code> and <code class="language-plaintext highlighter-rouge">$\mathbf{G}_t := E_{q(w|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) ] } -\gamma \mathbf{S}_t$</code>.</p>

<p>The second-order term shown in red is used for the positive-definite constraint <a class="citation" href="#lin2020handling">[6]</a> known as a retraction in Riemannian gradient descent (RGD).  The higher-order term <code class="language-plaintext highlighter-rouge">$O(\beta^3)$</code> will be used for structured precision matrices in the next section.</p>

<p>Well-known (group) structures in matrix <code class="language-plaintext highlighter-rouge">$\mathbf{B}$</code> are illustrated in the following figure.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Dense (invertible)</th>
      <th style="text-align: center">Triangular (Cholesky)</th>
      <th style="text-align: center">Diagonal (invertible)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/img/icml2021-group-full.png" width="250"></td>
      <td style="text-align: center"><img src="/img/icml2021-group-tri.png" width="250"></td>
      <td style="text-align: center"><img src="/img/icml2021-group-diag.png" width="250"></td>
    </tr>
  </tbody>
</table>

<h1 id="structured-gaussian-with-flexible-precision">Structured Gaussian with Flexible Precision</h1>

<p>Structures in precision <code class="language-plaintext highlighter-rouge">$\mathbf{S}$</code>, where <code class="language-plaintext highlighter-rouge">$\mathbf{S}=\mathbf{B}\mathbf{B}^T$</code> and matrix <code class="language-plaintext highlighter-rouge">$\mathbf{B}$</code>
is a sparse (group) member as below.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Block lower<br> triangular</th>
      <th style="text-align: center">Block upper<br> triangular</th>
      <th style="text-align: center">Hierarchical<br> (lower Heisenberg)</th>
      <th style="text-align: center">Kronecker product</th>
      <th style="text-align: center">Triangular-Toeplitz</th>
      <th style="text-align: center">Sparse Cholesky</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/img/icml2021-group-low.png" width="220"></td>
      <td style="text-align: center"><img src="/img/icml2021-group-up.png" width="220"></td>
      <td style="text-align: center"><img src="/img/icml2021-group-hie.png" width="220"></td>
      <td style="text-align: center"><img src="/img/icml2021-group-kro.png" width="220"></td>
      <td style="text-align: center"><img src="/img/icml2021-group-tri-Toep.png" width="220"></td>
      <td style="text-align: center"><img src="/img/icml2021-group-sparse.png" width="220"></td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">$\begin{bmatrix} \mathbf{B}_A &amp;  \mathbf{0}  \\ \mathbf{B}_C &amp;  \mathbf{B}_D  \end{bmatrix}$</code></td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">$\begin{bmatrix} \mathbf{B}_A &amp;  \mathbf{B}_B  \\ \mathbf{0} &amp;  \mathbf{B}_D  \end{bmatrix}$</code></td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">$\begin{bmatrix} \mathbf{B}_A &amp;  \mathbf{0}  \\ \mathbf{B}_C &amp;  \begin{bmatrix} \mathbf{B}_{D_1} &amp; \mathbf{0} \\ \mathbf{B}_{3} &amp; \mathbf{B}_{4} \end{bmatrix} \end{bmatrix}$</code></td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">$\begin{bmatrix} d &amp;  0  \\ s &amp;  t  \end{bmatrix} \otimes \begin{bmatrix} r &amp;  0 &amp; 0 \\ {b}_1 &amp; {o}_1 &amp; 0 \\ {b}_2 &amp; 0 &amp; {o}_2     \end{bmatrix} $</code></td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">$\begin{bmatrix} r &amp; 0 &amp; 0 &amp;0 \\ y &amp;  r &amp; 0 &amp; 0  \\ g &amp; y &amp; r &amp; 0 \\ b &amp; g &amp; y &amp; r \end{bmatrix}$</code></td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">$\begin{bmatrix} \mathbf{B}_{D_1} &amp; \mathbf{0} &amp; \mathbf{0} \\ \mathbf{B}_{A} &amp; \mathbf{B}_{B} &amp; \mathbf{0} \\ \mathbf{B}_{D_2} &amp; \mathbf{0} &amp; \mathbf{B}_{D_3} \end{bmatrix}$</code></td>
    </tr>
  </tbody>
</table>

<h2 id="a-structured-gaussian-example">A Structured Gaussian Example:</h2>

<blockquote>
  <p>Auxiliary  parameter $\mathbf{B}$ lives in  a structured space (matrix Lie group):  <code class="language-plaintext highlighter-rouge">${\cal{B}_{\text{up}}}(k)$</code>,
a <span style="color:red">block upper-triangular</span> sub-group of <code class="language-plaintext highlighter-rouge">$\mathrm{GL}^{p \times p}$</code>;</p>

  <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
{\cal{B}_{\text{up}}}(k)  := \Big\{ 
\begin{bmatrix}
\mathbf{B}_A &amp;  \mathbf{B}_B  \\
\mathbf{0} &amp; \mathbf{B}_D
     \end{bmatrix} \Big| &amp; \mathbf{B}_A \in \mathrm{GL}^{k \times k},\,
\mathbf{B}_D  \in{\cal D}^{(p-k) \times (p-k)}_{++}  \Big\},\,\,
\end{aligned}
$$</code></p>

  <p>When <code class="language-plaintext highlighter-rouge">$k=0$</code>, the space <code class="language-plaintext highlighter-rouge">${\cal{B}_{\text{up}}}(0) = {\cal D}^{p \times p}_{++}$</code> becomes  the diagonal case.
When <code class="language-plaintext highlighter-rouge">$k=p$</code>, <code class="language-plaintext highlighter-rouge">${\cal{B}_{\text{up}}}(p) = \mathrm{GL}^{p\times p}$</code> becomes the dense case.</p>

  <p>Consider a local parameter space (sub-space of Lie algebra): <code class="language-plaintext highlighter-rouge">${\cal{M}_{\text{up}}}(k)$</code>.</p>

  <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
{\cal{M}_{\text{up}}}(k):  = \Big\{ 
\begin{bmatrix}
\mathbf{M}_A &amp;  \mathbf{M}_B  \\
\mathbf{0} &amp; \mathbf{M}_D
     \end{bmatrix} \Big| &amp;  \mathbf{M}_A \in{\cal S}^{k \times k}, \,
\mathbf{M}_D  \in{\cal D}^{(p-k) \times (p-k)} \Big\}
\end{aligned}
$$</code></p>

  <p>The global, auxiliary, and local parameterizations :
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
      \tau &amp;= \Big\{\mu \in \mathcal{R}^p,  \mathbf{S}=\mathbf{B} \mathbf{B}^T \in \mathcal{S}_{++}^{p\times p} | \mathbf{B} \in {\cal{B}_{\text{up}}}(k) \Big\}, \\
       \lambda &amp; = \Big\{ \mu \in \mathcal{R}^p,  \mathbf{B} \in {\cal{B}_{\text{up}}}(k) \Big\},\\
       \eta &amp;= \Big\{ \delta\in \mathcal{R}^p,  \mathbf{M} \in {\cal{M}_{\text{up}}}(k)   \Big\}.
\end{aligned}
$$</code></p>

  <p>Maps <code class="language-plaintext highlighter-rouge">$\psi$</code> and <code class="language-plaintext highlighter-rouge">$\phi_{\lambda_t}$</code> are defined in  <code class="language-plaintext highlighter-rouge">$\eqref{3}$</code>.
Our NGD update in the auxiliary space is shown below, where we assume $\eta_0=\mathbf{0}$.</p>
  <div class="notice--success">
    <fieldset class="field-set">
      <legend class="leg-title"><span style="color:red">Structure-preserving update in $\lambda$ space</span></legend>
      <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
 \Big\{ \begin{array}{c} \mu_{t+1} \\ \mathbf{B}_{t+1} \end{array} \Big\} = \lambda_{t+1} = 
 \phi_{\lambda_t} \big( \eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} \big)
 =\Big\{ \begin{array}{c}  \mu_t - \beta \mathbf{B}_{t}^{-T} \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ 
\mathbf{B}_t  \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \kappa_{\text{up}}\big( 2 \mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \Big)  \end{array} \Big\}   
\end{aligned}
$$</code></p>
    </fieldset>
  </div>
  <p>where <code class="language-plaintext highlighter-rouge">$\odot$</code> is the elementwise product ,
<code class="language-plaintext highlighter-rouge">$\kappa_{\text{up}}(\mathbf{X}) \in {\cal{M}_{\text{up}}}(k)$</code> extracts non-zero entries of <code class="language-plaintext highlighter-rouge">${\cal{M}_{\text{up}}}(k)$</code> from <code class="language-plaintext highlighter-rouge">$\mathbf{X}$</code>, 
<code class="language-plaintext highlighter-rouge">$ \mathbf{C}_{\text{up}} = 
\begin{bmatrix}
\frac{1}{2} \mathbf{J}_A &amp;  \mathbf{J}_B  \\
\mathbf{0} &amp; \frac{1}{2} \mathbf{I}_D
     \end{bmatrix}  \in {\cal{M}_{\text{up}}}(k)$</code>, and $\mathbf{J}$ is a matrix of ones.</p>

  <p>Note that (see <a class="citation" href="#lin2021tractable">[1]</a>  for the detail)</p>
  <ul>
    <li>
<code class="language-plaintext highlighter-rouge">$ \mathbf{B}_{t+1} \in$</code> matrix Lie group <code class="language-plaintext highlighter-rouge">${\cal{B}_{\text{up}}}(k)$</code> since 
  <code class="language-plaintext highlighter-rouge">$$
      \begin{aligned}
           &amp;\mathbf{h}\big(\mathbf{M}\big) \in {\cal{B}_{\text{up}}}(k) \text{ for }  \mathbf{M} \in \text{Lie algebra of } {\cal{B}_{\text{up}}}(k) \,\,\,\,  &amp;(\text{by design, } \mathbf{h}(\cdot) \text{ is a Lie-group retraction}) \\
          &amp;\mathbf{B}_{t} \in {\cal{B}_{\text{up}}}(k)  \,\,\,\, &amp; (\text{by construction}) \\
          &amp;\mathbf{B}_{t+1} =  \mathbf{B}_{t}\mathbf{h}\big(\mathbf{M}\big)  \,\,\,\, &amp; (\text{closed under the group product}) 
      \end{aligned}
  $$</code>
</li>
    <li>
<code class="language-plaintext highlighter-rouge">$\mathbf{B}$</code> also induces a low-rank-plus-diagonal structure in covariance
matrix <code class="language-plaintext highlighter-rouge">$\Sigma=\mathbf{S}^{-1}$</code>, where <code class="language-plaintext highlighter-rouge">$\mathbf{S}=\mathbf{B}\mathbf{B}^T$</code>.</li>
  </ul>
</blockquote>

<div class="notice--success">
  <p>In summary, our NGD method:</p>
  <ul>
    <li>is a systematic approach to incorporate structures</li>
    <li>induces exact and non-singular FIMs</li>
  </ul>
</div>

<hr>
<h1 id="applications">Applications</h1>

<h2 id="structured-2nd-order-methods-for-non-convex-optimization">Structured 2nd-order Methods for Non-convex Optimization</h2>

<p>Given an optimization problem
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{\mu \in \mathcal{R}^p} \ell(\mu),
 \end{aligned}\tag{5}\label{5}
$$</code></p>

<p>we formulate a new problem over Gaussian <code class="language-plaintext highlighter-rouge">$q(\mathbf{w}|\tau)$</code> with structured precision, which is a special case of <code class="language-plaintext highlighter-rouge">$\eqref{1}$</code> with <code class="language-plaintext highlighter-rouge">$\gamma=1$</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
   \min_{\tau \in \Omega_\tau} E_{q(w|\tau)} \big[ \ell(\mathbf{w}) \big] +  E_{q(w|\tau)} \big[ \log q(\mathbf{w}|\tau)\big],
 \end{aligned}\tag{6}\label{6}
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathbf{B} \in {\cal{B}_{\text{up}}}(k)$</code> is a block upper-triangular group member,  $\tau=(\mu,\mathbf{S})$ with mean $\mu$ and precision matrix $\mathbf{S}=\mathbf{B}\mathbf{B}^T$.</p>

<p>Using our NGD to solve <code class="language-plaintext highlighter-rouge">$\eqref{6}$</code></p>
<ul>
  <li>gives the following update
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mu_{t+1}  &amp; \leftarrow \mu_{t} - \beta \mathbf{S}_t^{-1} \mathbf{g}_{\mu_t},\\
\mathbf{B}_{t+1}  &amp; \leftarrow   \mathbf{B}_t \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \kappa_{\text{up}}\big( 2 \mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \Big)
 \end{aligned}
$$</code>
</li>
  <li>obtains an update to solve <code class="language-plaintext highlighter-rouge">$\eqref{5}$</code> with group-structural invariance <a class="citation" href="#lin2021structured">[4]</a>:
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
 \mu_{t+1} &amp;  \leftarrow  \mu_t - \beta \mathbf{S}_{t}^{-1} \nabla_{\mu_t} \ell( \mu), \\ 
\mathbf{B}_{t+1} &amp; \leftarrow 
   \mathbf{B}_t \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot { \color{red}\kappa_{\text{up}}\big(  \mathbf{B}_t^{-1} \nabla_{\mu_t}^2 \ell( \mu)  \mathbf{B}_t^{-T} - \mathbf{I} \big)} \Big)
 \end{aligned}\tag{7}\label{7}
$$</code> by using <code class="language-plaintext highlighter-rouge">$\eqref{4}$</code> evaluated at the mean <code class="language-plaintext highlighter-rouge">$\mu_t$</code>
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{g}_{\mu_t} \approx \nabla_{\mu_t} \ell( \mu),\,\,\,\,
 \mathbf{g}_{\Sigma_t} \approx \frac{1}{2} \big[ \nabla_{\mu_t}^2 \ell( \mu) - \mathbf{S}_t\big].
 \end{aligned}\tag{8}\label{8}
$$</code> where $\Sigma=\mathbf{S}^{-1}$ is the covariance.</li>
</ul>

<div class="notice--info">
  <details>
	<summary>Group-structural invariance: (Click to expand)</summary>
<fieldset class="field-set">

      <p>Recall that <code class="language-plaintext highlighter-rouge">$\mathbf{B} \in {\cal{B}_{\text{up}}}(k)$</code>. 
The update in <code class="language-plaintext highlighter-rouge">$\eqref{7}$</code> is invariant under any (group) transform <code class="language-plaintext highlighter-rouge">$\mathbf{R}^T \in  {\cal{B}_{\text{up}}}(k)$</code> of  <code class="language-plaintext highlighter-rouge">$\eqref{5}$</code>  such as <code class="language-plaintext highlighter-rouge">$\min_{y \in \mathcal{R}^p}  \ell(\mathbf{R} \, \mathbf{y})$</code>.</p>
    </fieldset>
</details>
</div>

<div class="notice--info">
  <details>
	<summary>Time complexity: (Click to expand)</summary>
<fieldset class="field-set">
      <ul>
        <li>
<code class="language-plaintext highlighter-rouge">$O(k^2 p)$</code> for triangular structure,</li>
        <li>
<code class="language-plaintext highlighter-rouge">$O( (k_1^2+k_2^2) p)$</code> for hierarchical structure.</li>
      </ul>

      <p>Implementation using Hessian-vector products (HVPs);</p>
      <ul>
        <li>Off-diagonal: <code class="language-plaintext highlighter-rouge">$k$</code> HVPs (triangular), <code class="language-plaintext highlighter-rouge">$(k_1+k_2)$</code> HVPs (hierarchical),</li>
        <li>Diagonal: compute/approximate diagonal entries of <code class="language-plaintext highlighter-rouge">$\nabla_{\mu_t}^2 \ell( \mu)$</code>.</li>
      </ul>
    </fieldset>
</details>
</div>

<div class="notice--info">
  <details>
	<summary>Classical non-convex optimization: (Click to expand)</summary>
<fieldset class="field-set">

      <table>
        <thead>
          <tr>
            <th style="text-align: center" colspan="2">
<font size="4"> 200-dim non-separable, non-convex functions
</font>
</th>
            
          </tr>
          <tr>
            <th style="text-align: center"><img src="/img/icml2021-rbfun.png" width="90%"></th>
            <th style="text-align: left"><img src="/img/icml2021-dpfun.png" width="90%"></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="text-align: center" colspan="2">
<font size="3"> Performance of our method with group structures (lower-triangular, upper-triangular, upper Heisenberg, lower Heisenberg), Adam, and BFGS </font>
</td>
            
          </tr>
        </tbody>
      </table>

    </fieldset>
</details>
</div>

<h2 id="structured-adaptive-gradient-methods-for-deep-learning">Structured Adaptive-gradient Methods for Deep Learning</h2>
<p>At each NN layer,
consider a  Gaussian family
       <code class="language-plaintext highlighter-rouge">$q(\mathbf{w}|\mu,\mathbf{S})$</code> with a Kronecker product structure, where <code class="language-plaintext highlighter-rouge">$\tau=(\mu,\mathbf{S})$</code>.</p>

<p>Our method gives adaptive-gradient updates with group-structural invariance  by
 approximating <code class="language-plaintext highlighter-rouge">$\nabla_{\mu_t}^2 \ell( \mu)$</code>  in <code class="language-plaintext highlighter-rouge">$\eqref{8}$</code> using the Gauss-Newton.</p>

<p>The Kronecker product (<code class="language-plaintext highlighter-rouge">$\mathbf{B}=\mathbf{B}_1 \otimes \mathbf{B}_2$</code>) of two sparse structured groups (<code class="language-plaintext highlighter-rouge">$\mathbf{B}
_1$</code> and <code class="language-plaintext highlighter-rouge">$\mathbf{B}_2$</code>) further reduces the time complexity, where precision <code class="language-plaintext highlighter-rouge">$\mathbf{S}=\mathbf{B}\mathbf{B}^T= (\mathbf{B}_1 \mathbf{B}_1^T) \otimes (\mathbf{B}_2 \mathbf{B}_2^T)$</code></p>

<div class="notice--info">
  <details>
	<summary>Time complexity: (Click to expand)</summary>
<fieldset class="field-set">
      <ul>
        <li>
<code class="language-plaintext highlighter-rouge">$O(k p)$</code> for our Kronecker product with triangular groups, where <code class="language-plaintext highlighter-rouge">$0&lt;k&lt;p^{1/2}$</code>
</li>
        <li>
<code class="language-plaintext highlighter-rouge">$O( (k_1+k_2) p)$</code> for our  Kronecker product with hierarchical groups, where <code class="language-plaintext highlighter-rouge">$0&lt;k_1+k_2&lt;p^{1/2}$</code>
</li>
        <li>
<code class="language-plaintext highlighter-rouge">$O(p)$</code> for Adam and our diagonal groups</li>
        <li>
<code class="language-plaintext highlighter-rouge">$O(p^{3/2})$</code> for KFAC and our Kronecker product with dense groups</li>
      </ul>

      <p>Implementation:</p>
      <ul>
        <li>Automatically parallelized by Auto-Differentiation</li>
        <li>No sequential conjugate-gradient (CG) steps</li>
      </ul>
    </fieldset>
</details>
</div>

<div class="notice--info">
  <details>
	<summary>Image classification problems: (Click to expand)</summary>
<fieldset class="field-set">

      <table>
        <thead>
          <tr>
            <th style="text-align: center" colspan="2">
<font size="4"> Kronecker product of lower-triangular groups for CNN </font>
</th>
            
          </tr>
          <tr>
            <th style="text-align: center"><img src="/img/icml2021-stl10.png" width="90%"></th>
            <th style="text-align: left"><img src="/img/icml2021-cifar10.png" width="90%"></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="text-align: center" colspan="2">
<font size="4"> Comparison between our method with Kronecker product groups and Adam </font>
</td>
            
          </tr>
        </tbody>
      </table>

    </fieldset>
</details>
</div>

<h2 id="variational-inference-with-gaussian-mixtures">Variational Inference with Gaussian Mixtures</h2>

<p>Our NGD</p>
<ul>
  <li>can use structured Gaussian mixtures as flexible variational distributions: <code class="language-plaintext highlighter-rouge">$q(\mathbf{w}|\tau)=\frac{1}{C}\sum_{c=1}^{C}q(\mathbf{w}|\mu_c,\mathbf{S}_c)$</code>
</li>
  <li>gives efficient stochastic natural-gradient variational methods beyond mean-field/diagonal covariance</li>
</ul>

<div class="notice--info">
  <details>
	<summary>Approximating 80-dim multimodal distributions: (Click to expand)</summary>
<fieldset class="field-set">

      <table>
        <thead>
          <tr>
            <th style="text-align: center" colspan="2">
<font size="4"> First 8 marginal distributions of Gaussian mixture approximation with upper-triangular structure </font>
</th>
            
          </tr>
          <tr>
            <th style="text-align: center"><img src="/img/icml2021-tmm80d-01.png" width="90%"></th>
            <th style="text-align: left"><img src="/img/icml2021-tmm80d-02.png" width="90%"></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="text-align: center" colspan="2">
<font size="4"> Comparison between our approximation ($k=5$) and the ground-truth (mixture of t distributions) </font>
</td>
            
          </tr>
        </tbody>
      </table>

    </fieldset>
</details>
</div>

<hr>
<h1 id="references">References</h1>
<p class="bibliography"></p>
<p><span id="lin2021tractable">[1] W. Lin, F. Nielsen, M. E. Khan, &amp; M. Schmidt, "Tractable structured natural gradient descent using local parameterizations," <i>International Conference on Machine Learning (ICML)</i> (2021).</span></p>
<p><span id="opper2009variational">[2] M. Opper &amp; C. Archambeau, "The variational Gaussian approximation revisited," <i>Neural computation</i> <b>21</b>:786–792 (2009).</span></p>
<p><span id="glasmachers2010exponential">[3] T. Glasmachers, T. Schaul, S. Yi, D. Wierstra, &amp; J. Schmidhuber, "Exponential natural evolution strategies," <i>Proceedings of the 12th annual conference on Genetic and evolutionary computation</i> (2010), pp. 393–400.</span></p>
<p><span id="lin2021structured">[4] W. Lin, F. Nielsen, M. E. Khan, &amp; M. Schmidt, "Structured second-order methods via natural gradient descent," <i>arXiv preprint arXiv:2107.10884</i> (2021).</span></p>
<p><span id="lin2019stein">[5] W. Lin, M. E. Khan, &amp; M. Schmidt, "Stein’s Lemma for the Reparameterization Trick with Exponential Family Mixtures," <i>arXiv preprint arXiv:1910.13398</i> (2019).</span></p>
<p><span id="lin2020handling">[6] W. Lin, M. Schmidt, &amp; M. E. Khan, "Handling the positive-definite constraint in the bayesian learning rule," <i>International Conference on Machine Learning</i> (PMLR, 2020), pp. 6116–6126.</span></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#exponential-family" class="page__taxonomy-item" rel="tag">Exponential Family</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#information-geometry" class="page__taxonomy-item" rel="tag">Information Geometry</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#matrix-lie-groups" class="page__taxonomy-item" rel="tag">Matrix Lie Groups</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#natural-gradient-descent" class="page__taxonomy-item" rel="tag">Natural Gradient Descent</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-07-05T00:00:00-07:00">July 05, 2021</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title" data-translate="share_on_label">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Structured+Natural+Gradient+Descent+%28ICML+2021%29%20informationgeometryML.github.io%2Fposts%2F2021%2F07%2FICML%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=informationgeometryML.github.io%2Fposts%2F2021%2F07%2FICML%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=informationgeometryML.github.io%2Fposts%2F2021%2F07%2FICML%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>



     

  <script src="https://utteranc.es/client.js" repo="informationgeometryML/informationgeometryML.github.io" issue-term="url" label="blog-comments" theme="github-light" crossorigin="anonymous" async>
  </script>





</section>


      
  <nav class="pagination">
    
      <a href="#" class="pagination--pager disabled">Previous</a>
    
    
      <a href="/posts/2021/09/Geomopt01/" class="pagination--pager" title="Part I: Smooth Manifolds with the Fisher-Rao Metric
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/12/Geomopt06/" rel="permalink">Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (Part VI is incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/12/Geomopt05/" rel="permalink">Part V: Efficient Natural-gradient Methods for Exponential Family
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  19 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/11/Geomopt04/" rel="permalink">Part IV: Natural Gradient Descent and its Extension—Riemannian Gradient Descent
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  19 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/11/Geomopt03/" rel="permalink">Part III: Invariance of Natural-Gradients
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  13 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Goal
This blog post focuses on invariant properties of natural-gradients.
We will discuss

  transformation rules of natural-gradients,
  the automatic compu...</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow</strong></li>
    
    
    
    
      <li><a href="https://github.com/informationgeometryML"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">© 2022 Information Geometry in ML. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.2.0/js/all.js"></script>












  
    <script src="/assets/js/custom.js"></script>
  
    <script src="/assets/js/math-code.js"></script>
  



  </body>
</html>
