<!DOCTYPE html>
<!--
  Minimal Mistakes Jekyll Theme 4.12.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
--><html lang="en" class="no-js">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Part V: Efficient Natural-gradient Methods for Exponential Family - Information Geometry in Machine Learning</title>
<meta name="description" content="Warning: working in Progress (incomplete)">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Information Geometry in Machine Learning">
<meta property="og:title" content="Part V: Efficient Natural-gradient Methods for Exponential Family">
<meta property="og:url" content="/posts/2021/12/Geomopt05/">


  <meta property="og:description" content="Warning: working in Progress (incomplete)">







  <meta property="article:published_time" content="2021-12-14T00:00:00-08:00">





  

  


<link rel="canonical" href="/posts/2021/12/Geomopt05/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Information Geometry in ML",
      "url": "https://github.com/pages/informationgeometryML/informationgeometryML.github.io",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Information Geometry in Machine Learning Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

<!-- end custom head snippets -->

  </head>

  <body class="layout--single mywide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Information Geometry in Machine Learning</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="/">Home</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/news/">News</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/year-archive/">Blog Posts</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  

  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Part V: Efficient Natural-gradient Methods for Exponential Family">
    <meta itemprop="description" content="Warning: working in Progress (incomplete)">
    <meta itemprop="datePublished" content="December 14, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Part V: Efficient Natural-gradient Methods for Exponential Family
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  18 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>Warning: working in Progress (incomplete)</p>

<h2 id="goal">Goal</h2>
<p>This blog post should show that we can efficiently implement natural-gradient methods in many cases.</p>

<p>We will give an informal introduction with a focus on high level of ideas.</p>

<div class="notice--info">
  <details>
<summary>Click to see how to cite this blog post</summary>
<fieldset class="field-set">

      <div class="language-latex highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>@misc<span class="p">{</span>lin2021NGDblog05,
  title = <span class="p">{</span>Introduction to Natural-gradient Descent: Part V<span class="p">}</span>,
  author = <span class="p">{</span>Lin, Wu and Nielsen, Frank and Khan, Mohammad Emtiyaz and Schmidt, Mark<span class="p">}</span>,
  url = <span class="p">{</span>https://informationgeometryml.github.io/year-archive/<span class="p">}</span>, 
  howpublished = <span class="p">{</span><span class="k">\url</span><span class="p">{</span>https://informationgeometryml.github.io/posts/2021/12/Geomopt05/<span class="p">}}</span>,
  year = <span class="p">{</span>2021<span class="p">}</span>,
  note = <span class="p">{</span>Accessed: 2021-12-14<span class="p">}</span>
<span class="p">}</span>
</code></pre></div>      </div>
    </fieldset>
</details>
</div>

<h1 id="exponential-family">Exponential Family</h1>
<hr>

<p>An exponential family takes the following (canonical) form as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p(\mathbf{w}|\mathbf{\eta}) = h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta (\mathbf{\eta}) )
\end{aligned}
$$</code> where   <code class="language-plaintext highlighter-rouge">$C_\eta(\eta) :=  \int h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}(\mathbf{w}) \rangle ) d \mathbf{w}
$</code> is the normalization constant.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><code class="language-plaintext highlighter-rouge">$A_\eta(\mathbf{\eta}):=\log C_\eta(\eta)$</code></td>
      <td style="text-align: center">log-partition function</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="language-plaintext highlighter-rouge">$h_\eta(\mathbf{w})$</code></td>
      <td style="text-align: center">base measure</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="language-plaintext highlighter-rouge">$\mathbf{T}_\eta(\mathbf{w})$</code></td>
      <td style="text-align: center">sufficient statistics</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="language-plaintext highlighter-rouge">$\eta$</code></td>
      <td style="text-align: center">natural parameter</td>
    </tr>
  </tbody>
</table>

<p>The parameter space of <code class="language-plaintext highlighter-rouge">$\eta$</code> denoted by <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> is determined so that the normalization constant is well-defined and (strictly and finitely) positive.</p>

<div class="notice--success">
  <p><strong>Regular</strong> natural parametrization <code class="language-plaintext highlighter-rouge">$\eta$</code>: parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> is open.</p>
</div>

<p>In this post, we only consider
regular natural parametrizations since commonly used exponential family distributions have a regular natural parametrization.
This natural parametrization is special since the inner product <code class="language-plaintext highlighter-rouge">$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$</code> is <strong>linear</strong> in <code class="language-plaintext highlighter-rouge">$\eta$</code>. As we will discuss later,  this linearity is essential.</p>

<p>Examples of an exmponential family are Gaussian, Bernoulli, Von Mises–Fisher, and <a href="https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions">more</a>.</p>

<p>Readers should be aware of the following points when using an exponential family.</p>

<ul>
  <li>
    <p>The support of <code class="language-plaintext highlighter-rouge">$\mathbf{w}$</code> should not depend on parametrization <code class="language-plaintext highlighter-rouge">$\eta$</code>.</p>
  </li>
  <li>The base measure and the log-partition function are only unique up to a constant as illustrated by the following example.
    <div class="notice--info">
      <details>
  <summary>Univariate Gaussian as an exponential family (click to expand)</summary>
  <fieldset class="field-set">
          <blockquote>

            <p>Recall that in <a href="/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families">Part I</a>, we consider this family as
<code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&gt;0 \}$</code> with mean <code class="language-plaintext highlighter-rouge">$\mu$</code> and variance <code class="language-plaintext highlighter-rouge">$\sigma$</code>, where <code class="language-plaintext highlighter-rouge">$\mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $</code>.</p>

            <p>We re-express it in an exponential form as</p>

            <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p({w}|\mathbf{\eta})  &amp;= \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] \\
&amp;= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2\sigma} \\ \frac{\mu}{\sigma}  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} w^2 \\ w  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ]     )   \\
\end{aligned}
$$</code>
Since <code class="language-plaintext highlighter-rouge">$\sigma= -\frac{1}{2\eta_1} $</code> and <code class="language-plaintext highlighter-rouge">$\mu = -\frac{\eta_2}{2\eta_1}$</code>,  <code class="language-plaintext highlighter-rouge">$A_\eta(\mathbf{\eta}) = \frac{1} {2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ] = \frac{1}{2} [ \log ( 2\pi ) + \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $</code>.</p>

            <p>It is also valid that $ h_\eta({w}) = \frac{1}{\sqrt{2\pi}} $ and 
<code class="language-plaintext highlighter-rouge">$A_\eta(\mathbf{\eta}) =  \frac{1}{2} [  \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $</code>.</p>

            <p>We can easily verify that parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta= \{ (\eta_1,\eta_2) | \eta_1&lt;0 , \eta_2 \in \mathcal{R} \}$</code> is open (in $\mathcal{R}^2$).</p>
          </blockquote>
        </fieldset>
  </details>
    </div>
  </li>
  <li>The log-partition function can be differentiable w.r.t. <code class="language-plaintext highlighter-rouge">$\eta$</code> <a class="citation" href="#johansen1979introduction">[1]</a> even when <code class="language-plaintext highlighter-rouge">$\mathbf{w}$</code> is discrete.
    <div class="notice--info">
      <details>
  <summary>Bernoulli as an exponential family (click to expand)</summary>
  <fieldset class="field-set">
          <blockquote>

            <p>Recall that in <a href="/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families">Part I</a>, we consider this family as
 <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \Big| 0&lt;\pi&lt;1 \}$</code></p>

            <p>We re-express it in an exponential form as</p>

            <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;=  \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \\
&amp;=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \log \frac{\pi}{1-\pi}}_{\eta} , \underbrace{ \mathcal{I}(w=0)}_{T_\eta(w) } \rangle - \log \frac{1}{1-\pi} )
\end{aligned}
$$</code>
Since <code class="language-plaintext highlighter-rouge">$\pi = \frac{\exp(\eta)}{1+ \exp(\eta) } $</code> , we have <code class="language-plaintext highlighter-rouge">$A_\eta(\mathbf{\eta}) =  \log \frac{1}{1-\pi} = \log(1+\exp(\eta))$</code>. <br>
We easily to verify that parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta= \{ \eta | \eta \in \mathcal{R} \}$</code> is open in $\mathcal{R}$.</p>
          </blockquote>
        </fieldset>
  </details>
    </div>
  </li>
  <li>An invertiable linear reparametrization could also be a natural parametrization.  Thus, in this setting, natural-gradient descent could be better than (Euclidean) gradient descent since natural-gradient descent is linearly invariant.
    <div class="notice--info">
      <details>
  <summary>Natural parametrization is not unique (click to expand)</summary>
  <fieldset class="field-set">
          <blockquote>

            <p>For simplicity, let’s assume set <code class="language-plaintext highlighter-rouge">$\Omega_\lambda := \{ \mathbf{U}^{-1} \eta | \eta \in \Omega_\eta \} = \Omega_\eta$</code>, where <code class="language-plaintext highlighter-rouge">$\mathbf{U}$</code> is a constant invertiable matrix.</p>

            <p>Consider a linear reparametrization such as <code class="language-plaintext highlighter-rouge">$\lambda=\mathbf{U}^{-1} \mathbf{\eta}$</code>, parametrization <code class="language-plaintext highlighter-rouge">$\lambda$</code> is also a natural parametrization as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p(\mathbf{w}|\mathbf{\lambda})
&amp;= h_\eta(\mathbf{w}) \exp( \langle \mathbf{U}\mathbf{\lambda} , \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta( \mathbf{U}\mathbf{\lambda}) ) \\
&amp;=  h_\eta(\mathbf{w})\exp( \langle \mathbf{\lambda} , \mathbf{U}^T \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta(\mathbf{U}\mathbf{\lambda}) ) \\
&amp;= h_\lambda(\mathbf{w})  \exp( \langle \mathbf{\lambda} ,  \mathbf{T}_\lambda(\mathbf{w}) \rangle - A_\lambda(\mathbf{\lambda}) ) 
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$h_\lambda(\mathbf{w}):= h_\eta(\mathbf{w})$</code>,  <code class="language-plaintext highlighter-rouge">$\mathbf{T}_\lambda(\mathbf{w}):= \mathbf{U}^T\mathbf{T}_\eta(\mathbf{w})$</code>, and 
<code class="language-plaintext highlighter-rouge">$A_\lambda(\mathbf{\lambda}):= A_\eta(\mathbf{U}\mathbf{\lambda})$</code>.</p>
          </blockquote>
        </fieldset>
  </details>
    </div>
  </li>
</ul>

<h2 id="minimal-parametrizations-of-exponential-family">Minimal Parametrizations of Exponential Family</h2>

<p>Now, we discuss particular parametrizations of an exponential family. We could efficiently compute natural-gradients when
using this class of parametrizations.</p>

<div class="notice--success">
  <p><strong>Minimal</strong> natural parametrization: the corresponding sufficient stattistics <code class="language-plaintext highlighter-rouge">$\mathcal{T}(\mathbf{w})$</code> is linearly independent.</p>
</div>

<p>A regular, minimal, and natural parametrization <code class="language-plaintext highlighter-rouge">$\eta$</code> has many nice properties <a class="citation" href="#johansen1979introduction">[1]</a> <a class="citation" href="#wainwright2008graphical">[2]</a>.</p>

<ul>
  <li>
    <p>It is an intrinsic parametrization as we discussed in
 <a href="/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families">Part I</a>.</p>
  </li>
  <li>
    <p>The parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> is an <strong>open</strong> set in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>, where
<code class="language-plaintext highlighter-rouge">$K$</code> is the number of entires of this parameter array.</p>
  </li>
  <li>
    <p>The log-partition function <code class="language-plaintext highlighter-rouge">$A_\eta(\eta)$</code> is infinitely differentiable and strictly convex in <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code>.</p>
  </li>
  <li>
    <p>The FIM <code class="language-plaintext highlighter-rouge">$\mathbf{F}_\eta(\eta) = \nabla_\eta^2 A_\eta(\eta)$</code> is positive-definite in its domain.</p>
  </li>
</ul>

<p>We will only show the first property in this post. The remaining properties can be found in the literature.
Note that the linearity in the inner product <code class="language-plaintext highlighter-rouge">$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$</code>,  plays a key role in showing these properties.</p>

<div class="notice--success">
  <p><strong>Claim</strong>:</p>

  <p>A regular, minimal, and natural parametrization is intrinsic.</p>
</div>

<div class="notice--info">
  <details>
<summary>Proof of the claim (click to expand)</summary>
<fieldset class="field-set">
      <blockquote>
        <p>Proof by contradiction:</p>

        <p>Recall that a parametrization is intrinsic if 
<code class="language-plaintext highlighter-rouge">$K$</code> partial derivatives 
<code class="language-plaintext highlighter-rouge">$ \{ \partial_{\eta_i} \log p(\mathbf{w}|\eta) \} $</code>  are linearly independent.
Since the parameter space is open in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code> and the log-partition function <code class="language-plaintext highlighter-rouge">$A_\eta(\eta)$</code> is differentiable,  these partial derivatives are well-defined and can be computed as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\partial_{\eta_i} \log p(\mathbf{w}|\eta) = \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle - \partial_{\eta_i}A_\eta(\eta) 
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathbf{e}_i$</code> is an one-hot/unit array which has zero in all entries except the $i$-th entry.</p>

        <p>Suppose a regular, minimal, and natural parametrization is not intrinsic.
These partial derivatives must be linearly dependent.
Thus, there exist a set of non-zero constant <code class="language-plaintext highlighter-rouge">$c_i$</code> such that
<code class="language-plaintext highlighter-rouge">$\sum_i c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta)= 0 $</code>, where the value of <code class="language-plaintext highlighter-rouge">$c_i$</code> does not depent on  <code class="language-plaintext highlighter-rouge">$\mathbf{w}$</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
0 &amp;= \sum_{i=1}^{K} c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta) \\
&amp;= \sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle -   c_i\partial_{\eta_i}A_\eta(\eta) \\
\end{aligned}
$$</code></p>

        <p>Since  <code class="language-plaintext highlighter-rouge">$c_i$</code> is a non-zero constant and its value does not depent on  <code class="language-plaintext highlighter-rouge">$\mathbf{w}$</code>, we must have
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
0 =\sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle ,
\end{aligned}
$$</code> which implies that
the sufficient stattistics <code class="language-plaintext highlighter-rouge">$\mathcal{T}(\mathbf{w})$</code> is linearly dependent. This is a contradiction since <code class="language-plaintext highlighter-rouge">$\eta$</code> is a minimal natural parametrization.</p>
      </blockquote>
    </fieldset>
</details>
</div>

<p>Now, we give an example of a regular, minimal and natural parametrization.</p>
<div class="notice--info">
  <details>
<summary>Minimal parametrization for multivariate Gaussian (click to expand)</summary>
<fieldset class="field-set">
      <blockquote>

        <p>Consider a $d$-dimensional Gaussian family.</p>

        <p>We specify a parameterization $\mathbf{\tau}$ of the  family as <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(\mathbf{w} |\mathbf{\mu},\mathbf{\Sigma}) \Big| \mathbf{\mu} \in \mathcal{R}^d, \mathbf{\Sigma}   \succ \mathbf{0} \}$</code> with mean <code class="language-plaintext highlighter-rouge">$\mathbf{\mu}$</code> and covariance <code class="language-plaintext highlighter-rouge">$\mathbf{\Sigma}$</code>.</p>

        <p>We re-express it in an exponential form as</p>

        <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p({w}|\mathbf{\lambda})  &amp;= \frac{1}{\sqrt{ \mathrm{det}( 2\pi \Sigma )} } \exp [- \frac{1}{2} (\mathbf{w}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{w}-\mathbf{\mu})  ] \\
&amp;= \underbrace{ \exp(0) }_{  h_\lambda({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \mathrm{vec}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\lambda} }  ,  \underbrace{\begin{bmatrix} \mathrm{vec}( \mathbf{w} \mathbf{w}^T) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\lambda ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathrm{vec}()$</code> is the vectorization function
and  <code class="language-plaintext highlighter-rouge">$\lambda$</code> is a $(d+d^2)$-dim array.</p>

        <p>Parametrization $\lambda$ is a natural and regular parametrization. However, it is NOT a minimal
parametrization since <code class="language-plaintext highlighter-rouge">$\mathbf{w} \mathbf{w}^T$</code> is symmetric and therefore the sufficient statistics is linearly dependent.
It can be shown that $\Omega_\lambda$ is relatively open but not open in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>, where <code class="language-plaintext highlighter-rouge">$K=d+d^2$</code>.</p>

        <p>Recall that <code class="language-plaintext highlighter-rouge">$\mathrm{vech}()$</code> is the <a href="https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization">half-vectorization function</a>.
We define a new map <code class="language-plaintext highlighter-rouge">$\mathrm{vec2h}(\mathrm{S}):=\mathrm{vech}\big(2\mathrm{S} - \mathrm{Diag}( \mathrm{diag}(\mathrm{S}) ) \big)$</code>, which is like map $\mathrm{vech}()$ except that the off-diagonal entries in the low-triangular part are multiplied by 2.</p>

        <p>A minimal natural parametrization $\eta$ should be defined as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} {\color{red}\mathrm{vech}}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix}  {\color{red}\mathrm{vec2h} }( \mathrm{w}\mathbf{w}^T )   ) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
\end{aligned}
$$</code> where  <code class="language-plaintext highlighter-rouge">$\eta$</code> is a $(d+\frac{d(d+1)}{2})$-dim array.
It can be shown that $\Omega_\eta$ is open in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>, where <code class="language-plaintext highlighter-rouge">$K=d+\frac{d(d+1)}{2}$</code>.</p>

        <p>As we discussed in <a href="/posts/2021/10/Geomopt02/#riemannian-steepest-direction">Part II</a>,  <code class="language-plaintext highlighter-rouge">$\mathrm{vech}(\Sigma^{-1})$</code> is an intrinsic parameterization while <code class="language-plaintext highlighter-rouge">$\mathrm{vec}(\Sigma^{-1})$</code> is not.</p>
      </blockquote>
    </fieldset>
</details>
</div>

<p>The following example illustrates
a non-minimal natural parametrization</p>
<div class="notice--info">
  <details>
<summary>Non-minimal parametrization for Bernoulli (click to exapnd)</summary>
<fieldset class="field-set">
      <blockquote>

        <p>We consider this family in <a href="#exponential-family">the previous section</a> with another
parametrization dicussed in
<a href="/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families">Part I</a>.
 <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{I}(w=0) \frac{\pi_1}{\pi_1+\pi_2} + \mathcal{I}(w=1) \frac{\pi_2}{\pi_1+\pi_2} \Big| \pi_1&gt;0, \pi_2&gt;0 \}$</code></p>

        <p>We re-express it in another exponential form as</p>

        <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;=  \mathcal{I}(w=0) \frac{\pi_1}{\pi_1+\pi_2} + \mathcal{I}(w=1) \frac{\pi_2}{\pi_1+\pi_2} \\
&amp;=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \begin{bmatrix} \log \frac{\pi_1}{\pi_1+\pi_2} \\  \log \frac{\pi_2}{\pi_1+\pi_2} \end{bmatrix} }_{\eta} , \underbrace{ \begin{bmatrix} \mathcal{I}(w=0) \\   \mathcal{I}(w=1)\end{bmatrix} }_{\mathbf{T}_\eta(w) } \rangle -\underbrace{ 0}_{A_\eta(\eta)} )
\end{aligned}
$$</code></p>

        <p>The natural parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta= \{ (\eta_1,\eta_2) | \exp(\eta_1)+\exp(\eta_2) = 1 \}$</code> is not open in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^2$</code>.</p>

        <p>This is not a minimal natural parametrization since the sufficient statistics $\mathbf{T}_\eta(w)$ is linearly dependent as 
<code class="language-plaintext highlighter-rouge">$\mathcal{I}(w=0)+\mathcal{I}(w=1)=1$</code>.</p>
      </blockquote>
    </fieldset>
</details>
</div>

<h1 id="efficient-natural-gradient-computation">Efficient Natural-gradient Computation</h1>

<p>In general, natural-gradient computation can be challenging due to the inverse of the Fisher matrix.
In cases of an exponential family,  natural-gradient computation often can be quite efficient without directly computing the inverse of the Fisher matrix.
We will assume <code class="language-plaintext highlighter-rouge">$\eta$</code> is a reguar, minimal, and natural parametrization.</p>

<h2 id="expectation-parametrization">Expectation Parametrization</h2>
<p>We introduce a dual parametrization <code class="language-plaintext highlighter-rouge">$\mathbf{m} := E_{p(w|\eta)}[ \mathbf{T}_\eta(\mathbf{w}) ] $</code>, which is known as the expectation parametrization. This new parametrization plays a key role for the efficient natural-gradient computation.</p>

<p>Recall that in
<a href="/posts/2021/11/Geomopt04/#the-hessian-is-not-a-valid-manifold-metric">Part IV</a>,  we use the identity of the score function.  We  can establish a connection 
between these two parametrizations via this identity.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{0} &amp; = E_{p(w|\eta)} [ \nabla_\eta \log p(\mathbf{w}|\eta) ] &amp;\,\,\,( \text{expectation of the score is zero} ) \\
&amp;=E_{p(w|\eta)} [ \mathbf{T}_\eta(\mathbf{w}) - \nabla_\eta A_\eta(\eta) ] \\
&amp;= \mathbf{m}  - \nabla_\eta A_\eta(\eta)
\end{aligned}\tag{1}\label{1}
$$</code>  which is a valid Legendre (dual) transformation<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> since 
<code class="language-plaintext highlighter-rouge">$\nabla_\eta^2 A_\eta(\eta)$</code> is positive-definite in its domain <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code>.</p>

<h2 id="expectation-parameter-space">Expectation Parameter Space</h2>
<p>We can view
the expectation parameter as an ouput of
a continous map 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{m}(\eta):=\nabla_\eta A_\eta(\eta),
\end{aligned}\tag{2}\label{2}
$$</code>
 where the input space of this map is <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code>.</p>

<p>We define the expectation parameter space as the output space of the map 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\Omega_m :=\{\mathbf{m}(\eta) | \eta \in \Omega_\eta \}.
\end{aligned}
$$</code></p>

<p>Since <code class="language-plaintext highlighter-rouge">$\nabla_\eta^2 A_\eta(\eta)$</code> is positive-definite in open set <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code>, we can show that there exists an one-to-one relationship between the
natural parameter <code class="language-plaintext highlighter-rouge">$\eta$</code> and the expectation parameter <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code>, which implies that map <code class="language-plaintext highlighter-rouge">$\mathbf{m}(\cdot)$</code> is injective.</p>

<p>Since <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> is open in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>, we can show that
the expectation parameter space <code class="language-plaintext highlighter-rouge">$\Omega_m$</code> is also open in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code> due to the <a href="https://en.wikipedia.org/wiki/Invariance_of_domain">invariance of domain</a>.</p>

<h2 id="natural-gradient-computation">Natural-gradient Computation</h2>
<p>Note that the FIM of the exponential family under parametrization <code class="language-plaintext highlighter-rouge">$\eta$</code> is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{F}_\eta(\eta)=\nabla_\eta^2 A_\eta(\eta) = \nabla_\eta \mathbf{m}
\end{aligned}\tag{3}\label{3}
$$</code> which means this FIM is a Jacobian matrix <code class="language-plaintext highlighter-rouge">$\mathbf{J}=\nabla_\eta \mathbf{m}(\eta)$</code>.</p>

<p>As we discussed in
<a href="/posts/2021/10/Geomopt02/#riemannian-steepest-direction">Part II</a>,
a natural-gradient w.r.t. <code class="language-plaintext highlighter-rouge">$f(\eta)$</code> can be computed as below, where <code class="language-plaintext highlighter-rouge">$\mathbf{g}_\eta=\nabla_\eta f(\eta)$</code> is a Euclidean gradient w.r.t.  natural parameter <code class="language-plaintext highlighter-rouge">$\eta$</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\hat{\mathbf{g}}_\eta &amp; = \mathbf{F}^{-1}_\eta(\eta) \mathbf{g}_\eta \\
&amp;= (\nabla_\eta \mathbf{m} )^{-1} [ \nabla_\eta f(\eta) ] \\
&amp;= [\nabla_{m} \eta ] [ \nabla_\eta f(\eta) ] \\
&amp;=  \nabla_{m} f(\eta)  
\end{aligned}\tag{4}\label{4}
$$</code> where 
<code class="language-plaintext highlighter-rouge">$\nabla_{m} f( \eta )$</code> is a Euclidean gradient w.r.t. expectation parameter <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code> and
<code class="language-plaintext highlighter-rouge">$\eta=\eta( \mathbf{m} )$</code>
can be viewed  as a function of <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code>.</p>

<p>Therefore, we can  efficinetly compute natural-gradients w.r.t. natural parameter <code class="language-plaintext highlighter-rouge">$\eta$</code> <a class="citation" href="#khan2017conjugate">[3]</a>  if we  can  easily compute Euclidean gradients w.r.t. its expectation parameter <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code>.</p>

<h2 id="efficient-ngd-for-multivariate-gaussians">Efficient NGD for multivariate Gaussians</h2>
<p>Given a $d$-dim multivariate Gaussian with a full covariance structure, 
the naive way to compute natural-gradients in this case has $O( (d^2)^3 )=O(d^6)$ iteration cost since the covariance matrix has $d^2$ entries.
Now, we show how to efficiently compute natural-gradients in this case.</p>

<div class="notice--success">
  <p><strong>Claim</strong>:</p>

  <p>We can efficiently  compute natural-gradients 
in $O( d^3 )$ iteration cost in this case with the number of $O(d^2)$ parameters.</p>
</div>

<div class="notice--info">
  <details>
	<summary>Proof of the claim (Click to expand)</summary>
<fieldset class="field-set">
      <blockquote>
        <p>As shown in <a href="#minimal-parametrizations-of-exponential-family">the previous section</a>, a minimal,regular, and natural parameter is <code class="language-plaintext highlighter-rouge">$\eta=(\mathbf{S}\mu,-\frac{1}{2}\mathrm{vech}(\mathbf{S}) )$</code>, where <code class="language-plaintext highlighter-rouge">$\Sigma$</code> is the covariance and <code class="language-plaintext highlighter-rouge">$\mathbf{S}= ( \Sigma )^{-1}$</code>.
$\Sigma$ is considered as a d-by-d matrix with <code class="language-plaintext highlighter-rouge">$\frac{d(d+1)}{2}$</code> distinct entries (degrees of freedom).</p>

        <p>The corresponding expectation parameter  is $\mathbf{m}=( E_{p(w)}[ \mathbf{w} ], E_{p(w)}[ \mathrm{vec2h}( \mathbf{w}\mathbf{w}^T ) ]) = (\mu, \mathrm{vec2h}(\mu\mu^T+\Sigma ) ) $, where
$\mathrm{vec2h}(\mathrm{S}):=\mathrm{vech}\big(2\mathrm{S} - \mathrm{Diag}( \mathrm{diag}(\mathrm{S}) ) \big)$ is defined in the previous section.</p>

        <p>By using the result in this section, we can efficiently  compute each natural gradient w.r.t. $\eta$ by  computing the corresponding Eucldiean gradient w.r.t. $\mathbf{m}$.</p>

        <p>Denote $\mathbf{m}^{(1)}:=\mu$ and $\mathbf{m}^{(2)}:=\mathrm{vec2h}(\mu\mu^T+\Sigma)$.</p>

        <p>Notice that we can re-express the mean and the covariance in terms of <code class="language-plaintext highlighter-rouge">$\mathbf{m}=(\mathbf{m}^{(1)}, \mathbf{m}^{(2)})$</code> as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mu = \mathbf{m}^{(1)} , \mathrm{vec2h}(\Sigma) = \mathbf{m}^{(2)} -\mathrm{vec2h}( \mathbf{m}^{(1)}(\mathbf{m}^{(1)})^T)
\end{aligned}
$$</code></p>

        <p>Given a smooth scalar function denoted by $\ell$,
by the chain rule, we have
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\overbrace{ \frac{\partial \ell}{\partial m^{(1)}_i} }^{\text{scalar}}&amp; = \overbrace{ \frac{\partial \ell}{\partial \mu}}^{\text{row vector  } } \overbrace{ \frac{\partial \mu }{\partial m^{(1)}_i}}^{\text{column vector}} + \frac{\partial \ell}{\partial \mathrm{vec2h}(\Sigma)} \frac{\partial \mathrm{vec2h}(\Sigma) }{\partial m^{(1)}_i} \\
\frac{\partial \ell}{\partial m^{(2)}_i} &amp; = \frac{\partial \ell}{\partial \mu} \frac{\partial \mu }{\partial m^{(2)}_i} + \frac{\partial \ell}{\partial \mathrm{vec2h}(\Sigma)} \frac{\partial \mathrm{vec2h}(\Sigma) }{\partial m^{(2)}_i}
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$m^{(1)}_i$</code> denotes the $i$-th entry of  <code class="language-plaintext highlighter-rouge">$\mathbf{m}^{(1)}$</code>.</p>

        <p>First, notice that we can efficiently compute the following Euclidean gradients by Auto-Diff
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\frac{\partial \ell}{\partial \mu} &amp;= \mathbf{g}_\mu^T \\
\frac{\partial \ell}{\partial V } &amp; =\mathbf{g}_{V} \,\,\,\, ( \mathbf{V} \text{ is a d-by-d matrix with } d^2  \text{ degrees of freedom} ) \\
\frac{\partial \ell}{\partial \Sigma } &amp; =\mathbf{g}_{\Sigma} \,\,\,\, ( \Sigma \text{ is a d-by-d matrix with } \frac{d(d+1)}{2}  \text{ degrees of freedom} ) \\
\frac{\partial \ell}{\partial \mathrm{vec2h}(\Sigma)} &amp;=  ( \mathrm{vech}(\mathbf{g}_{V}))^T 
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathbf{V}$</code> takes the same value of <code class="language-plaintext highlighter-rouge">$\Sigma$</code>.</p>

        <div class="notice--danger">
          <p>Note:</p>

          <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{vech}(\mathbf{g}_\Sigma) = \mathrm{vec2h}(\mathbf{g}_V)
\end{aligned}
$$</code></p>
        </div>
        <p>We can verify the following identity for <code class="language-plaintext highlighter-rouge">$\mathbf{m}^{(1)}$</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\frac{\partial \ell}{\partial \mathrm{vec2h}(\Sigma)} \frac{\partial \mathrm{vec2h}(\Sigma) }{\partial m^{(1)}_i} &amp;= \mathrm{Trace}\big(\frac{\partial \ell}{\partial \mathbf{V}} \frac{\partial \mathbf{V} }{\partial m^{(1)}_i} \big)
\end{aligned}
$$</code>  where <code class="language-plaintext highlighter-rouge">$\mathbf{V}= \mathbf{m}^{(2)} - \mathbf{m}^{(1)}(\mathbf{m}^{(1)})^T$</code>.</p>

        <p>By the identity, we can easily compute the Euclidean gradients w.r.t. the expectation parameter <code class="language-plaintext highlighter-rouge">$\mathbf{m}= (\mathbf{m}^{(1)}, \mathbf{m}^{(2)})$</code> as</p>

        <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\frac{\partial \ell}{\partial m^{(1)}} &amp;= \mathbf{g}_\mu^T - 2 (\mathbf{m}^{(1)})^T \mathbf{g}_{V}^T = \mathbf{g}_\mu^T - 2 \mu^T \mathbf{g}_{V}^T \\
\frac{\partial \ell}{\partial m^{(2)}} &amp;= \mathbf{0} + ( \mathrm{vech}(\mathbf{g}_{V}))^T = ( \mathrm{vech}(\mathbf{g}_{V}))^T
\end{aligned}
$$</code></p>
      </blockquote>
    </fieldset>
</details>
</div>

<h1 id="natural-gradient-descent-as-unconstrained-mirror-descent">Natural-gradient Descent as Unconstrained Mirror Descent</h1>
<p>We assume natural parametrization <code class="language-plaintext highlighter-rouge">$\eta$</code> is both regular and minimal.
In exponential family cases, we will show that natural-gradient descent as a mirror descent update when the natural parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> is unconstrained.</p>

<p>Since mirror descent is defined by using a Bregman divergence, we first introduce the Bregman divergence.</p>

<h2 id="bregman-divergence">Bregman Divergence</h2>
<p>Given a strictly convex function <code class="language-plaintext highlighter-rouge">$\Phi(\cdot)$</code> in its domain, a Bregman divergence equipped with <code class="language-plaintext highlighter-rouge">$\Phi(\cdot)$</code> is defined as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{B}_\Phi(\mathbf{x},\mathbf{y}):= \Phi(\mathbf{x})- \Phi(\mathbf{y}) - \langle \nabla \Phi(\mathbf{y}), (\mathbf{x}-\mathbf{y}) \rangle
\end{aligned}
$$</code></p>

<p>In particular, the Kullback–Leibler (KL) divergence is a Bregman divergence under natural parametrization <code class="language-plaintext highlighter-rouge">$\eta$</code>:
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\, &amp; \,\mathrm{KL} [p(\mathbf{w}| \eta_1 ) || p(\mathbf{w}|{\color{red}\eta_2})]\\
=&amp; \,E_{p(w|\eta_1)} [ \log \frac{p(\mathbf{w}|\eta_1)} {p(\mathbf{w}|\eta_2)} ] \\
=&amp; \,E_{p(w|\eta_1)} [  \langle \eta_1-\eta_2 , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta(\eta_1) + A_\eta(\eta_2) ] &amp; ( p(\mathbf{w}|\eta) \text{ is an exponential family}) \\
=&amp; \,A_\eta(\eta_2) - A_\eta(\eta_1) - E_{p(w|\eta_1)} [  \langle \eta_2-\eta_1,  \mathbf{T}_\eta (\mathbf{w}) \rangle ] \\
=&amp; \,A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \underbrace{ E_{p(w|\eta_1)} [ \mathbf{T}_\eta (\mathbf{w}) ] }_{ \nabla_\eta A_\eta(\eta_1) }  \rangle    \\
=&amp; \, \mathrm{B}_{A_\eta}( {\color{red} \mathbf{\eta}_2},  \mathbf{\eta}_1 ) &amp; ( A_\eta(\eta) \text{ is strictly convex})
\end{aligned}
$$</code></p>

<p>We denote the expectation parameter as <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code>.
Recall that by the
Legendre transformation, we have <code class="language-plaintext highlighter-rouge">$\mathbf{m}=\nabla_\eta A_\eta(\eta)$</code>, where <code class="language-plaintext highlighter-rouge">$\Omega_m$</code> has been defined <a href="#expectation-parameter-space">here</a>.
Note that we assume natural parameter <code class="language-plaintext highlighter-rouge">$\eta$</code> is minimal. In other words,  <code class="language-plaintext highlighter-rouge">$\nabla_\eta^2 A_\eta(\eta)$</code> is
positive-definite and <code class="language-plaintext highlighter-rouge">$A_\eta(\eta)$</code> is strictly convex in <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code>.</p>

<p>We define <code class="language-plaintext highlighter-rouge">$A_\eta(\mathbf{x}):=+\infty$</code> when <code class="language-plaintext highlighter-rouge">$\mathbf{x} \not \in \Omega_\eta$</code>.
The convex conjugate  of the log-partition function <code class="language-plaintext highlighter-rouge">$A_\eta$</code> is defined as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
A^*_\eta( \mathbf{m}) &amp;:= \sup_{x} \{ \langle \mathbf{x},\mathbf{m} \rangle - A_\eta(\mathbf{x}) \} \\
&amp;= \langle \mathbf{\eta},\mathbf{m} \rangle - A_\eta(\mathbf{\eta})  \,\,\,\, (\text{the supremum attains at } \mathbf{x}=\eta \in \Omega_\eta )\\
\end{aligned}\tag{5}\label{5}
$$</code> where 
the domain of  <code class="language-plaintext highlighter-rouge">$A^*_\eta(\mathbf{m})$</code>  is <code class="language-plaintext highlighter-rouge">$\Omega_m$</code>, and 
<code class="language-plaintext highlighter-rouge">$\eta=\eta( \mathbf{m} )$</code>
should be viewed as a function of <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code>.</p>

<p>When <code class="language-plaintext highlighter-rouge">$\mathbf{m} \in \Omega_m$</code>, we have the following identity, which is indeed another Legendre transformation.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\nabla_{\mathbf{m}} A^*_\eta( \mathbf{m})
&amp;= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - \nabla_{\mathbf{m}} A_\eta(\mathbf{\eta}) \\
&amp;= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - [\nabla_{\mathbf{m}} \eta] \underbrace{ [\nabla_\eta A_\eta(\mathbf{\eta})] }_{ = \mathbf{m}}\\
&amp;= \mathbf{\eta} ,
\end{aligned}\tag{6}\label{6}
$$</code> where <code class="language-plaintext highlighter-rouge">$\eta \in \Omega_\eta$</code> due to <code class="language-plaintext highlighter-rouge">$\eqref{5}$</code>.</p>

<p>The convex conjugate <code class="language-plaintext highlighter-rouge">$A^*_\eta( \mathbf{m})$</code> is strictly convex w.r.t. <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code> since the Hessian <code class="language-plaintext highlighter-rouge">$\nabla_m^2 A^*_\eta( \mathbf{m})$</code>
is positive-definite as shown below.</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})
&amp;= \nabla_{\mathbf{m}} \mathbf{\eta}
\end{aligned}
$$</code></p>

<p>Note that due to <code class="language-plaintext highlighter-rouge">$\eqref{3}$</code>,
the FIM <code class="language-plaintext highlighter-rouge">$\mathbf{F}_\eta(\eta)$</code> under natural parameter <code class="language-plaintext highlighter-rouge">$\mathbf{\eta}$</code>  is the Jacobian matrix <code class="language-plaintext highlighter-rouge">$\mathbf{J}= \nabla_{\eta} \mathbf{m}$</code>
and 
positive-definite.</p>

<p>Therefore, it is easy to see that
<code class="language-plaintext highlighter-rouge">$
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m}) = 
\mathbf{F}^{-1}_\eta(\eta),
$</code> which is 
positive-definite and therefore strictly convex.</p>

<p>By the <a href="/posts/2021/11/Geomopt03/#transformation-rule-for-the-fisher-information-matrix">transformation rule</a> of the FIM,  we have the following relationship.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{F}_{\eta} (\eta) &amp; = \mathbf{J}^T \mathbf{F}_{m}(\mathbf{m}) \mathbf{J} \\
&amp;= \mathbf{F}^T_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) \\
&amp;= \mathbf{F}_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) &amp; (\text{the FIM is symmetric})
\end{aligned}
$$</code>  which implies that
the FIM under expectation parameter <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code> is
<code class="language-plaintext highlighter-rouge">$\mathbf{F}_m(\mathbf{m})=\mathbf{F}^{-1}_\eta(\eta) = \nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})= \nabla_{\mathbf{m}} \mathbf{\eta}$</code>.</p>

<p>Moreover, we have the following identity since by <code class="language-plaintext highlighter-rouge">$\eqref{5}$</code>, <code class="language-plaintext highlighter-rouge">$A_\eta(\eta)=\langle \mathbf{\eta},\mathbf{m} \rangle- A^*_\eta( \mathbf{m}) $</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{B}_{A_\eta}(\mathbf{\eta}_2, {\color{red}\mathbf{\eta}_1 })
&amp;= A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \overbrace{ \nabla_\eta A_\eta(\eta_1) }^{= \mathbf{m}_1} \rangle    \\
&amp;= [  \langle \mathbf{\eta}_2,\mathbf{m}_2 \rangle- A^*_\eta( \mathbf{m}_2) ]   
-[  \langle \mathbf{\eta}_1,\mathbf{m}_1 \rangle- A^*_\eta( \mathbf{m}_1) ]
-\langle \eta_2-\eta_1, \mathbf{m}_1 \rangle \\
&amp;=  A^*_\eta( \mathbf{m}_1) - A^*_\eta( \mathbf{m}_2) -    
\langle \mathbf{m}_1-\mathbf{m}_2, \underbrace{ \eta_2}_{ = \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_2)} \rangle\\
&amp;= \mathrm{B}_{A^*_\eta}( {\color{red} \mathbf{m}_1 },\mathbf{m}_2) &amp; (\text{the order is changed})
\end{aligned}
$$</code></p>

<h2 id="mirror-descent">Mirror Descent</h2>

<p>Now, we give the definition of mirror descent.</p>

<p>Consider the following optimization problem over a convex domain denoted by <code class="language-plaintext highlighter-rouge">$\Omega_\theta$</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\min_{\theta \in \Omega_\theta} \ell_\theta(\mathbf{\theta})
\end{aligned}
$$</code></p>

<p>Given a strictly convex function <code class="language-plaintext highlighter-rouge">$\Phi(\mathbf{\theta})$</code> in the domain , mirror
descent with step-size <code class="language-plaintext highlighter-rouge">$\alpha$</code> is defined as</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{\theta}_{k+1} \leftarrow \arg \min_{x \in \Omega_\theta}\{ \langle \nabla_\theta \ell_\theta(\mathbf{\theta}_k), \mathbf{x}-\mathbf{\theta}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{\Phi}(\mathbf{x},\mathbf{\theta}_k) \}
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathrm{B}_\Phi(\cdot,\cdot)$</code> is a Bregman divergence  equipped  with the strictly convex function <code class="language-plaintext highlighter-rouge">$\Phi(\cdot)$</code>.</p>

<h2 id="natural-gradient-descent-as-mirror-descent">Natural-gradient Descent as Mirror Descent</h2>

<p>To show natural-gradient descent as a mirror descent update, we have to make the following assumption.</p>

<div class="notice--success">
  <p><strong>Additional assumption</strong>:</p>

  <p>Natural parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code>
is unconstrainted (<code class="language-plaintext highlighter-rouge">$\Omega_\eta=\mathcal{R}^K$</code>), where <code class="language-plaintext highlighter-rouge">$K$</code> is the number of entries of parameter array <code class="language-plaintext highlighter-rouge">$\eta$</code>.</p>
</div>

<p>The following example illustrates that the expectation space <code class="language-plaintext highlighter-rouge">$\Omega_m$</code> is constrained even when
<code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> is unconstrained.</p>
<div class="notice--info">
  <details>
<summary>Example: $\Omega_m$ is constrained while $\Omega_\eta$ is unconstrained (click to expand)</summary>
<fieldset class="field-set">
      <blockquote>
        <p>Example: Bernoulli family</p>

        <p>We consider this family as discussed in <a href="#exponential-family">the previous section</a>
 <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \Big| 0&lt;\pi&lt;1 \}$</code></p>

        <p>We re-express it in an exponential form as</p>

        <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \log \frac{\pi}{1-\pi}}_{\eta} , \underbrace{ \mathcal{I}(w=0)}_{T_\eta(w) } \rangle - \log \frac{1}{1-\pi} )
\end{aligned}
$$</code>
where  <code class="language-plaintext highlighter-rouge">$\pi = \frac{\exp(\eta)}{1+ \exp(\eta) } $</code> and<code class="language-plaintext highlighter-rouge">$A_\eta(\mathbf{\eta}) =  \log \frac{1}{1-\pi} = \log(1+\exp(\eta))$</code>.</p>

        <p>The natural parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta= \{ \eta | \eta \in \mathcal{R} \}=\mathcal{R}^1$</code>.</p>

        <p>The corresponding expectation parameter is <code class="language-plaintext highlighter-rouge">$m = E_{q(w|\eta)}[ T_\eta (w) ] = \pi$</code></p>

        <p>The expectation parameter space <code class="language-plaintext highlighter-rouge">$\Omega_m= \{ m| 0&lt;m&lt;1 \}$</code> is a constrained open set in $\mathcal{R}^1$.</p>
      </blockquote>
    </fieldset>
</details>
</div>

<p>Now, consider the following mirror descent in the expectation parameter space <code class="language-plaintext highlighter-rouge">$\Omega_m$</code>, where <code class="language-plaintext highlighter-rouge">$\alpha&gt;0$</code>.</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{x \in \Omega_m}\{ \langle \nabla_m \ell_m(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k) \}
\end{aligned}\tag{7}\label{7}
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathbf{m}_{k} \in \Omega_m$</code>,  <code class="language-plaintext highlighter-rouge">$\nabla_m \ell_m(\mathbf{m}_k):= \nabla_m \ell_\eta (\eta(\mathbf{m}_k))$</code> and the Bregman divergence <code class="language-plaintext highlighter-rouge">$\mathrm{B}_{A^*_\eta}(\cdot,\cdot)$</code> is well-defined
since  <code class="language-plaintext highlighter-rouge">$A^*_\eta$</code> is strictly convex in <code class="language-plaintext highlighter-rouge">$\Omega_m$</code>.
Recall  that <code class="language-plaintext highlighter-rouge">$\Omega_m$</code> can still be constrained.</p>

<div class="notice--success">
  <p><strong>Claim</strong>:</p>

  <p>When <code class="language-plaintext highlighter-rouge">$\Omega_\eta = \mathcal{R}^K$</code>, the solution of <code class="language-plaintext highlighter-rouge">$\eqref{7}$</code> is equivalent to <code class="language-plaintext highlighter-rouge">$\eta_{k+1}  \leftarrow  \eta_k - \alpha \nabla_m \ell_m(\mathbf{m}_k)$</code>.</p>
</div>

<div class="notice--info">
  <details>
<summary>Proof of the claim (click to expand)</summary>
<fieldset class="field-set">
      <blockquote>
        <p>Proof :</p>

        <p>Denote 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
u(\mathbf{x}) &amp;:=\langle \nabla_m \ell_m(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha} [ \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k)] \\
&amp; = \langle \nabla_m \ell_m(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  [A^*_\eta( \mathbf{x}) - A^*_\eta( \mathbf{m}_k) -    
\langle \mathbf{x}-\mathbf{m}_k, \underbrace{ \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_k)}_{ = \eta_k \text{ by } \eqref{6}  } \rangle ],
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathbf{m}_k \in \Omega_m $</code>.</p>

        <p>A stationary point of <code class="language-plaintext highlighter-rouge">$\eqref{7}$</code>,  <code class="language-plaintext highlighter-rouge">$\hat{\mathbf{x}}$</code>,  must satisfy the following
condition.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{0} &amp;= \nabla_x u(\hat{\mathbf{x}}) &amp; (\mathbf{m}_k \text{ is considered to be a constant}) \\
&amp;= \nabla_m \ell_m(\mathbf{m}_k)+ \frac{1}{\alpha}  [ \nabla_x A^*_\eta( \hat{\mathbf{x}})  -  \eta_k ],
\end{aligned}
$$</code> which implies that <code class="language-plaintext highlighter-rouge">$\nabla_x A^*_\eta( \hat{\mathbf{x}}) =  \eta_k - \alpha \nabla_m \ell_m(\mathbf{m}_k)$</code></p>

        <p>We first show that there exists a stationary point in the domain <code class="language-plaintext highlighter-rouge">$(\hat{\mathbf{x}} \in \Omega_m)$</code>.
Let’s denote <code class="language-plaintext highlighter-rouge">$\mathbf{\eta}_{k+1}:= \eta_k - \alpha \nabla_m \ell_m(\mathbf{m}_k)$</code>. Since <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> is unconstrained, it
is obvious that <code class="language-plaintext highlighter-rouge">$\mathbf{\eta}_{k+1} \in \Omega_\eta$</code>.
By <code class="language-plaintext highlighter-rouge">$\eqref{2}$</code>, <code class="language-plaintext highlighter-rouge">$\mathbf{m}(\mathbf{\eta}_{k+1}) =\nabla_\eta A_\eta( \mathbf{\eta}_{k+1}) \in
\Omega_m$</code>. Notice that,  by <code class="language-plaintext highlighter-rouge">$\eqref{6}$</code>, we have  <code class="language-plaintext highlighter-rouge">$\nabla_m A^*_\eta ( \mathbf{m}(\mathbf{\eta}_{k+1}) ) = \mathbf{\eta}_{k+1}$</code>, which implies that <code class="language-plaintext highlighter-rouge">$\mathbf{m}(\mathbf{\eta}_{k+1})$</code> is a stationary point and 
<code class="language-plaintext highlighter-rouge">$\mathbf{m}(\mathbf{\eta}_{k+1}) \in \Omega_m$</code>.</p>

        <p>Moreover,
<code class="language-plaintext highlighter-rouge">$\mathbf{m}(\mathbf{\eta}_{k+1})$</code> is 
the unique  solution of <code class="language-plaintext highlighter-rouge">$\eqref{7}$</code> since <code class="language-plaintext highlighter-rouge">$\nabla_x^2 u(\mathbf{x}) =\frac{1}{\alpha}\nabla_x^2 A^*_\eta(
{\mathbf{x}})$</code> is positive-definite for any <code class="language-plaintext highlighter-rouge">$\mathbf{x} \in \Omega_m$</code> and therefore strictly convex.
In other words, 
<code class="language-plaintext highlighter-rouge">$\mathbf{m}_{k+1} = \mathbf{m}( {\mathbf{\eta}_{k+1}}) $</code>.</p>

        <p>In summary, when <code class="language-plaintext highlighter-rouge">$\Omega_\eta = \mathcal{R}^K$</code>, the unique solution of <code class="language-plaintext highlighter-rouge">$\eqref{7}$</code> is 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\eta_{k+1} &amp; \leftarrow  \eta_k - \alpha \nabla_m \ell_m(\mathbf{m}_k) \\
\mathbf{m}_{k+1} &amp; \leftarrow  \nabla_\eta A_\eta( {\mathbf{\eta}_{k+1}})
\end{aligned}
$$</code></p>
      </blockquote>
    </fieldset>
</details>
</div>

<p>By the claim,
mirror descent of <code class="language-plaintext highlighter-rouge">$\eqref{7}$</code> in <strong>expectation</strong> parameter space <code class="language-plaintext highlighter-rouge">$\Omega_m$</code> is equivalent to
the following update
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha \nabla_m \ell_m(\mathbf{m}_k)
= \eta_k - \alpha\nabla_m \ell_\eta( \underbrace{ \eta(\mathbf{m}_k) }_{= \eta_k}),
\end{aligned}\tag{8}\label{8}
$$</code> which is exactly natural gradient
descent in <strong>natural</strong> parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta=\mathcal{R}^K$</code> since by <code class="language-plaintext highlighter-rouge">$\eqref{4}$</code>, we have <code class="language-plaintext highlighter-rouge">$\nabla_m \ell_m(\mathbf{m}_k) = \nabla_m \ell_\eta( \eta_k)= \mathbf{F}
_\eta^{-1} (\eta_k) \nabla_\eta \ell_\eta(\eta_k)$</code>.</p>

<hr>
<h1 id="references">References</h1>
<p class="bibliography"></p>
<p><span id="johansen1979introduction">[1] S. Johansen, <i>Introduction to the theory of regular exponential families</i> (Institute of Mathematical Statistics, University of Copenhagen, 1979).</span></p>
<p><span id="wainwright2008graphical">[2] M. J. Wainwright &amp; M. I. Jordan, <i>Graphical models, exponential families, and variational inference</i> (Now Publishers Inc, 2008).</span></p>
<p><span id="khan2017conjugate">[3] M. Khan &amp; W. Lin, "Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models," <i>Artificial Intelligence and Statistics</i> (PMLR, 2017), pp. 878–887.</span></p>

<h2 id="footnotes">Footnotes:</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>When the natural parameter <code class="language-plaintext highlighter-rouge">$\eta$</code> is minimal, this Legendre transformation is diffeomorphic since <code class="language-plaintext highlighter-rouge">$\nabla_\eta^2 A_\eta(\eta)$</code> is positive-definite in its domain. In other words, the expectation parameter <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code> is also intrinsic when the natural parameter <code class="language-plaintext highlighter-rouge">$\eta$</code> is minimal. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#exponential-family" class="page__taxonomy-item" rel="tag">Exponential Family</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#information-geometry" class="page__taxonomy-item" rel="tag">Information Geometry</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#natural-gradient-descent" class="page__taxonomy-item" rel="tag">Natural Gradient Descent</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#riemannian-manifold" class="page__taxonomy-item" rel="tag">Riemannian Manifold</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-12-14T00:00:00-08:00">December 14, 2021</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title" data-translate="share_on_label">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Part+V%3A+Efficient+Natural-gradient+Methods+for+Exponential+Family%20informationgeometryML.github.io%2Fposts%2F2021%2F12%2FGeomopt05%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=informationgeometryML.github.io%2Fposts%2F2021%2F12%2FGeomopt05%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=informationgeometryML.github.io%2Fposts%2F2021%2F12%2FGeomopt05%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>



     

  <script src="https://utteranc.es/client.js" repo="informationgeometryML/informationgeometryML.github.io" issue-term="url" label="blog-comments" theme="github-light" crossorigin="anonymous" async>
  </script>





</section>


      
  <nav class="pagination">
    
      <a href="/posts/2021/11/Geomopt04/" class="pagination--pager" title="Part IV: Natural and Riemannian  Gradient Descent
">Previous</a>
    
    
      <a href="/posts/2021/12/Geomopt06/" class="pagination--pager" title="Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/12/Geomopt06/" rel="permalink">Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (Part VI is incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/11/Geomopt04/" rel="permalink">Part IV: Natural and Riemannian  Gradient Descent
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  19 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/11/Geomopt03/" rel="permalink">Part III: Invariance of Natural-Gradients
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  12 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Goal
This blog post focuses on invariant properties of natural-gradients.
We will discuss

  transformation rules of natural-gradients,
  the automatic compu...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/10/Geomopt02/" rel="permalink">Part II: Derivation of Natural-gradients
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  15 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Goal
This blog post focuses on the derivation of natural-gradients, which are known as Riemannian gradients with the Fisher-Rao metric. We will discuss the f...</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow</strong></li>
    
    
    
    
      <li><a href="https://github.com/informationgeometryML"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">© 2022 Information Geometry in ML. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.2.0/js/all.js"></script>












  
    <script src="/assets/js/custom.js"></script>
  
    <script src="/assets/js/math-code.js"></script>
  



  </body>
</html>
