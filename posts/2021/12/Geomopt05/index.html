<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.12.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Part V: Efficient Natural-gradient Methods for Exponential Family - Information Geometry in Machine Learning</title>
<meta name="description" content="Warning: working in Progress (incomplete)">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Information Geometry in Machine Learning">
<meta property="og:title" content="Part V: Efficient Natural-gradient Methods for Exponential Family">
<meta property="og:url" content="/posts/2021/12/Geomopt05/">


  <meta property="og:description" content="Warning: working in Progress (incomplete)">







  <meta property="article:published_time" content="2021-12-14T00:00:00-08:00">





  

  


<link rel="canonical" href="/posts/2021/12/Geomopt05/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Information Geometry in ML",
      "url": "https://github.com/pages/informationgeometryML/informationgeometryML.github.io",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Information Geometry in Machine Learning Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>


<!-- end custom head snippets -->

  </head>

  <body class="layout--single mywide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Information Geometry in Machine Learning</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="/" >Home</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/news/" >News</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="/year-archive/" >Blog Posts</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  

  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Part V: Efficient Natural-gradient Methods for Exponential Family">
    <meta itemprop="description" content="Warning: working in Progress (incomplete)">
    <meta itemprop="datePublished" content="December 14, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Part V: Efficient Natural-gradient Methods for Exponential Family
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  14 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>Warning: working in Progress (incomplete)</p>

<h2 id="goal">Goal</h2>
<p>This blog post should show that we can efficiently implement natural-gradient methods in many cases.</p>

<p>We will give an informal introduction with a focus on high level of ideas.</p>

<h1 id="exponential-family">Exponential Family</h1>
<hr />

<p>An exponential family takes the following (canonical) form as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p(\mathbf{w}|\mathbf{\eta}) = h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta (\mathbf{\eta}) )
\end{aligned}
$$</code> where   <code class="language-plaintext highlighter-rouge">$C_\eta(\eta) :=  \int h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}(\mathbf{w}) \rangle ) d \mathbf{w}
$</code> is the normalization constant.   <code class="language-plaintext highlighter-rouge">$A_\eta(\mathbf{\eta}):=\log C_\eta(\eta)$</code>,  <code class="language-plaintext highlighter-rouge">$h_\eta(\mathbf{w})$</code>, <code class="language-plaintext highlighter-rouge">$\mathbf{T}_\eta(\mathbf{w})$</code>, and <code class="language-plaintext highlighter-rouge">$\eta$</code>  are known as the log-partition function, the base measure, the sufficient statistics, and the <strong>natural</strong> parameter,  respectively.</p>

<p>The parameter space of <code class="language-plaintext highlighter-rouge">$\eta$</code> denoted by <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> is determined so that the normalization constant is well-defined and (strictly) positive.</p>

<p><strong>Regular</strong> natural parametrization <code class="language-plaintext highlighter-rouge">$\eta$</code>: parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> is relatively open.</p>

<p>In this post, we only consider
regular natural parametrizations since commonly used exponential family distributions have a regular natural parametrization.</p>

<p>This natural parametrization is special since the inner product <code class="language-plaintext highlighter-rouge">$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$</code> is <strong>linear</strong> in <code class="language-plaintext highlighter-rouge">$\eta$</code>. As we will discuss later,  this linearity is essential.</p>

<p>Readers should be aware of the following points when using an exponential family.</p>

<ul>
  <li>
    <p>The support of <code class="language-plaintext highlighter-rouge">$\mathbf{w}$</code> should not depend on parametrization <code class="language-plaintext highlighter-rouge">$\eta$</code>.</p>
  </li>
  <li>The base measure and the log-partition function are only unique up to a constant as illustrated by the following example.
    <blockquote>

      <p>Example: Univariate Gaussian as an exponential family:</p>

      <p>Recall that in <a href="/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families">Part I</a>, we consider this family as
<code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&gt;0 \}$</code>, where <code class="language-plaintext highlighter-rouge">$\mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $</code>.</p>

      <p>We re-express it in an exponential form as</p>

      <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p({w}|\mathbf{\eta})  &amp;= \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] \\
&amp;= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2\sigma} \\ \frac{\mu}{\sigma}  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} w^2 \\ w  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ]     )   \\
\end{aligned}
$$</code>
Since <code class="language-plaintext highlighter-rouge">$\sigma= -\frac{1}{2\eta_1} $</code> and <code class="language-plaintext highlighter-rouge">$\mu = -\frac{\eta_2}{2\eta_1}$</code>,  <code class="language-plaintext highlighter-rouge">$A_\eta(\mathbf{\eta}) = \frac{1} {2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ] = \frac{1}{2} [ \log ( 2\pi ) + \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $</code>.</p>

      <p>It is also valid that $ h_\eta({w}) = \frac{1}{\sqrt{2\pi}} $ and 
<code class="language-plaintext highlighter-rouge">$A_\eta(\mathbf{\eta}) =  \frac{1}{2} [  \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $</code>.</p>

      <p>We easily to verify that parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta= \{ (\eta_1,\eta_2) | \eta_1&lt;0 , \eta_2 \in \mathcal{R} \}$</code> is open in $\mathcal{R}^2$.</p>
    </blockquote>
  </li>
  <li>The log-partition function could be differentiable w.r.t. <code class="language-plaintext highlighter-rouge">$\eta$</code> even when <code class="language-plaintext highlighter-rouge">$\mathbf{w}$</code> is discrete.
    <blockquote>

      <p>Example: Bernoulli family as an exponential family:</p>

      <p>Recall that in <a href="/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families">Part I</a>, we consider this family as
 <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \Big| 0&lt;\pi&lt;1 \}$</code></p>

      <p>We re-express it in an exponential form as</p>

      <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;=  \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \\
&amp;=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \log \frac{\pi}{1-\pi}}_{\eta} , \underbrace{ \mathcal{I}(w=0)}_{T_\eta(w) } \rangle - \log \frac{1}{1-\pi} )
\end{aligned}
$$</code>
Since <code class="language-plaintext highlighter-rouge">$\pi = \frac{\exp(\eta)}{1+ \exp(\eta) } $</code> , we have <code class="language-plaintext highlighter-rouge">$A_\eta(\mathbf{\eta}) =  \log \frac{1}{1-\pi} = \log(1+\exp(\eta))$</code>. <br />
We easily to verify that parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta= \{ \eta | \eta \in \mathcal{R} \}$</code> is open in $\mathcal{R}$.</p>
    </blockquote>
  </li>
  <li>An invertiable linear reparametrization could also be a natural parametrization. This is one of the reasons using natural-gradient descent since it is linearly invariant.
    <blockquote>

      <p>For simplicity, let’s assume set <code class="language-plaintext highlighter-rouge">$\Omega_\lambda := \{ \mathbf{U}^{-1} \eta | \eta \in \Omega_\eta \} = \Omega_\eta$</code>, where <code class="language-plaintext highlighter-rouge">$\mathbf{U}$</code> is a constant invertiable matrix.</p>

      <p>Consider a linear reparametrization such as <code class="language-plaintext highlighter-rouge">$\lambda=\mathbf{U}^{-1} \mathbf{\eta}$</code>, parametrization <code class="language-plaintext highlighter-rouge">$\lambda$</code> is also a natural parametrization as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p(\mathbf{w}|\mathbf{\lambda})
&amp;= h_\eta(\mathbf{w}) \exp( \langle \mathbf{U}\mathbf{\lambda} , \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta( \mathbf{U}\mathbf{\lambda}) ) \\
&amp;=  h_\eta(\mathbf{w})\exp( \langle \mathbf{\lambda} , \mathbf{U}^T \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta(\mathbf{U}\mathbf{\lambda}) ) \\
&amp;= h_\lambda(\mathbf{w})  \exp( \langle \mathbf{\lambda} ,  \mathbf{T}_\lambda(\mathbf{w}) \rangle - A_\lambda(\mathbf{\lambda}) ) 
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$h_\lambda(\mathbf{w}):= h_\eta(\mathbf{w})$</code>,  <code class="language-plaintext highlighter-rouge">$\mathbf{T}_\lambda(\mathbf{w}):= \mathbf{U}^T\mathbf{T}_\eta(\mathbf{w})$</code>, and 
<code class="language-plaintext highlighter-rouge">$A_\lambda(\mathbf{\lambda}):= A_\eta(\mathbf{U}\mathbf{\lambda})$</code>.</p>
    </blockquote>
  </li>
</ul>

<h2 id="minimal-parametrizations-of-exponential-family">Minimal Parametrizations of Exponential Family</h2>

<p>Now, we discuss particular parametrizations of an exponential family. We could efficiently compute natural-gradients when
using this class of parametrizations.</p>

<p><strong>Minimal</strong> natural parametrization: the corresponding sufficient stattistics <code class="language-plaintext highlighter-rouge">$\mathcal{T}(\mathbf{w})$</code> is linearly independent.</p>

<p>A regular, minimal, and natural parametrization <code class="language-plaintext highlighter-rouge">$\eta$</code> has many nice properties.</p>

<ul>
  <li>
    <p>It is an intrinstic parametrization as we discussed in
 <a href="/posts/2021/09/Geomopt01/#intrinsic-parameterizations-for-parametric-families">Part I</a>.</p>
  </li>
  <li>
    <p>The parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> is a open set in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>, where
<code class="language-plaintext highlighter-rouge">$K$</code> is the number of entires of this parameter array.</p>
  </li>
  <li>
    <p>The log-partition function <code class="language-plaintext highlighter-rouge">$A_\eta(\eta)$</code> is infinitely differentiable and strictly convex in <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code>.</p>
  </li>
  <li>
    <p>The FIM <code class="language-plaintext highlighter-rouge">$\mathbf{F}_\eta(\eta) = \nabla_\eta^2 A_\eta(\eta)$</code> is positive-definite.</p>
  </li>
</ul>

<p>We will only show the first property in this post. The remaining properties can be found in the literature.
Note that the linearity in the inner product <code class="language-plaintext highlighter-rouge">$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$</code>,  plays a key role in showing these properties.</p>

<p>Recall that a parametrization is intrinstic if 
<code class="language-plaintext highlighter-rouge">$K$</code> partial derivatives 
<code class="language-plaintext highlighter-rouge">$ \{ \partial_{\eta_i} \log p(\mathbf{w}|\eta) \} $</code>  are linearly independent.
Since the parameter space is open in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code> and the log-partition function <code class="language-plaintext highlighter-rouge">$A_\eta(\eta)$</code> is differentiable,  these partial derivatives are well-defined and can be computed as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\partial_{\eta_i} \log p(\mathbf{w}|\eta) = \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle - \partial_{\eta_i}A_\eta(\eta) 
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathbf{e}_i$</code> is an one-hot array which has zero in all entries except the $i$-th entry.</p>

<blockquote>
  <p>Proof by contradiction:</p>

  <p>If these partial derivatives are linearly dependent, there exist a set of non-zero constant <code class="language-plaintext highlighter-rouge">$c_i$</code> such that
<code class="language-plaintext highlighter-rouge">$\sum_i c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta)= 0 $</code>, where the value of <code class="language-plaintext highlighter-rouge">$c_i$</code> does not depent on  <code class="language-plaintext highlighter-rouge">$\mathbf{w}$</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
0 &amp;= \sum_{i=1}^{K} c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta) \\
&amp;= \sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle -   c_i\partial_{\eta_i}A_\eta(\eta) \\
\end{aligned}
$$</code></p>

  <p>Since  <code class="language-plaintext highlighter-rouge">$c_i$</code> is a non-zero constant and its value does not depent on  <code class="language-plaintext highlighter-rouge">$\mathbf{w}$</code>, we must have
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
0 =\sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle ,
\end{aligned}
$$</code> which implies that
the sufficient stattistics <code class="language-plaintext highlighter-rouge">$\mathcal{T}(\mathbf{w})$</code> is linearly dependent. This is a contradiction since <code class="language-plaintext highlighter-rouge">$\eta$</code> is a minimal natural parametrization.</p>
</blockquote>

<p>Now, we give an example of a regular, minimal and natural parametrization.</p>

<blockquote>
  <p>Example: Minimal parametrization for multivate Gaussians</p>

  <p>Consider a $d$-dimensional Gaussian family
We specify a parameterization $\mathbf{\tau}$ of the  family as <code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(\mathbf{w} |\mathbf{\mu},\mathbf{\Sigma}) \Big| \mathbf{\mu} \in \mathcal{R}^d, \mathbf{\Sigma}   \succ \mathbf{0} \}$</code></p>

  <p>We re-express it in an exponential form as</p>

  <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p({w}|\mathbf{\lambda})  &amp;= \frac{1}{\sqrt{ \mathrm{det}( 2\pi \Sigma )} } \exp [- \frac{1}{2} (\mathbf{w}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{w}-\mathbf{\mu})  ] \\
&amp;= \underbrace{ \exp(0) }_{  h_\lambda({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \mathrm{vec}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\lambda} }  ,  \underbrace{\begin{bmatrix} \mathrm{vec}( \mathbf{w} \mathbf{w}^T) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\lambda ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathrm{vec}()$</code> is the vectorization function
and  <code class="language-plaintext highlighter-rouge">$\lambda$</code> is a $(d+d^2)$-dim array.</p>

  <p>Parametrization $\lambda$ is a natural and regular parametrization. However, it is NOT a minimal
parametrization since <code class="language-plaintext highlighter-rouge">$\mathbf{w} \mathbf{w}^T$</code> is symmetric and therefore the sufficient statistics is linearly dependent.
It can be shown that $\Omega_\lambda$ is relatively open but not open in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>, where <code class="language-plaintext highlighter-rouge">$K=d+d^2$</code>.</p>

  <p>A minimal natural parametrization $\eta$ should be defined as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \mathrm{vech}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} \mathrm{vech}( \mathbf{w} \mathbf{w}^T) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
\end{aligned}
$$</code> where <code class="language-plaintext highlighter-rouge">$\mathrm{vech}()$</code> is the <a href="https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization">half-vectorization function</a> 
and  <code class="language-plaintext highlighter-rouge">$\eta$</code> is a $(d+\frac{d(d+1)}{2})$-dim array.
It can be shown that $\Omega_\eta$ is open in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>, where <code class="language-plaintext highlighter-rouge">$K=d+\frac{d(d+1)}{2}$</code>.</p>

  <p>As we discussed in <a href="/posts/2021/10/Geomopt02/#riemannian-steepest-direction">Part II</a>,  <code class="language-plaintext highlighter-rouge">$\mathrm{vech}(\Sigma^{-1})$</code> is an intrinstic parameterization while <code class="language-plaintext highlighter-rouge">$\mathrm{vec}(\Sigma^{-1})$</code> is not.</p>
</blockquote>

<h1 id="efficient-natural-gradient-computation">Efficient Natural-gradient Computation</h1>

<p>In general, natural-gradient computation can be challenging due to the inverse of the Fisher matrix.
In cases of an exponential family,  natural-gradient computation often can be quite efficient without computing the inverse of the Fisher matrix.</p>

<p>We will assume <code class="language-plaintext highlighter-rouge">$\eta$</code> is a reguar, minimal, natural parametrization.
Now, we introduce a dual parametrization <code class="language-plaintext highlighter-rouge">$\mathbf{m} := E_{p(w|\eta)}[ \mathbf{T}_\eta(\mathbf{w}) ] $</code>, which is known as the expectation parametrization.</p>

<p>Recall that in
<a href="/posts/2021/11/Geomopt04/#newtons-method-is-linearly-invariant">Part IV</a>,  we use the identity of the score function. This identity also shows us a connection 
between these two parametrizations.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{0} &amp; = E_{p(w|\eta)} [ \nabla_\eta \log p(\mathbf{w}|\eta) ]\\
&amp;=E_{p(w|\eta)} [ \mathbf{T}_\eta(\mathbf{w}) - \nabla_\eta A_\eta(\eta) ] \\
&amp;= \mathbf{m}  - \nabla_\eta A_\eta(\eta)
\end{aligned}
$$</code>  which is a valid Legendre (dual) transformation since 
<code class="language-plaintext highlighter-rouge">$\nabla_\eta^2 A_\eta(\eta)$</code> is positive-definite in its domain.</p>

<p>Moreover, the FIM of the exponential family under parametrization <code class="language-plaintext highlighter-rouge">$\eta$</code> is
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{F}_\eta(\eta)=\nabla_\eta^2 A_\eta(\eta) = \nabla_\eta \mathbf{m}
\end{aligned}\tag{1}\label{1}
$$</code> which means this FIM is a Jacobian matrix <code class="language-plaintext highlighter-rouge">$\mathbf{J}=\nabla_\eta \mathbf{m}$</code>.</p>

<p>As we discussed in
<a href="/posts/2021/10/Geomopt02/#riemannian-steepest-direction">Part II</a>,
a natural-gradient w.r.t. <code class="language-plaintext highlighter-rouge">$f(\eta)$</code> can be computed as below, where <code class="language-plaintext highlighter-rouge">$\mathbf{g}_\eta=\nabla_\eta f(\eta)$</code> is a Euclidean gradient w.r.t.  natural parameter <code class="language-plaintext highlighter-rouge">$\eta$</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\hat{\mathbf{g}}_\eta &amp; = \mathbf{F}^{-1}_\eta(\eta) \mathbf{g}_\eta \\
&amp;= (\nabla_\eta \mathbf{m} )^{-1} [ \nabla_\eta f(\eta) ] \\
&amp;= [\nabla_{m} \eta ] [ \nabla_\eta f(\eta) ] \\
&amp;=  \nabla_{m} f(\eta)  
\end{aligned}\tag{2}\label{2}
$$</code> where 
<code class="language-plaintext highlighter-rouge">$\nabla_{m} f( \eta )$</code> is a Euclidean gradient w.r.t. expectation parameter <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code> and
<code class="language-plaintext highlighter-rouge">$\eta=\eta( \mathbf{m} )$</code>
can be viewed  as a function of <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code>.</p>

<p>Therefore, natural-gradients w.r.t. natural parameter <code class="language-plaintext highlighter-rouge">$\eta$</code> can be efficinetly computed if we  can  easily compute Euclidean gradients w.r.t. its expectation parameter <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code>.</p>

<p>to do: add the Gaussian example</p>

<h1 id="natural-gradient-descent-as-unconstrained-mirror-descent">Natural-gradient Descent as Unconstrained Mirror Descent</h1>

<p>In the  following sections, we will assume a natural parametrization <code class="language-plaintext highlighter-rouge">$\eta$</code> is also regular and minimal.</p>

<p>Since mirror descent is defined by using a Bregman divergence, we first introduce the Bregman divergence.</p>

<h2 id="bregman-divergence">Bregman Divergence</h2>
<p>Given a strictly convex function <code class="language-plaintext highlighter-rouge">$\Phi(\cdot)$</code> in its domain, a Bregman divergence is defined as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{B}_\Phi(\mathbf{x},\mathbf{y}):= \Phi(\mathbf{x})- \Phi(\mathbf{y}) - \langle \nabla \Phi(\mathbf{y}), (\mathbf{x}-\mathbf{y}) \rangle
\end{aligned}
$$</code></p>

<p>In particular, the Kullback–Leibler (KL) divergence is a Bregman divergence when we use natural parametrization <code class="language-plaintext highlighter-rouge">$\eta$</code>:
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{KL} [p(\mathbf{w}|\eta_1) || p(\mathbf{w}|\eta_2)]
&amp;= E_{p(w|\eta_1)} [ \log \frac{p(\mathbf{w}|\eta_1)} {p(\mathbf{w}|\eta_2)} ] \\
&amp;= E_{p(w|\eta_1)} [  \langle \eta_1-\eta_2 , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta(\eta_1) + A_\eta(\eta_2) ] &amp;\,\,\,\,\, ( p(\mathbf{w}|\eta) \text{ is an exponential family}) \\
&amp;= A_\eta(\eta_2) - A_\eta(\eta_1) - E_{p(w|\eta_1)} [  \langle \eta_2-\eta_1,  \mathbf{T}_\eta (\mathbf{w}) \rangle ] \\
&amp;= A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \underbrace{ E_{p(w|\eta_1)} [ \mathbf{T}_\eta (\mathbf{w}) ] }_{ \nabla_\eta A_\eta(\eta_1) }  \rangle    \\
&amp;=  \mathrm{B}_{A_\eta}(\mathbf{\eta}_2,\mathbf{\eta}_1) &amp;\,\,\,\,\, ( A_\eta(\eta) \text{ is strictly convex})
\end{aligned}
$$</code></p>

<p>We denote the expectation parameter as <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code>.
The convex conjugate (Legendre transformation) of the log-partition function <code class="language-plaintext highlighter-rouge">$A_\eta$</code> is defined as
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
A^*_\eta( \mathbf{m}) &amp;:= \sup_{x} \{ \langle \mathbf{x},\mathbf{m} \rangle - A_\eta(\mathbf{x}) \} \\
&amp;= \langle \mathbf{\eta},\mathbf{m} \rangle - A_\eta(\mathbf{\eta})  \,\,\,\, (\text{the supremum attains at } \mathbf{x}=\eta )\\
\end{aligned}\tag{3}\label{3}
$$</code> where
<code class="language-plaintext highlighter-rouge">$\eta=\eta( \mathbf{m} )$</code>
can be viewed  as a function of <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code>.</p>

<p>Notice that
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\nabla_{\mathbf{m}} A^*_\eta( \mathbf{m})
&amp;= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - \nabla_{\mathbf{m}} A_\eta(\mathbf{\eta}) \\
&amp;= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - [\nabla_{\mathbf{m}} \eta] \underbrace{ [\nabla_\eta A_\eta(\mathbf{\eta})] }_{ = \mathbf{m}}\\
&amp;= \mathbf{\eta}
\end{aligned}
$$</code>.</p>

<p>The convex conjugate <code class="language-plaintext highlighter-rouge">$A^*_\eta( \mathbf{m})$</code> is strictly convex w.r.t. <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code> since the Hessian <code class="language-plaintext highlighter-rouge">$\nabla_m^2 A^*_\eta( \mathbf{m})$</code>
is positive-definite as shown below.</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})
&amp;= \nabla_{\mathbf{m}} \mathbf{\eta}
\end{aligned}
$$</code></p>

<p>Note that due to <code class="language-plaintext highlighter-rouge">$\eqref{1}$</code>,
the FIM <code class="language-plaintext highlighter-rouge">$\mathbf{F}_\eta(\eta)$</code> under natural parameter <code class="language-plaintext highlighter-rouge">$\mathbf{\eta}$</code>  is the Jacobian matrix <code class="language-plaintext highlighter-rouge">$\mathbf{J}= \nabla_{\eta} \mathbf{m}$</code>
and 
positive-definite.</p>

<p>Therefore, it is easy to see that
<code class="language-plaintext highlighter-rouge">$
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m}) = 
\mathbf{F}^{-1}_\eta(\eta),
$</code> which is 
positive-definite and therefore strictly convex.</p>

<p>By the <a href="/posts/2021/11/Geomopt03/#transformation-rule-for-the-fisher-information-matrix">transformation rule</a> of the FIM,  we have the following relationship.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{F}_{\eta} (\eta) &amp; = \mathbf{J}^T \mathbf{F}_{m}(\mathbf{m}) \mathbf{J} \\
&amp;= \mathbf{F}^T_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) \\
&amp;= \mathbf{F}_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) \\
\end{aligned}
$$</code>  which implies that
the FIM under expectation parameter <code class="language-plaintext highlighter-rouge">$\mathbf{m}$</code> is
<code class="language-plaintext highlighter-rouge">$\mathbf{F}_m(\mathbf{m})=\mathbf{F}^{-1}_\eta(\eta) = \nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})= \nabla_{\mathbf{m}} \mathbf{\eta}$</code>.</p>

<p>Moreover, we have the following identity since by <code class="language-plaintext highlighter-rouge">$\eqref{3}$</code>, <code class="language-plaintext highlighter-rouge">$A_\eta(\eta)=\langle \mathbf{\eta},\mathbf{m} \rangle- A^*_\eta( \mathbf{m}) $</code>.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathrm{B}_{A_\eta}(\mathbf{\eta}_2, \color{red}{\mathbf{\eta}_1 })
&amp;= A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \overbrace{ \nabla_\eta A_\eta(\eta_1) }^{= \mathbf{m}_1} \rangle    \\
&amp;= [  \langle \mathbf{\eta}_2,\mathbf{m}_2 \rangle- A^*_\eta( \mathbf{m}_2) ]   
-[  \langle \mathbf{\eta}_1,\mathbf{m}_1 \rangle- A^*_\eta( \mathbf{m}_1) ]
-\langle \eta_2-\eta_1, \mathbf{m}_1 \rangle \\
&amp;=  A^*_\eta( \mathbf{m}_1) - A^*_\eta( \mathbf{m}_2) -    
\langle \mathbf{m}_1-\mathbf{m}_2, \underbrace{ \eta_2}_{ = \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_2)} \rangle\\
&amp;= \mathrm{B}_{A^*_\eta}( \color{red}{ \mathbf{m}_1 },\mathbf{m}_2) &amp;\,\,\,\, (\text{the order is changed})
\end{aligned}
$$</code></p>

<h2 id="mirror-descent-in-the-expectation-space">Mirror Descent in the Expectation Space</h2>
<p>To show natural-gradient descent as unconstrained mirror descent, we have to make the following assumption.</p>

<p><strong>Additional</strong> assumption:
Natural parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> and its
expectation parameter space <code class="language-plaintext highlighter-rouge">$\Omega_{m_\eta}$</code> are both unconstrainted (<code class="language-plaintext highlighter-rouge">$\Omega_\eta=\Omega_{m_\eta} =\mathcal{R}^K$</code>), where <code class="language-plaintext highlighter-rouge">$K$</code> is the number of entries of parameter array <code class="language-plaintext highlighter-rouge">$\eta$</code>.</p>

<p>Now, consider the following mirror descent in the 
<strong>expectation</strong> parameter space <code class="language-plaintext highlighter-rouge">$\Omega_m$</code> as</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{x \in \Omega_m}\{ \langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k) \}
\end{aligned}\tag{4}\label{4}
$$</code> where <code class="language-plaintext highlighter-rouge">$\nabla_m \ell(\mathbf{m}_k):= \nabla_m f(\eta(\mathbf{m}_k))$</code>.</p>

<p>Denote 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
u(\mathbf{x}) &amp;:=\langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha} [ \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k)] \\
&amp; = \langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  [A^*_\eta( \mathbf{x}) - A^*_\eta( \mathbf{m}_k) -    
\langle \mathbf{x}-\mathbf{m}_k, \underbrace{ \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_k)}_{ = \eta_k} \rangle ],
\end{aligned}
$$</code> where $\mathbf{m}_k$ is considered to be fixed.</p>

<p>When <code class="language-plaintext highlighter-rouge">$\Omega_m=\mathcal{R}^k$</code>, the optimal expectation parameter <code class="language-plaintext highlighter-rouge">$\mathbf{m}_{k+1}=\bar{\mathbf{x}}$</code> must satisfy the following expression.</p>

<p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{0} &amp;= \nabla_x u(\bar{\mathbf{x}}) \\
&amp;= \nabla_m \ell(\mathbf{m}_k)+ \frac{1}{\alpha}  [\underbrace{ \nabla_x A^*_\eta( \bar{\mathbf{x}})}_{\eta_{\bar{x}}}  -  \eta_k ],
\end{aligned}
$$</code> which implies that
<code class="language-plaintext highlighter-rouge">$\eta_{\bar{x}} = \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)$</code>.</p>

<p>Note that <code class="language-plaintext highlighter-rouge">$\eta_{\bar{x}}$</code> is the corresponding natural parameter for the expectation
parameter
<code class="language-plaintext highlighter-rouge">$\mathbf{m}_{k+1}$</code>.</p>

<p>Therefore, unconstrained mirror descent in <strong>expectation</strong> parameter space <code class="language-plaintext highlighter-rouge">$\Omega_m$</code> gives 
the following update
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)
= \eta_k - \alpha\nabla_m f( \underbrace{ \eta(\mathbf{m}_k) }_{= \eta_k}),
\end{aligned}\tag{5}\label{5}
$$</code> which is exactly natural gradient
descent in <strong>natural</strong> parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> since by <code class="language-plaintext highlighter-rouge">$\eqref{2}$</code>, we have <code class="language-plaintext highlighter-rouge">$\nabla_m \ell(\mathbf{m}_k) = \nabla_m f( \eta_k)= \mathbf{F}
_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$</code>.</p>

<h1 id="handling-parameter-constraints">Handling Parameter Constraints</h1>
<hr />
<p>Unfortunately, the connection between natural-gradient descent and mirror desecent breaks down when the natural
parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> is constrained.
Since the Legendre transformation is defined in <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> , which implies the dual (expectation) parameter space <code class="language-plaintext highlighter-rouge">$\Omega_m$</code> is constrained in general.</p>

<p>The following example illustrate this point</p>
<blockquote>
  <p>Example: Univariate Gaussian</p>

  <p>we consider this family as discussed in <a href="#exponential-family">the previous section</a>
<code class="language-plaintext highlighter-rouge">$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma&gt;0 \}$</code> with mean $\mu$ and variance $\sigma$.</p>

  <p>It can be re-expressed in an exponential form as</p>

  <p><code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
p({w}|\mathbf{\eta})
&amp;= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2\sigma} \\ \frac{\mu}{\sigma}  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} w^2 \\ w  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ]     )   \\
\end{aligned}
$$</code>
where  <code class="language-plaintext highlighter-rouge">$\sigma= -\frac{1}{2\eta_1} $</code>,  <code class="language-plaintext highlighter-rouge">$\mu = -\frac{\eta_2}{2\eta_1}$</code>, and  <code class="language-plaintext highlighter-rouge">$A_\eta(\mathbf{\eta}) = \frac{1} {2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ] = \frac{1}{2} [ \log ( 2\pi ) + \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $</code>.</p>

  <p>The natural parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta= \{ (\eta_1,\eta_2) | \eta_1&lt;0 , \eta_2 \in \mathcal{R} \}$</code> is a constrained open set in $\mathcal{R}^2$, where $K=2$.</p>

  <p>The corresponding expectation parameter is <code class="language-plaintext highlighter-rouge">$\mathbf{m} = E_{q(w|\eta)}[ \mathbf{T}_\eta (w) ] = [ \mu^2+\sigma , \mu ] $</code>.</p>

  <p>The expectation parameter space <code class="language-plaintext highlighter-rouge">$\Omega_m= \{ (m_1,m_2) | m_1 - m_2^2 &gt;0 , m_2 \in \mathcal{R} \}$</code> is a constrained open set in $\mathcal{R}^2$.</p>
</blockquote>

<p>When <code class="language-plaintext highlighter-rouge">$\Omega_m \neq \mathcal{R}^K$</code>, (constrained) mirror descent in <code class="language-plaintext highlighter-rouge">$\eqref{4}$</code> in general does not give us the same update in <code class="language-plaintext highlighter-rouge">$\eqref{5}$</code> as
natural-gradient descent.</p>

<blockquote>
  <p>Proof by contradiction:</p>

  <p>Suppose when <code class="language-plaintext highlighter-rouge">$\Omega_m \neq \mathcal{R}^K$</code>,  <code class="language-plaintext highlighter-rouge">$\eqref{4}$</code> gives the same update in <code class="language-plaintext highlighter-rouge">$\eqref{5}$</code> in general.</p>

  <p>By the definition of (constrained) mirror descent in <code class="language-plaintext highlighter-rouge">$\eqref{4}$</code>, the expectation parameter must satisfy <code class="language-plaintext highlighter-rouge">$\mathbf{m}_{k+1} \in \Omega_m$</code>.</p>

  <p>Therefore, the corresponding natural parameter must satisfy <code class="language-plaintext highlighter-rouge">$\mathbf{\eta}_{k+1} \in \Omega_\eta$</code>.</p>

  <p>By our hypothesis, <code class="language-plaintext highlighter-rouge">$\mathbf{\eta}_{k+1}$</code> is updated according to <code class="language-plaintext highlighter-rouge">$\eqref{5}$</code> and it must satisfy the natural parameter constraint.</p>

  <p>However, it is obvious that <code class="language-plaintext highlighter-rouge">$\eqref{5}$</code> in general does not satisfy the natural parameter constraint when the step-size $\alpha$ is
large enough since <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code>  is just a proper open subset of <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>.</p>

  <p>This is a contradiction.</p>
</blockquote>

<h2 id="projected-natural-gradient-descent-and-constrained-mirror-descent">Projected Natural-gradient Descent and (Constrained) Mirror Descent</h2>

<p>As we discussed before, natural-gradient descent and mirror desecent in general are <strong>distinct</strong> methods when the natural parameter space <code class="language-plaintext highlighter-rouge">$\Omega_\eta$</code> is constrained.</p>

<p>A straightforward approach from natural-gradient descent is the projected natural-gradient descent.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\eta_{k+1} \leftarrow \arg\min_{z \in \Omega_\eta} \|\eta_k - \alpha
\mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k) -\mathbf{z} \|^2_{ \color{red}{ \mathbf{F}_\eta(\eta_k)} }
\end{aligned}\tag{6}\label{6}
$$</code> where we have to use 
the <a href="/posts/2021/10/Geomopt02/#distance-induced-by-the-fisher-rao-metric">weighted inner product</a> with the FIM highlighted in red.</p>

<p>On the other hand, the constrained mirror descent remains the same as in  <code class="language-plaintext highlighter-rouge">$\eqref{4}$</code>. 
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{x \in \Omega_m}\{ \langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k) \}
\end{aligned}
$$</code>
where 
<code class="language-plaintext highlighter-rouge">$\nabla_m \ell(\mathbf{m}_k) =  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$</code>.</p>

<p>These two methods could be very difficult to solve since <code class="language-plaintext highlighter-rouge">$\Omega_m$</code> can be an arbitrary open subset in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>.</p>

<h2 id="using-an-adaptive-step-size">Using an Adaptive Step-size</h2>

<p>When the step-size <code class="language-plaintext highlighter-rouge">$\alpha$</code> is small enough, the connection between natural-gradient descent and mirror desecent could
still hold.</p>

<p>Therefore, one idea is to use an adaptive step-size to satisfy the parameter constraint at each iteration.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha_k \nabla_m \ell(\mathbf{m}_k)
\end{aligned}\tag{7}\label{7}
$$</code> where 
<code class="language-plaintext highlighter-rouge">$\nabla_m \ell(\mathbf{m}_k) =  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$</code> and the step-size <code class="language-plaintext highlighter-rouge">$\alpha_k$</code> is selected  so that
<code class="language-plaintext highlighter-rouge">$\eta_{k+1} \in \Omega_\eta$</code>.</p>

<p>Since <code class="language-plaintext highlighter-rouge">$\Omega_m$</code> is a open set in <code class="language-plaintext highlighter-rouge">$\mathcal{R}^K$</code>, this update is valid when the step-size <code class="language-plaintext highlighter-rouge">$\alpha_k$</code> is small enough.</p>

<p>However, for a general parameter constraint <code class="language-plaintext highlighter-rouge">$\Omega_m$</code>, this approach can be inefficient due to the selection precedure and will often select an extremally small step-size
<code class="language-plaintext highlighter-rouge">$\alpha_k$</code>,
which greatly slows down the progression of the method.</p>

<h2 id="riemannian-gradient-descent">Riemannian Gradient Descent</h2>

<p>An alternative approach is to use Riemannian gradient descent as we discussed in 
<a href="/posts/2021/11/Geomopt04/#riemannian-gradient-descent-and-its-non-linear-invariance">Part IV</a>, which is a generalization of natural-gradient descent. 
Note that this approach cannot be derived from mirror descent.</p>

<p>To avoid solving the geodeisc ODE to get the manifold exponential map, we could use an (inexact) approximation, which
induces a retraction map.
<code class="language-plaintext highlighter-rouge">$$
\begin{aligned}
\eta_{k+1} \leftarrow \mathrm{Ret}_{\eta_k} (- \alpha  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k) )  
\end{aligned}\tag{8}\label{8}
$$</code></p>

<p>As mentioned in 
<a href="/posts/2021/11/Geomopt04/#natural-gradient-descent-as-inexact-riemannian-gradient-descent">Part IV</a>,
we have to carefully select a retraction map to handle the parameter constraint.</p>

<p>However, for a general parameter constraint <code class="language-plaintext highlighter-rouge">$\Omega_m$</code>, it can be difficult to come out an efficient retraction map to satisfy
the constraint.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#exponential-family" class="page__taxonomy-item" rel="tag">Exponential Family</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#information-geometry" class="page__taxonomy-item" rel="tag">Information Geometry</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#natural-gradient-descent" class="page__taxonomy-item" rel="tag">Natural Gradient Descent</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#riemannian-manifold" class="page__taxonomy-item" rel="tag">Riemannian Manifold</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-12-14T00:00:00-08:00">December 14, 2021</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title" data-translate="share_on_label">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Part+V%3A+Efficient+Natural-gradient+Methods+for+Exponential+Family%20informationgeometryML.github.io%2Fposts%2F2021%2F12%2FGeomopt05%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=informationgeometryML.github.io%2Fposts%2F2021%2F12%2FGeomopt05%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=informationgeometryML.github.io%2Fposts%2F2021%2F12%2FGeomopt05%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>



     

  <script src="https://utteranc.es/client.js"
    repo=informationgeometryML/informationgeometryML.github.io
    issue-term=url
    label=blog-comments
    theme=github-light
    crossorigin= "anonymous"
    async>
  </script>





</section>


      
  <nav class="pagination">
    
      <a href="/posts/2021/11/Geomopt04/" class="pagination--pager" title="Part IV: Natural and Riemannian  Gradient Descent
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/11/Geomopt04/" rel="permalink">Part IV: Natural and Riemannian  Gradient Descent
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  13 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Warning: working in Progress (incomplete)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/11/Geomopt03/" rel="permalink">Part III: Invariance of Natural-Gradients
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  9 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Goal
This blog post should help readers to understand the invariance of natural-gradients.
We will also discuss why the Euclidean steepest direction is NOT i...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/10/Geomopt02/" rel="permalink">Part II: Natural-Gradients Evaluted at one Point
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Goal
This blog post should help readers to understand natural-gradients, which are known as Riemannian gradients with the Fisher-Rao metric.
The main propose...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2021/09/Geomopt01/" rel="permalink">Part I: Manifolds with the Fisher-Rao Metric
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  12 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Goal
This blog post focuses on the Fisher-Rao metric, also known as the Fisher information matrix (FIM). The purpose is to show that,

  The FIM plays an ess...</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow</strong></li>
    
    
    
    
      <li><a href="https://github.com/informationgeometryML"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Information Geometry in ML. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.2.0/js/all.js"></script>












  
    <script src="/assets/js/custom.js"></script>
  
    <script src="/assets/js/translations.js"></script>
  
    <script src="/assets/js/math-code.js"></script>
  



  </body>
</html>

