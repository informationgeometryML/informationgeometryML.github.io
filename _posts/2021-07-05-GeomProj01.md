---
title: 'Structured Natural Gradient Descent (ICML 2021)' 
date: 2021-07-05
permalink: /posts/2021/07/ICML/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Matrix Lie Groups
---

More about this work {% cite lin2021tractable %}: [(Youtube) talk](https://www.youtube.com/watch?v=vEY1ZxDJX8o&t=11s), [ICML paper](https://arxiv.org/abs/2102.07405), [workshop paper](https://arxiv.org/abs/2107.10884),
[poster](/img/poster.pdf) 

# Introduction

Natural-gradient descent (NGD) on structured parameter spaces  is computationally challenging.
We propose a flexible and efficient NGD method to incorporate structures via matrix groups.

Our NGD method 
* generalizes the exponential natural evolutionary strategy {% cite glasmachers2010exponential %} 
* recovers existing  Newton-like algorithms 
* yields new structured 2nd-order methods and adaptive-gradient methods with group-structural invariance {%  cite lin2021structured %}
* gives new NGD to learn structured covariances of Gaussian, Wishart and their mixtures 

Applications of our method:
* deep learning (structured adaptive-gradient), 
* non-convex optimization (structured 2nd-order),
* evolution strategies (gradient-free), 
* variational inference (multimodal density with Monte Carlo gradient).


------  
# NGD for Optimization, Inference, and Search
     	
 
A unified view for problems in optimization, inference, and search
as optimization over a parametric family `$q(w|\tau)$`:
`$$
\begin{aligned}
   \min_{ \tau \in \Omega_\tau } \mathcal{L}(\tau):= \mathrm{E}_{q(\text{w}| \tau )} \big[ \ell(\mathbf{w}) \big] + \gamma \mathrm{E}_{q(\text{w} |\tau )} \big[ \log q(w|\tau) \big] 
\end{aligned} \tag{1}\label{1}
$$`
where `$\mathbf{w}$` is the decision variable,  `$\ell(\mathbf{w})$` is a loss function, `$\Omega_\tau$` is the parameter space of `$q$`, and `$\gamma\ge 0$` is a constant.

Gradient descent and natural-gradient descent  to solve `$\eqref{1}$`:
`$$
\begin{aligned}
\textrm{GD: } &\tau_{t+1} \leftarrow \tau_t - \alpha \nabla_{\tau_t} \mathcal{L}(\tau) \\
\textrm{Standard NGD: } & \tau_{t+1} \leftarrow \tau_t - \beta\,\, \big[ \mathbf{F}_{\tau} (\tau_t) \big]^{-1} \nabla_{\tau_t} \mathcal{L}(\tau)
\end{aligned} 
$$`

Advantages of NGD:
* recovers a Newton-like update for Gaussian family `$q(\mathbf{w}|\mu,\mathbf{S})$` with parameter `$\tau=(\mu,\mathbf{S})$`, mean `$\mu$`, and precision `$\mathbf{S}$`.
`$$
\begin{aligned}
\mu_{t+1}  & \leftarrow \mu_t - \beta \mathbf{S}_{t}^{-1} E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w \ell( \mathbf{w}) \big] } \\
\mathbf{S}_{t+1}  & \leftarrow (1-\beta \gamma)\mathbf{S}_t + \beta  E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) \big] }
\end{aligned} \tag{2}\label{2}
$$`
* is less sensitive to parameter transformations  than GD
* converges faster than GD


<img src="/img/icml2021-fig01.png"  width="500"/>  



Challenges of standard NGD:
* NGD could violate parameterization constraints (e.g., `$\mathbf{S}_{t+1}$` in `$\eqref{2}$` may not be positive-definite)
* Singular Fisher information matrix (FIM) `$\mathbf{F}_{\tau}(\tau)$` of `$q(w|\tau)$`
* Limited precision/covariance structures
* Ad-hoc approximations for cost reductions 
* Complicated and inefficient NG computation 


    

	
# NGD using Local Parameterizations

Our method performs NGD updates in local parameter spaces while maintaining structures via matrix groups in auxiliary parameters. This decoupling enables a tractable NGD that exploits the structures in auxiliary parameter spaces.

 We consider the following three kinds of parameterizations.
* Global (original) parameterization `$\tau$` for `$q(w|\tau)$`
* New auxiliary parameterization `$\lambda$`  with a surjective map: `$\tau= \psi(\lambda)$`
* Local parameterization `$\eta$` for `$\lambda$` at a current value `$\lambda_t$` with a local map:
`$\lambda = \phi_{\lambda_t} (\eta)$`, where  
`$\phi_{\lambda_t}$` is
<span style="color:red"> tight </span> at `$\lambda_t$`: $\lambda_t \equiv \phi_{\lambda_t} (\eta_0)$, and  we assume `$\eta_0 =\mathbf{0}$` to be a relative origin.

<fieldset class="field-set" markdown="1">
<legend class="leg-title"><span style="color:red">Our NGD:</span></legend>
`$$ 
\begin{aligned} 
\lambda_{t+1} & \leftarrow  \phi_{\lambda_t} \big( \overbrace{\eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} }^{ \text{NGD in local space} } \big) \,\,\,\, \textrm{(structure-preserving update in auxiliary space)}\\
\tau_{t+1} & \leftarrow \psi\big( \lambda_{t+1} \big) \,\,\,\, \textrm{(constraint-satisfaction update in global space)}
\end{aligned} 
$$`
</fieldset>
where `$\hat{\mathbf{g}}_{\eta_0}^{(t)}$` is
 the natural-gradient `$\hat{\mathbf{g}}_{\eta_0}^{(t)}$` at `$\eta_0$` tied to `$\lambda_t$`, which is computed by the chain rule,
`$$
\begin{aligned} 
        \hat{\mathbf{g}}_{\eta_0}^{(t)} &=  \color{green} {\mathbf{F}_{\eta}(\eta_0)^{-1} } 
         \,\, \big[ \nabla_{\eta_0} \big[ \psi \circ \phi_{\lambda_t} (\eta) \big]
        \nabla_{\tau_t}\mathcal{L}(\tau) \big]  
\end{aligned} 
$$` where `$\mathbf{F}_{\eta}(\eta_0)$`  is the (exact) FIM for  `$\eta_0$` tied to  `$\lambda_t$`.   

<img src="/img/icml2021-fig03.png"  width="500"/>  | When `$\tau$` space has a local vector-space structure, <br> standard NGD in $\tau$ space is a speical case of our NGD,  <br> where we choose `$\psi$` to be the identity map and `$\phi_{\lambda_t}$` to be a linear map.




# Gaussian Example with Full Precision


Notations:
* `$\mathrm{GL}^{p\times p}$`: Invertible Matrices (General Linear Group),
* `$\mathcal{D}^{p\times p}$`: Diagonal Matrices,
* `$\mathcal{D}_{++}^{p\times p}$`: Diagonal and invertible Matrices (Diagonal Matrix Group),
* `$\mathcal{S}_{++}^{p\times p}$`: (Symmetric) positive-definite Matrices,
* `$\mathcal{S}^{p\times p}$`: Symmetric Matrices.



		
Consider a Gaussian family `$q(w|\mu,\mathbf{S})$` with mean $\mu$ and precision $\mathbf{S}=\Sigma^{-1}$.  

The global, auxiliary, and local parameterizations are:
`$$
\begin{aligned}
        \tau &= \Big\{\mu \in \mathcal{R}^p, \mathbf{S} \in \mathcal{S}_{++}^{p\times p} \Big\},  & \mathbf{S}: \text{positive-definite matrix space } \\
        \lambda & = \Big\{ \mu \in  \mathcal{R}^p , \mathbf{B} \in\mathrm{GL}^{p\times p} \Big\}, &\mathbf{B}: \text{ (closed) matrix Lie group}\\
        \eta &= \Big\{ \delta\in  \mathcal{R}^p, \mathbf{M} \in\mathcal{S}^{p\times p}  \Big\}, & \mathbf{M}: \text{ Lie sub-algebra }
\end{aligned}
$$`


Define `$\mathbf{h}(\mathbf{M}):=\mathbf{I}+\mathbf{M}+\frac{1}{2} \mathbf{M}^2$`. 
Maps `$\psi$` and  `$\phi_{\lambda_t}$` are :
`$$
\begin{aligned}
        \Big\{ \begin{array}{c} \mu \\ \mathbf{S} \end{array} \Big\} = \psi(\lambda) & := \Big \{ \begin{array}{c} \mu \\ \mathbf{B}\mathbf{B}^\top \end{array} \Big \}, \\
       \Big \{ \begin{array}{c} \mu \\ \mathbf{B} \end{array} \Big \} = \phi_{\lambda_t}(\eta) & := \Big \{ \begin{array}{c} \mu_t + \mathbf{B}_t^{-T} \delta \\ \mathbf{B}_t \mathbf{h} (\mathbf{M}) \end{array} \Big \}.
\end{aligned} \tag{3}\label{3}
$$`

 Our NGD update in `$  \lambda $` space is shown below, where we assume $\eta_0=\mathbf{0}$.
`$$
\begin{aligned}
  \Big\{ \begin{array}{c} \mu_{t+1} \\ \mathbf{B}_{t+1} \end{array} \Big\} = \lambda_{t+1} = 
  \phi_{\lambda_t} \big( \eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} \big)
  =\Big\{ \begin{array}{c}  \mu_t - \beta \mathbf{B}_{t}^{-T} \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ \mathbf{B}_t  \mathbf{h}\big(\beta \mathbf{B}_t^{-1}\mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \end{array} \Big\} 
 \end{aligned}
$$`
where <span style="color:red">**tractable**</span> natural-gradient  `$\hat{\mathbf{g}}_{\eta_0}^{(t)}$`  at `$\eta_0=\{\delta_0, \mathbf{M}_0\}$` tied to `$\lambda_t=\{\mu_t,\mathbf{B}_t\}$`:

`$$
\begin{aligned}
    \hat{\mathbf{g}}_{\eta_0}^{(t)} =
 \Big(  \begin{array}{c} \hat{\mathbf{g}}_{\delta_0}^{(t)}\\ \mathrm{vec}(  \hat{\mathbf{g}}_{M_0}^{(t)})\end{array}   \Big)
= \underbrace{  \color{green}{\Big(\begin{array}{cc} \mathbf{I}_p & 0 \\ 0 & 2 \mathbf{I}_{p^2} \end{array} \Big)^{-1}}  }_{ \text{inverse of exact FIM } } \Big[\begin{array}{c}  \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ \mathrm{vec}( -2\mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T}) \end{array} \Big]
\end{aligned}
$$`

Note that `$\mathbf{g}_\mu$` and `$\mathbf{g}_{\Sigma}$` are Euclidean gradients of `$\eqref{1}$` computed via Stein's lemma {% cite lin2019stein %} :
`$$
\begin{aligned}
\mathbf{g}_\mu = \nabla_{\mu}\mathcal{L}(\tau) = E_{q}{ \big[ \nabla_w \ell( \mathbf{w} ) \big] }, \,\,\,\,\,
\mathbf{g}_{\Sigma}  = \nabla_{S^{-1}}\mathcal{L}(\tau)
 = \frac{1}{2}  E_{q}{ \big[ \nabla_w^2 \ell( \mathbf{w}) \big] } - \frac{\gamma}{2} \mathbf{S} 
 \end{aligned} \tag{4}\label{4}
$$`
 

Our update on `$\mathbf{S}_{t+1}=\mathbf{B}_{t+1}\mathbf{B}_{t+1}^T$` and $\mu_{t+1}$ is like update of `$\eqref{2}$` as
`$$
\begin{aligned}
& \mu_{t+1}   = \mu_t - \beta \mathbf{S}_{t}^{-1} E_{q(\text{w}|\tau_t)}{ \big[ \nabla_w \ell( \mathbf{w} ) \big] } \\
&\mathbf{S}_{t+1} =   \underbrace{ \overbrace{(1-\beta \gamma)\mathbf{S}_t + \beta  E_{q(w|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) \big] }}^{\text{standard NGD on $\mathbf{S}$ }} + \color{red}{ \frac{\beta^2}{2} \mathbf{G}_t \mathbf{S}_t^{-1}\mathbf{G}_t}
}_{\color{red}{\text{ RGD with retraction}}}+ O(\beta^3)
 \end{aligned}
$$` where $\mathbf{B}$ is a <span style="color:red">**dense**</span> matrix in matrix group `$\mathrm{GL}^{p\times p}$` and `$\mathbf{G}_t := E_{q(w|\tau_t)}{ \big[ \nabla_w^2 \ell(\mathbf{w}) ] } -\gamma \mathbf{S}_t$`.

The second-order term shown in red is used for the positive-definite constraint {% cite  lin2020handling %} known as a retraction in Riemannian gradient descent (RGD).  The higher-order term `$O(\beta^3)$` will be used for structured precision in the next section. 

Well-known structures in matrix $\mathbf{B}$ are illustrated in the following figure. 

 Dense     | Diagonal | Triangular
:-------------------------:|:-------------------------::-------------------------:
<img src="/img/icml2021-group-full.png"  width="250"/> | <img src="/img/icml2021-group-diag.png"  width="250"/> |  <img src="/img/icml2021-group-tri.png"  width="250"> 


# Gaussian Example with Flexiable Structured Precision

More flexiable structures in matrix $\mathbf{B}$:

Block lower-triangular | Block upper-triangular | Hierarchical | Kronecker product | Triangular-Toeplitz
:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:
<img src="/img/icml2021-group-low.png"  width="250"/> |  <img src="/img/icml2021-group-up.png"  width="250"/> |  <img src="/img/icml2021-group-hie.png"  width="250"/> |  <img src="/img/icml2021-group-kro.png"  width="250"/>  |  <img src="/img/icml2021-group-tri-Toep.png"  width="250"/> 

A structured Gaussian example: 

>Auxiliary  parameter $\mathbf{B}$ lives in  a structured space (matrix Lie group):  `${\cal{B}_{\text{up}}}(k)$`,
>a <span style="color:red">block upper-triangular</span> sub-group of `$\mathrm{GL}^{p \times p}$`;
>
>`$$
\begin{aligned}
{\cal{B}_{\text{up}}}(k)  := \Big\{ 
\begin{bmatrix}
\mathbf{B}_A &  \mathbf{B}_B  \\
 \mathbf{0} & \mathbf{B}_D
      \end{bmatrix} \Big| & \mathbf{B}_A \in \mathrm{GL}^{k \times k},\,
 \mathbf{B}_D  \in{\cal D}^{(p-k) \times (p-k)}_{++}  \Big\},\,\,
 \end{aligned}
$$`
>
>When `$k=0$`, the space `${\cal{B}_{\text{up}}}(0) = {\cal D}^{p \times p}_{++}$` becomes  the diagonal case.
>When `$k=p$`, `${\cal{B}_{\text{up}}}(p) = \mathrm{GL}^{p\times p}$` becomes the dense case.
>
>Consider a local parameter space (Lie sub-algebra): `${\cal{M}_{\text{up}}}(k)$`.
>
>`$$
\begin{aligned}
{\cal{M}_{\text{up}}}(k):  = \Big\{ 
\begin{bmatrix}
\mathbf{M}_A &  \mathbf{M}_B  \\
 \mathbf{0} & \mathbf{M}_D
      \end{bmatrix} \Big| &  \mathbf{M}_A \in{\cal S}^{k \times k}, \,
 \mathbf{M}_D  \in{\cal D}^{(p-k) \times (p-k)} \Big\}
 \end{aligned}
$$`
>
>
>The global, auxiliary, and local parameterizations :
>`$$
\begin{aligned}
       \tau &= \Big\{\mu \in \mathcal{R}^p,  \mathbf{S}=\mathbf{B} \mathbf{B}^T \in \mathcal{S}_{++}^{p\times p} | \mathbf{B} \in {\cal{B}_{\text{up}}}(k) \Big\}, \\
        \lambda & = \Big\{ \mu \in \mathcal{R}^p,  \mathbf{B} \in {\cal{B}_{\text{up}}}(k) \Big\},\\
        \eta &= \Big\{ \delta\in \mathcal{R}^p,  \mathbf{M} \in {\cal{M}_{\text{up}}}(k)   \Big\}.
 \end{aligned}
$$`
>
>Maps `$\psi$` and `$\phi_{\lambda_t}$` are defined in  `$\eqref{3}$`.
> Our NGD update in the auxiliary space is shown below, where we assume $\eta_0=\mathbf{0}$.
> <fieldset class="field-set" markdown="1">
><legend class="leg-title"><span style="color:red">Structure-preserving update in $\lambda$ space</span></legend>
>`$$
\begin{aligned}
  \Big\{ \begin{array}{c} \mu_{t+1} \\ \mathbf{B}_{t+1} \end{array} \Big\} = \lambda_{t+1} = 
  \phi_{\lambda_t} \big( \eta_0-\beta \hat{\mathbf{g}}_{\eta_0}^{(t)} \big)
  =\Big\{ \begin{array}{c}  \mu_t - \beta \mathbf{B}_{t}^{-T} \mathbf{B}_t^{-1} \mathbf{g}_{\mu_t} \\ 
\mathbf{B}_t  \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \kappa_{\text{up}}\big( 2 \mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \Big)  \end{array} \Big\}   
 \end{aligned}
$$`
></fieldset>
>where `$\odot$` is the elementwise product ,
> `$\kappa_{\text{up}}(\mathbf{X}) \in {\cal{M}_{\text{up}}}(k)$` extracts non-zero entries of `${\cal{M}_{\text{up}}}(k)$` from `$\mathbf{X}$`, and
>`$ \mathbf{C}_{\text{up}} = 
 \begin{bmatrix}
\frac{1}{2} \mathbb{1} &  \mathbb{1}   \\
 \mathbf{0} & \frac{1}{2} \mathbf{I}_D
      \end{bmatrix}  \in {\cal{M}_{\text{up}}}(k)$`.
>	
>Note that `$ \mathbf{B}_{t+1} \in {\cal{B}_{\text{up}}}(k)$` thanks to the property of `$\cal{M}_{\text{up}}(k)$`:
>`$\mathbf{h}\big(\mathbf{M}\big) \in {\cal{B}_{\text{up}}}(k) $` for any `$\mathbf{M} \in \cal{M}_{\text{up}}(k)$`.


In summary, our NGD method:
* is a systemic approach to incorporate structures 
* induces exact and non-singular FIMs 


Existing NG approach for rank-one covariance   | Our NGD for rank-one covariance 
:-------------------------:|:-------------------------:
 <img src="/img/icml2021-fig02.png"  width="500"/> |  <img src="/img/icml2021-fig04.png"  width="500"/>


    

# Applications

## Structured 2nd-order Methods for Non-convex Optimization    
   
Given an optimization problem
`$$
\begin{aligned}
\min_{\mu \in \mathcal{R}^p} \ell(\mu),
 \end{aligned}\tag{5}\label{5}
$$` 

we formulate a new problem over Gaussian `$q(\mathbf{w}|\tau)$` with structured precision, which is a speical case of `$\eqref{1}$` with `$\gamma=1$`.
`$$
\begin{aligned}
   \min_{\tau \in \Omega_\tau} E_{q(w|\tau)} \big[ \ell(\mathbf{w}) \big] +  E_{q(w|\tau)} \big[ \log q(\mathbf{w}|\tau)\big],
 \end{aligned}\tag{6}\label{6}
$$` where `$\mathbf{B} \in {\cal{B}_{\text{up}}}(k)$` is a block upper-triangular group member,  $\tau=(\mu,\mathbf{S})$ with mean $\mu$ and precision matrix $\mathbf{S}=\mathbf{B}\mathbf{B}^T$.


Using our NGD to solve `$\eqref{6}$`
* gives the following update
`$$
\begin{aligned}
\mu_{t+1}  & \leftarrow \mu_{t} - \beta \mathbf{S}_t^{-1} \mathbf{g}_{\mu_t},\\
\mathbf{B}_{t+1}  & \leftarrow   \mathbf{B}_t \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \kappa_{\text{up}}\big( 2 \mathbf{B}_t^{-1} \mathbf{g}_{\Sigma_t} \mathbf{B}_t^{-T} \big) \Big)
 \end{aligned}
$$` 
* obtains an update to solve `$\eqref{5}$` with group-structural invariance:
`$$
\begin{aligned}
   \mu_{t+1} &  \leftarrow  \mu_t - \beta \mathbf{S}_{t}^{-1} \nabla_{\mu_t} \ell( \mu), \\ 
  \mathbf{B}_{t+1} & \leftarrow 
     \mathbf{B}_t \mathbf{h} \Big( \beta \mathbf{C}_{\text{up}} \odot \color{red}{\kappa_{\text{up}}\big(  \mathbf{B}_t^{-1} \nabla_{\mu_t}^2 \ell( \mu)  \mathbf{B}_t^{-T} - \mathbf{I} \big)} \Big)
 \end{aligned}\tag{7}\label{7}
$$` by using `$\eqref{4}$` evaluated at the mean `$\mu_t$`
`$$
\begin{aligned}
\mathbf{g}_{\mu_t} \approx \nabla_{\mu_t} \ell( \mu),\,\,\,\,
 \mathbf{g}_{\Sigma_t} \approx \frac{1}{2} \big[ \nabla_{\mu_t}^2 \ell( \mu) - \mathbf{S}_t\big].
 \end{aligned}\tag{8}\label{8}
$$` where $\Sigma=\mathbf{S}^{-1}$ is the covariance.



<details>
	<summary>Group-structural invariance:</summary>
<fieldset class="field-set" markdown="1">
Since `$\mathbf{B} \in {\cal{B}_{\text{up}}}(k)$`,  the update in `$\eqref{7}$` is invariant under any (group) transform `$\mathbf{R}^T \in  {\cal{B}_{\text{up}}}(k)$` of  `$\eqref{5}$`  such as `$\min_{y \in \mathcal{R}^p}  \ell(\mathbf{R} \, \mathbf{y})$`.
</fieldset>
</details>




<details>
	<summary>Time complexity:</summary>
<fieldset class="field-set" markdown="1">
* `$O(k^2 p)$` for triangular structure,
* `$O( (k_1^2+k_2^2) p)$` for hierarchical structure.

Implementation using Hessian-vector products (HVPs);
* Off-diagonal: `$k$` HVPs (triangular), `$(k_1+k_2)$` HVPs (hierarchical),
* Diagonal: compute/approximate diagonal entries of `$\nabla_{\mu_t}^2 \ell( \mu)$`.
</fieldset>
</details>


<details>
	<summary>Classical non-convex optimization:</summary>
<fieldset class="field-set" markdown="1">
200-dim non-separable non-convex functions:

 <img src="/img/icml2021-rbfun.png"  width="500"/> |  <img src="/img/icml2021-dpfun.png"  width="500"/>
</fieldset>
</details>



    
## Structured Adaptive-gradient Methods for Deep Learning

Consider a block-diagonal Gaussian 
       `$q(\mathbf{w}|\tau)=\prod_{i} q(\mathbf{w}^i|\mu^i,\mathbf{S}^i)$` with a Kronecker product group at each `$i$`-th layer. 

Our method gives adaptive-gradient updates with group-structural invariance by
 approximating `$\nabla_{\mu_t}^2 \ell( \mu)$`  in `$\eqref{8}$` using the Gauss-Newton.

The Kronecker product of two structured groups further reduces the time complexity.

<details>
	<summary>Time complexity:</summary>
<fieldset class="field-set" markdown="1">
* `$O(k p)$` for triangular structure,
* `$O( (k_1+k_2) p)$` for hierarchical structure.

Implementation:
* Automatically parallelized by Auto-Differentiation,
* No sequential conjugate-gradient (CG) steps.
</fieldset>
</details>



<details>
	<summary>Image classification problems:</summary>
<fieldset class="field-set" markdown="1">
Kronecker product of lower-triangular groups for a CNN

<img src="/img/icml2021-stl10.png"  width="500"/> |  <img src="/img/icml2021-cifar10.png"  width="500"/>
</fieldset>
</details>






##  Variational Inference with Gaussian Mixtures

Our NGD 
* can be applied to structured mixture: `$q(\mathbf{w}|\tau)=\frac{1}{K}\sum_{k=1}^{K}q(\mathbf{w}|\mu_k,\mathbf{S}_k)$`
* obtains stochastic natural-gradient variational inference with structured Gaussian mixtures approximations using MC gradients
      
<details>
	<summary>Approximating multimodal distributions (80-dim mixture of Student's Ts):</summary>
<fieldset class="field-set" markdown="1">
First 8 marginal distributions of our approximation (mixture with a block upper-triangular group, where `$k=5$`)

<img src="/img/icml2021-tmm80d-01.png"  width="500"/> |  <img src="/img/icml2021-tmm80d-02.png"  width="500"/>
</fieldset>
</details>








------
# References
{% bibliography --cited %}

