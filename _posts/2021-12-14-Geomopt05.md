---
title: 'Part V: Efficient Natural-gradient Methods for Exponential Family'
date: 2021-12-14
permalink: /posts/2021/12/Geomopt05/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
  - Exponential Family
---

Warning: working in Progress (incomplete)

Goal
------
This blog post should show that we can efficiently implement natural-gradient methods in many cases.

We will give an informal introduction with a focus on high level of ideas.


# Exponential Family
------

An expoential family takes the following (canonical) form as
`$$
\begin{aligned}
p(\mathbf{w}|\mathbf{\eta}) = h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta (\mathbf{\eta}) )
\end{aligned}
$$` where   `$C_\eta(\eta) :=  \int h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}(\mathbf{w}) \rangle ) d \mathbf{w}
$` is the normalization constant.   `$A_\eta(\mathbf{\eta}):=\log C_\eta(\eta)$`,  `$h_\eta(\mathbf{w})$`, `$\mathbf{T}_\eta(\mathbf{w})$`, and `$\eta$`  are known as the log-partition function, the base measure, the sufficient statistics, and the **natural** parameter,  respectively.

The parameter space of `$\eta$` denoted by `$\Omega_\eta$` is determined so that the normalization constant is well-defined and (strictly) positive.

**Regular** natural parametrization `$\eta$`: parameter space `$\Omega_\eta$` is relatively open.

 In this post, we only consider
regular natural parametrizations since commonly used expoential family distributions have a regular natural parametrization.

This natural parametrization is special since the inner product `$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$` is **linear** in `$\eta$`. As we will discuss later,  this linearity is essential.

* The support of `$\mathbf{w}$` should not depend on parametrization `$\eta$`.

* The base measure and the log-partition function are only unique up to a constant as illustrated by the following example. 
    >
>Example: Univariate Gaussian as an expoential family:
>
> Recall that in [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families), we consider this family as
>`$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$`, where `$\mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $`.
>
> We re-express it in an expoential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})  &= \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] \\
>&= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2\sigma} \\ \frac{\mu}{\sigma}  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} w^2 \\ w  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ]     )   \\
>\end{aligned}
>$$`
> Since `$\sigma= -\frac{1}{2\eta_1} $` and `$\mu = -\frac{\eta_2}{2\eta_1}$`,  `$A_\eta(\mathbf{\eta}) = \frac{1} {2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ] = \frac{1}{2} [ \log ( 2\pi ) + \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $`.
>
> It is also valid that $ h_\eta({w}) = \frac{1}{\sqrt{2\pi}} $ and 
> `$A_\eta(\mathbf{\eta}) =  \frac{1}{2} [  \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $`.
>
> We easily to verify that parameter space `$\Omega_\eta= \{ (\eta_1,\eta_2) | \eta_1<0 , \eta_2 \in \mathcal{R} \}$` is open in $\mathcal{R}^2$.

* The log-partition function could be differentiable w.r.t. `$\eta$` even when `$\mathbf{w}$` is discrete.
    >
>Example: Bernoulli family as an expoential family:
>
> Recall that in [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families), we consider this family as
>  `$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \Big| 0<\pi<1 \}$`
>
> We re-express it in an expoential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})
>&=  \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \\
>&=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \log \frac{\pi}{1-\pi}}_{\eta} , \underbrace{ \mathcal{I}(w=0)}_{T_\eta(w) } \rangle - \log \frac{1}{1-\pi} )
>\end{aligned}
>$$`
> Since `$\pi = \frac{\exp(\eta)}{1+ \exp(\eta) } $` , we have `$A_\eta(\mathbf{\eta}) =  \log \frac{1}{1-\pi} = \log(1+\exp(\eta))$`. <br />
> We easily to verify that parameter space `$\Omega_\eta= \{ \eta | \eta \in \mathcal{R} \}$` is open in $\mathcal{R}$.

* A natural parametrization is only unique up to an invertiable linear transformation.
    This is one of the reasons using natural-gradient descent since it is linearly invariant.
    >
> For simplicity, let's assume set `$\Omega_\lambda := \{ \mathbf{U}^{-1} \eta | \eta \in \Omega_\eta \} = \Omega_\eta$`, where `$\mathbf{U}$` is a constant invertiable matrix. 
>
>Consider a linear reparametrization such as `$\lambda=\mathbf{U}^{-1} \mathbf{\eta}$`, parametrization `$\lambda$` is also a natural parametrization as
>`$$
>\begin{aligned}
>p(\mathbf{w}|\mathbf{\lambda})
>&= h_\eta(\mathbf{w}) \exp( \langle \mathbf{U}\mathbf{\lambda} , \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta( \mathbf{U}\mathbf{\lambda}) ) \\
>&=  h_\eta(\mathbf{w})\exp( \langle \mathbf{\lambda} , \mathbf{U}^T \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta(\mathbf{U}\mathbf{\lambda}) ) \\
>&= h_\lambda(\mathbf{w})  \exp( \langle \mathbf{\lambda} ,  \mathbf{T}_\lambda(\mathbf{w}) \rangle - A_\lambda(\mathbf{\lambda}) ) 
>\end{aligned}
>$$` where `$h_\lambda(\mathbf{w}):= h_\eta(\mathbf{w})$`,  `$\mathbf{T}_\lambda(\mathbf{w}):= \mathbf{U}^T\mathbf{T}_\eta(\mathbf{w})$`, and 
>`$A_\lambda(\mathbf{\lambda}):= A_\eta(\mathbf{U}\mathbf{\lambda})$`.




## Minimal Parametrizations of Exponential Family

Now, we discuss particular parametrizations of an expoential family.

 **Minimal** natural parametrization: the corresponding sufficient stattistics `$\mathcal{T}(\mathbf{w})$` is linearly independent.


A regular, minimal, and natural parametrization `$\eta$` has many nice properties.

* It is an intrinstic parametrization as we discussed in
 [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families).

* The parameter space `$\Omega_\eta$` is open in `$\mathcal{R}^K$`, where
`$K$` is the number of entires of this parameter array.

* The log-partition function `$A_\eta(\eta)$` is  infinitely differentiable and strictly convex in `$\Omega_\eta$`.

* The FIM `$\mathbf{F}_\eta(\eta) = \nabla_\eta^2 A_\eta(\eta)$` is positive-definite.

We will only show the first property in this post. The remaining properties can be found in the literature.
Note that the linearity in the inner product `$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$`,  plays a key role in showing these properties.


Recall that a parametrization is intrinstic if 
 partial derivatives 
`$ \{ \partial_{\eta_i} \log p(\mathbf{w}|\eta) \} $`  are linearly independent.
Since the parameter space is open in `$\mathcal{R}^K$` and the log-partition function `$A_\eta(\eta)$` differentiable, we know that these partial derivatives  as
`$$
\begin{aligned}
\partial_{\eta_i} \log p(\mathbf{w}|\eta) = \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle - \partial_{\eta_i}A_\eta(\eta) 
\end{aligned}
$$` where `$\mathbf{e}_i$` is an one-hot array which has zero in all entries except the $i$-th entry.

> Proof by contradiction:
>
>If these partial derivatives are linearly dependent, there exist a set of non-zero constant `$c_i$` such that
>`$\sum_i c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta)= 0 $`, where the value of `$c_i$` does not depent on  `$\mathbf{w}$`.
>`$$
>\begin{aligned}
>0 &= \sum_{i=1}^{K} c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta) \\
>&= \sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle -   c_i\partial_{\eta_i}A_\eta(\eta) \\
>\end{aligned}
>$$`
>
>Since  `$c_i$` is a non-zero constant and its value does not depent on  `$\mathbf{w}$`, we must have
> `$$
>\begin{aligned}
> 0 =\sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle ,
>\end{aligned}
> $$` which implies that
>the sufficient stattistics `$\mathcal{T}(\mathbf{w})$` is linearly dependent. This is a contradiction since `$\eta$` is a minimal natural parametrization.

Now, we give an example of a minimal natural parametrization.

>Example: Minimal parametrization for multivate Gaussians
>
>Consider a $d$-dimensional Gaussian family
>We specify an intrinsic parameterization $\mathbf{\tau}$ of the  family as `$ \{ \mathcal{N}(\mathbf{w} |\mathbf{\mu},\mathbf{\Sigma}) \Big| \mathbf{\mu} \in \mathcal{R}^d, \mathbf{\Sigma}   \succ \mathbf{0} \}$`
>
> We re-express it in an expoential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\lambda})  &= \frac{1}{\sqrt{ \mathrm{det}( 2\pi \Sigma )} } \exp [- \frac{1}{2} (\mathbf{w}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{w}-\mathbf{\mu})  ] \\
>&= \underbrace{ \exp(0) }_{  h_\lambda({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \mathrm{vec}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\lambda} }  ,  \underbrace{\begin{bmatrix} \mathrm{vec}( \mathbf{w} \mathbf{w}^T) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\lambda ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
>\end{aligned}
>$$` where `$\mathrm{vec}()$` is the vectorization function
>and  `$\lambda$` is a $(d+d^2)$-dim array.
>
> Parametrization $\lambda$ is a natural and regular parametrization. However, it is NOT a minimal
>parametrization since `$\mathbf{w} \mathbf{w}^T$` is symmetric and therefore the sufficient statistics is linearly dependent.
> It can be shown that $\Omega_\lambda$ is relatively open but not open in `$\mathcal{R}^K$`, where `$K=d+d^2$`.
>
> A minimal natural parametrization $\eta$ should be
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})
>&= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \mathrm{vech}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} \mathrm{vech}( \mathbf{w} \mathbf{w}^T) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
>\end{aligned}
>$$` where `$\mathrm{vech}()$` is the [half-vectorization function](https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization) 
>and  `$\eta$` is a $(d+\frac{d(d+1)}{2})$-dim array.
> It can be shown that $\Omega_\eta$ is open in `$\mathcal{R}^K$`, where `$K=d+\frac{d(d+1)}{2}$`.
>
> As we discussed in [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-steepest-direction),  `$\eta$` is an intrinstic parameterization while `$\lambda$` is not.


# Efficient Natural-gradient Computation

In general, natural-gradient computation can be challenging due to the inverse of the Fisher matrix.
In cases of an expoential family,  natural-gradient computation often can be quite efficient without computing the inverse of the Fisher matrix.


We will assume `$\eta$` is a reguar, minimal, natural parametrization.
Now, we introduce the dual parametrization `$\mathbf{m}_\eta := E_{p(w|\eta)}[ \mathbf{T}_\eta(\mathbf{w}) ] $`, which is known as the expectation parametrization.

Recall that in
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#newtons-method-is-linearly-invariant),  we use the identity of the score function. This identity also shows us a connection 
between these two parametrizations.
`$$
\begin{aligned}
\mathbf{0} & = E_{p(w|\eta)} [ \nabla_\eta \log p(\mathbf{w}|\eta) ]\\
&=E_{p(w|\eta)} [ \mathbf{T}_\eta(\mathbf{w}) - \nabla_\eta A_\eta(\eta) ] \\
&= \mathbf{m}_\eta  - \nabla_\eta A_\eta(\eta)
\end{aligned}
$$`  which is a valid  Legendre transformation since 
`$\nabla_\eta^2 A_\eta(\eta)$` is positive-definite in its domain.

Moreover, the FIM under parametrization `$\eta$` is
`$$
\begin{aligned}
\mathbf{F}_\eta(\eta)=\nabla_\eta^2 A_\eta(\eta) = \nabla_\eta \mathbf{m}_\eta
\end{aligned}\tag{1}\label{1}
$$` which means this FIM is a Jacobian matrix `$\mathbf{J}=\nabla_\eta \mathbf{m}_\eta$`.

As we discussed in
[Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-steepest-direction),
a natural-gradient w.r.t. `$f(\eta)$` can be computed as below, where `$\mathbf{g}_\eta=\nabla_\eta f(\eta)$` is a Euclidean gradient w.r.t.  natural parameter `$\eta$`.
`$$
\begin{aligned}
\hat{\mathbf{g}}_\eta & = \mathbf{F}^{-1}_\eta(\eta) \mathbf{g}_\eta \\
&= (\nabla_\eta \mathbf{m}_\eta )^{-1} [ \nabla_\eta f(\eta) ] \\
&= [\nabla_{m_\eta} \eta ] [ \nabla_\eta f(\eta) ] \\
&=  \nabla_{m_\eta} f(\eta)  
\end{aligned}\tag{2}\label{2}
$$` where 
`$\nabla_{m_\eta} f( \eta )$` is a Euclidean gradient w.r.t. expectation parameter `$\mathbf{m}_\eta$` and
`$\eta=\eta( \mathbf{m}_\eta )$`
can be viewed  as a function of `$\mathbf{m}_\eta$`.

Therefore, natural-gradients w.r.t. natural parameter `$\eta$` can be efficinetly computed if we  can  easily compute Euclidean gradients w.r.t. its expectation parameter `$\mathbf{m}_\eta$`.


to do: add the Gaussian example

# Natural-gradient Descent as Unconstrained Mirror Descent
We will only consider a regular, minimal, and natural parametrization `$\eta$`.

## Bregman Divergence
Given a strictly convex function `$\Phi(\cdot)$` in its domain, a Bregman divergence is defined as
`$$
\begin{aligned}
\mathrm{B}_\Phi(\mathbf{x},\mathbf{y}):= \Phi(\mathbf{x})- \Phi(\mathbf{y}) - \langle \nabla \Phi(\mathbf{y}), (\mathbf{x}-\mathbf{y}) \rangle
\end{aligned}
$$`

In particular, the Kullback–Leibler (KL) divergence is a Bregman divergence when we use natural parametrization `$\eta$`:
`$$
\begin{aligned}
\mathrm{KL} [p(\mathbf{w}|\eta_1) || p(\mathbf{w}|\eta_2)]
&= E_{p(w|\eta_1)} [ \log \frac{p(\mathbf{w}|\eta_1)} {p(\mathbf{w}|\eta_2)} ] \\
&= E_{p(w|\eta_1)} [  \langle \eta_1-\eta_2 , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta(\eta_1) + A_\eta(\eta_2) ] \,\,\,\, ( p(\mathbf{w}|\eta) \text{ is an expoential family}) \\
&= A_\eta(\eta_2) - A_\eta(\eta_1) - E_{p(w|\eta_1)} [  \langle \eta_2-\eta_1,  \mathbf{T}_\eta (\mathbf{w}) \rangle ] \\
&= A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \underbrace{ E_{p(w|\eta_1)} [ \mathbf{T}_\eta (\mathbf{w}) }_{ \nabla_\eta A_\eta(\eta_1) } \rangle   ] \\
&=  \mathrm{B}_{A_\eta}(\mathbf{\eta}_2,\mathbf{\eta}_1)
\end{aligned}
$$`

We denote the expectation parameter as `$\mathbf{m}$`.
The convex conjugate of the log-partition function `$A_\eta$` is defined as
`$$
\begin{aligned}
A^*_\eta( \mathbf{m}) &:= \sup_{x} \{ \langle \mathbf{x},\mathbf{m} \rangle - A_\eta(\mathbf{x}) \} \\
&= \langle \mathbf{\eta},\mathbf{m} \rangle - A_\eta(\mathbf{\eta})  \,\,\,\, (\text{the supremum attains at } \mathbf{x}=\eta )\\
\end{aligned}
$$` where
`$\eta=\eta( \mathbf{m} )$`
can be viewed  as a function of `$\mathbf{m}$`.



Notice that
`$$
\begin{aligned}
\nabla_{\mathbf{m}} A^*_\eta( \mathbf{m})
&= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - \nabla_{\mathbf{m}} A_\eta(\mathbf{\eta}) \\
&= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - [\nabla_{\mathbf{m}} \eta] \underbrace{ [\nabla_\eta A_\eta(\mathbf{\eta})] }_{ = \mathbf{m}}\\
&= \mathbf{\eta}
\end{aligned}
$$` and  `$A_\eta(\eta)= \langle \mathbf{\eta},\mathbf{m} \rangle- A^*_\eta( \mathbf{m})$` 

The convex conjugate `$A^*_\eta( \mathbf{m})$` is strictly convex w.r.t. `$\mathbf{m}$` since  the Hessian `$\nabla_m^2 A^*_\eta( \mathbf{m})$`
is positive-definite.

`$$
\begin{aligned}
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})
&= \nabla_{\mathbf{m}} \mathbf{\eta}
\end{aligned}
$$` 

Note that due to `$\eqref{1}$`,
the FIM `$\mathbf{F}_\eta(\eta)$` under natural parameter `$\mathbf{\eta}$`  is the Jacobian matrix `$\mathbf{J}= \nabla_{\eta} \mathbf{m}$`
and 
positive-definite.

Therefore, it is easy to see that
`$
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m}) = 
\mathbf{F}^{-1}_\eta(\eta),
$` which is 
positive-definite and therefore strictly convex.


By the [transformation rule]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#transformation-rule-for-the-fisher-information-matrix) of the FIM,  we have the following relationship.
`$$
\begin{aligned}
\mathbf{F}_{\eta} (\eta) & = \mathbf{J}^T \mathbf{F}_{m}(\mathbf{m}) \mathbf{J} \\
&= \mathbf{F}^T_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) \\
&= \mathbf{F}_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) \\
\end{aligned}
$$`  which implies that
the FIM under expectation parameter `$\mathbf{m}$` is
`$\mathbf{F}_m(\mathbf{m})=\mathbf{F}^{-1}_\eta(\eta) = \nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})= \nabla_{\mathbf{m}} \mathbf{\eta}$`.


Moreover, we have the following identity.
`$$
\begin{aligned}
\mathrm{B}_{A_\eta}(\mathbf{\eta}_2,\mathbf{\eta}_1)
&= A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \overbrace{ \nabla_\eta A_\eta(\eta_1) }^{= \mathbf{m}_1} \rangle    \\
&= [  \langle \mathbf{\eta}_2,\mathbf{m}_2 \rangle- A^*_\eta( \mathbf{m}_2) ]   
-[  \langle \mathbf{\eta}_1,\mathbf{m}_1 \rangle- A^*_\eta( \mathbf{m}_1) ]
-\langle \eta_2-\eta_1, \mathbf{m}_1 \rangle \\
&=  A^*_\eta( \mathbf{m}_1) - A^*_\eta( \mathbf{m}_2) -    
\langle \mathbf{m}_1-\mathbf{m}_2, \underbrace{ \eta_2}_{ = \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_2)} \rangle\\
&= \mathrm{B}_{A^*_\eta}(\mathbf{m}_1,\mathbf{m}_2)
\end{aligned}
$$`


To show natural-gradient descent as unconstrained mirror descent, we have to make the following assumption.

**Additional** assumption:
Natural parameter space `$\Omega_\eta$` and its
expectation parameter space `$\Omega_{m_\eta}$` are both unconstrainted (`$\Omega_\eta=\Omega_{m_\eta} =\mathcal{R}^K$`), where `$K$` is the number of entries of parameter array `$\eta$`.

Now, consider the following mirror descent in the 
expectation parameter space as

`$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{x \in \Omega_m}\{ \langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k) \}
\end{aligned}\tag{3}\label{3}
$$` where `$\nabla_m \ell(\mathbf{m}_k):= \nabla_m f(\eta(\mathbf{m}_k))$`.

Denote 
`$$
\begin{aligned}
u(\mathbf{x}) &:=\langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha} [ \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k)] \\
& = \langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  [A^*_\eta( \mathbf{x}) - A^*_\eta( \mathbf{m}_k) -    
\langle \mathbf{x}-\mathbf{m}_k, \underbrace{ \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_k)}_{ = \eta_k} \rangle ],
\end{aligned}
$$` where $\mathbf{m}_k$ is considered to be fixed.

When `$\Omega_m=\mathcal{R}^k$`, the optimal expectation parameter `$\mathbf{m}_{k+1}=\bar{\mathbf{x}}$` must satisfy the following expression.

`$$
\begin{aligned}
\mathbf{0} &= \nabla_x u(\bar{\mathbf{x}}) \\
&= \nabla_m \ell(\mathbf{m}_k)+ \frac{1}{\alpha}  [\underbrace{ \nabla_x A^*_\eta( \bar{\mathbf{x}})}_{\eta_{\bar{x}}}  -  \eta_k ],
\end{aligned}
$$` which implies that
`$\eta_{\bar{x}} = \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)$`.

Note that `$\eta_{\bar{x}}$` is the corresponding natural parameter for the expectation
`$\mathbf{m}_{k+1}$`.
Therefore, unconstrained mirror descent in expectation parameter space `$\Omega_m$` gives 
the following update
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)
= \eta_k - \alpha\nabla_m f( \underbrace{ \eta(\mathbf{m}_k) }_{= \eta_k}),
\end{aligned}
$$` which is exactly natural gradient
descent in natural parameter space `$\Omega_\eta$` since by `$\eqref{2}$`, we have `$\nabla_m \ell(\mathbf{m}_k) = \nabla_m f( \eta_k)= \mathbf{F}
_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$`.




# Handling Parameter Constraints
------

## Projected Natural-gradient Descent and (Constrained) Mirror Descent

## Using an Adaptive Step-size

## Riemannian Gradient Descent


