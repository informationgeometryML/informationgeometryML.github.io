---
title: 'Part V: Efficient Natural-gradient Methods for Exponential Family'
date: 2021-12-14
permalink: /posts/2021/12/Geomopt05/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
  - Exponential Family
---

Warning: working in Progress (incomplete)

Goal
------
This blog post should show that we can efficiently implement natural-gradient methods in many cases.

We will give an informal introduction with a focus on high level of ideas.


# Exponential Family
------

An exponential family takes the following (canonical) form as
`$$
\begin{aligned}
p(\mathbf{w}|\mathbf{\eta}) = h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta (\mathbf{\eta}) )
\end{aligned}
$$` where   `$C_\eta(\eta) :=  \int h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}(\mathbf{w}) \rangle ) d \mathbf{w}
$` is the normalization constant.   `$A_\eta(\mathbf{\eta}):=\log C_\eta(\eta)$`,  `$h_\eta(\mathbf{w})$`, `$\mathbf{T}_\eta(\mathbf{w})$`, and `$\eta$`  are known as the log-partition function, the base measure, the sufficient statistics, and the **natural** parameter,  respectively.

The parameter space of `$\eta$` denoted by `$\Omega_\eta$` is determined so that the normalization constant is well-defined and (strictly and finitely) positive.

**Regular** natural parametrization `$\eta$`: parameter space `$\Omega_\eta$` is relatively open.

 In this post, we only consider
regular natural parametrizations since commonly used exponential family distributions have a regular natural parametrization.

This natural parametrization is special since the inner product `$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$` is **linear** in `$\eta$`. As we will discuss later,  this linearity is essential.


Readers should be aware of the following points when using an exponential family.

* The support of `$\mathbf{w}$` should not depend on parametrization `$\eta$`.

* The base measure and the log-partition function are only unique up to a constant as illustrated by the following example. 
    >
>Example: Univariate Gaussian family as an exponential family:
>
> Recall that in [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families), we consider this family as
>`$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$`, where `$\mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $`.
>
> We re-express it in an exponential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})  &= \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] \\
>&= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2\sigma} \\ \frac{\mu}{\sigma}  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} w^2 \\ w  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ]     )   \\
>\end{aligned}
>$$`
> Since `$\sigma= -\frac{1}{2\eta_1} $` and `$\mu = -\frac{\eta_2}{2\eta_1}$`,  `$A_\eta(\mathbf{\eta}) = \frac{1} {2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ] = \frac{1}{2} [ \log ( 2\pi ) + \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $`.
>
> It is also valid that $ h_\eta({w}) = \frac{1}{\sqrt{2\pi}} $ and 
> `$A_\eta(\mathbf{\eta}) =  \frac{1}{2} [  \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $`.
>
> We easily to verify that parameter space `$\Omega_\eta= \{ (\eta_1,\eta_2) | \eta_1<0 , \eta_2 \in \mathcal{R} \}$` is open in $\mathcal{R}^2$.

* The log-partition function could be differentiable w.r.t. `$\eta$` even when `$\mathbf{w}$` is discrete.
    >
>Example: Bernoulli family as an exponential family:
>
> Recall that in [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families), we consider this family as
>  `$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \Big| 0<\pi<1 \}$`
>
> We re-express it in an exponential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})
>&=  \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \\
>&=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \log \frac{\pi}{1-\pi}}_{\eta} , \underbrace{ \mathcal{I}(w=0)}_{T_\eta(w) } \rangle - \log \frac{1}{1-\pi} )
>\end{aligned}
>$$`
> Since `$\pi = \frac{\exp(\eta)}{1+ \exp(\eta) } $` , we have `$A_\eta(\mathbf{\eta}) =  \log \frac{1}{1-\pi} = \log(1+\exp(\eta))$`. <br />
> We easily to verify that parameter space `$\Omega_\eta= \{ \eta | \eta \in \mathcal{R} \}$` is open in $\mathcal{R}$.

* An invertiable linear reparametrization could also be a natural parametrization. This is one of the reasons using natural-gradient descent since it is linearly invariant.
    >
> For simplicity, let's assume set `$\Omega_\lambda := \{ \mathbf{U}^{-1} \eta | \eta \in \Omega_\eta \} = \Omega_\eta$`, where `$\mathbf{U}$` is a constant invertiable matrix. 
>
>Consider a linear reparametrization such as `$\lambda=\mathbf{U}^{-1} \mathbf{\eta}$`, parametrization `$\lambda$` is also a natural parametrization as
>`$$
>\begin{aligned}
>p(\mathbf{w}|\mathbf{\lambda})
>&= h_\eta(\mathbf{w}) \exp( \langle \mathbf{U}\mathbf{\lambda} , \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta( \mathbf{U}\mathbf{\lambda}) ) \\
>&=  h_\eta(\mathbf{w})\exp( \langle \mathbf{\lambda} , \mathbf{U}^T \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta(\mathbf{U}\mathbf{\lambda}) ) \\
>&= h_\lambda(\mathbf{w})  \exp( \langle \mathbf{\lambda} ,  \mathbf{T}_\lambda(\mathbf{w}) \rangle - A_\lambda(\mathbf{\lambda}) ) 
>\end{aligned}
>$$` where `$h_\lambda(\mathbf{w}):= h_\eta(\mathbf{w})$`,  `$\mathbf{T}_\lambda(\mathbf{w}):= \mathbf{U}^T\mathbf{T}_\eta(\mathbf{w})$`, and 
>`$A_\lambda(\mathbf{\lambda}):= A_\eta(\mathbf{U}\mathbf{\lambda})$`.


## Minimal Parametrizations of Exponential Family

Now, we discuss particular parametrizations of an exponential family. We could efficiently compute natural-gradients when
using this class of parametrizations. 

 **Minimal** natural parametrization: the corresponding sufficient stattistics `$\mathcal{T}(\mathbf{w})$` is linearly independent.


A regular, minimal, and natural parametrization `$\eta$` has many nice properties.

* It is an intrinstic parametrization as we discussed in
 [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families).

* The parameter space `$\Omega_\eta$` is an open set in `$\mathcal{R}^K$`, where
`$K$` is the number of entires of this parameter array.

* The log-partition function `$A_\eta(\eta)$` is infinitely differentiable and strictly convex in `$\Omega_\eta$`.

* The FIM `$\mathbf{F}_\eta(\eta) = \nabla_\eta^2 A_\eta(\eta)$` is positive-definite in its domain.

We will only show the first property in this post. The remaining properties can be found in the literature.
Note that the linearity in the inner product `$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$`,  plays a key role in showing these properties.


Recall that a parametrization is intrinstic if 
`$K$` partial derivatives 
`$ \{ \partial_{\eta_i} \log p(\mathbf{w}|\eta) \} $`  are linearly independent.
Since the parameter space is open in `$\mathcal{R}^K$` and the log-partition function `$A_\eta(\eta)$` is differentiable,  these partial derivatives are well-defined and can be computed as
`$$
\begin{aligned}
\partial_{\eta_i} \log p(\mathbf{w}|\eta) = \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle - \partial_{\eta_i}A_\eta(\eta) 
\end{aligned}
$$` where `$\mathbf{e}_i$` is an one-hot array which has zero in all entries except the $i$-th entry.

> Proof by contradiction:
>
>If these partial derivatives are linearly dependent, there exist a set of non-zero constant `$c_i$` such that
>`$\sum_i c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta)= 0 $`, where the value of `$c_i$` does not depent on  `$\mathbf{w}$`.
>`$$
>\begin{aligned}
>0 &= \sum_{i=1}^{K} c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta) \\
>&= \sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle -   c_i\partial_{\eta_i}A_\eta(\eta) \\
>\end{aligned}
>$$`
>
>Since  `$c_i$` is a non-zero constant and its value does not depent on  `$\mathbf{w}$`, we must have
> `$$
>\begin{aligned}
> 0 =\sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle ,
>\end{aligned}
> $$` which implies that
>the sufficient stattistics `$\mathcal{T}(\mathbf{w})$` is linearly dependent. This is a contradiction since `$\eta$` is a minimal natural parametrization.

Now, we give an example of a regular, minimal and natural parametrization.

>Example: Minimal parametrization for multivate Gaussians
>
>Consider a $d$-dimensional Gaussian family
>We specify a parameterization $\mathbf{\tau}$ of the  family as `$ \{ \mathcal{N}(\mathbf{w} |\mathbf{\mu},\mathbf{\Sigma}) \Big| \mathbf{\mu} \in \mathcal{R}^d, \mathbf{\Sigma}   \succ \mathbf{0} \}$`
>
> We re-express it in an exponential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\lambda})  &= \frac{1}{\sqrt{ \mathrm{det}( 2\pi \Sigma )} } \exp [- \frac{1}{2} (\mathbf{w}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{w}-\mathbf{\mu})  ] \\
>&= \underbrace{ \exp(0) }_{  h_\lambda({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \mathrm{vec}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\lambda} }  ,  \underbrace{\begin{bmatrix} \mathrm{vec}( \mathbf{w} \mathbf{w}^T) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\lambda ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
>\end{aligned}
>$$` where `$\mathrm{vec}()$` is the vectorization function
>and  `$\lambda$` is a $(d+d^2)$-dim array.
>
> Parametrization $\lambda$ is a natural and regular parametrization. However, it is NOT a minimal
>parametrization since `$\mathbf{w} \mathbf{w}^T$` is symmetric and therefore the sufficient statistics is linearly dependent.
> It can be shown that $\Omega_\lambda$ is relatively open but not open in `$\mathcal{R}^K$`, where `$K=d+d^2$`.
>
> A minimal natural parametrization $\eta$ should be defined as
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})
>&= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \mathrm{vech}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} \mathrm{vech}( \mathbf{w} \mathbf{w}^T) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
>\end{aligned}
>$$` where `$\mathrm{vech}()$` is the [half-vectorization function](https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization) 
>and  `$\eta$` is a $(d+\frac{d(d+1)}{2})$-dim array.
> It can be shown that $\Omega_\eta$ is open in `$\mathcal{R}^K$`, where `$K=d+\frac{d(d+1)}{2}$`.
>
> As we discussed in [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-steepest-direction),  `$\mathrm{vech}(\Sigma^{-1})$` is an intrinstic parameterization while `$\mathrm{vec}(\Sigma^{-1})$` is not.


The following example illustrates
a non-minimal natural parametrization
>Example: Non-minimal parametrization for Bernoulli family
>
> We consider this family in [the previous section](#exponential-family) with another
> parametrization dicussed in
> [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families).
>  `$ \{ \mathcal{I}(w=0) \frac{\pi_1}{\pi_1+\pi_2} + \mathcal{I}(w=1) \frac{\pi_2}{\pi_1+\pi_2} \Big| \pi_1>0, \pi_2>0 \}$`
>
> We re-express it in another exponential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})
>&=  \mathcal{I}(w=0) \frac{\pi_1}{\pi_1+\pi_2} + \mathcal{I}(w=1) \frac{\pi_2}{\pi_1+\pi_2} \\
>&=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \begin{bmatrix} \log \frac{\pi_1}{\pi_1+\pi_2} \\  \log \frac{\pi_2}{\pi_1+\pi_2} \end{bmatrix} }_{\eta} , \underbrace{ \begin{bmatrix} \mathcal{I}(w=0) \\   \mathcal{I}(w=1)\end{bmatrix} }_{\mathbf{T}_\eta(w) } \rangle -\underbrace{ 0}_{A_\eta(\eta)} )
>\end{aligned}
>$$`
>
> The natural parameter space `$\Omega_\eta= \{ (\eta_1,\eta_2) | \exp(\eta_1)+\exp(\eta_2) = 1 \}$` is not open in `$\mathcal{R}^2$`.
>
> This is not a minimal natural parametrization since the sufficient statistics $\mathbf{T}_\eta(w)$ is linearly dependent as 
>`$\mathcal{I}(w=0)+\mathcal{I}(w=1)=1$`.




# Efficient Natural-gradient Computation

In general, natural-gradient computation can be challenging due to the inverse of the Fisher matrix.
In cases of an exponential family,  natural-gradient computation often can be quite efficient without directly computing the inverse of the Fisher matrix.


We will assume `$\eta$` is a reguar, minimal, natural parametrization.

## Expectation Parametrization
We introduce a dual parametrization `$\mathbf{m} := E_{p(w|\eta)}[ \mathbf{T}_\eta(\mathbf{w}) ] $`, which is known as the expectation parametrization. This new parametrization plays a key role for the efficient natural-gradient computation.

Recall that in
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#the-hessian-is-not-a-valid-manifold-metric),  we use the identity of the score function. This identity also shows us a connection 
between these two parametrizations.
`$$
\begin{aligned}
\mathbf{0} & = E_{p(w|\eta)} [ \nabla_\eta \log p(\mathbf{w}|\eta) ]\\
&=E_{p(w|\eta)} [ \mathbf{T}_\eta(\mathbf{w}) - \nabla_\eta A_\eta(\eta) ] \\
&= \mathbf{m}  - \nabla_\eta A_\eta(\eta)
\end{aligned}\tag{1}\label{1}
$$`  which is a valid Legendre (dual) transformation since 
`$\nabla_\eta^2 A_\eta(\eta)$` is positive-definite in its domain `$\Omega_\eta$`.

## Expectation Parameter Space
The expectation parameter can be viewed as an ouput of
a continous map 
`$$
\begin{aligned}
\mathbf{m}(\eta):=\nabla_\eta A_\eta(\eta),
\end{aligned}\tag{2}\label{2}
$$`
 where the input space is `$\Omega_\eta$`. 

We define the expectation parameter space as the output space of the map `$\Omega_m :=\{\mathbf{m}(\eta) | \eta \in \Omega_\eta \}$`.

Since `$\nabla_\eta^2 A_\eta(\eta)$` is positive-definite in open set `$\Omega_\eta$`, we can show that there exists an one-to-one relationship between the
natural parameter `$\eta$` and the expectation parameter `$\mathbf{m}$`, which implies that map `$\mathbf{m}(\cdot)$` is injective.


Since `$\Omega_\eta$` is open in `$\mathcal{R}^K$`, we can show that
the expectation parameter space `$\Omega_m$` is also open in `$\mathcal{R}^K$` due to the [invariance of domain](https://en.wikipedia.org/wiki/Invariance_of_domain).


## Natural-gradient Computation
Note that the FIM of the exponential family under parametrization `$\eta$` is
`$$
\begin{aligned}
\mathbf{F}_\eta(\eta)=\nabla_\eta^2 A_\eta(\eta) = \nabla_\eta \mathbf{m}
\end{aligned}\tag{3}\label{3}
$$` which means this FIM is a Jacobian matrix `$\mathbf{J}=\nabla_\eta \mathbf{m}$`.

As we discussed in
[Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-steepest-direction),
a natural-gradient w.r.t. `$f(\eta)$` can be computed as below, where `$\mathbf{g}_\eta=\nabla_\eta f(\eta)$` is a Euclidean gradient w.r.t.  natural parameter `$\eta$`.
`$$
\begin{aligned}
\hat{\mathbf{g}}_\eta & = \mathbf{F}^{-1}_\eta(\eta) \mathbf{g}_\eta \\
&= (\nabla_\eta \mathbf{m} )^{-1} [ \nabla_\eta f(\eta) ] \\
&= [\nabla_{m} \eta ] [ \nabla_\eta f(\eta) ] \\
&=  \nabla_{m} f(\eta)  
\end{aligned}\tag{4}\label{4}
$$` where 
`$\nabla_{m} f( \eta )$` is a Euclidean gradient w.r.t. expectation parameter `$\mathbf{m}$` and
`$\eta=\eta( \mathbf{m} )$`
can be viewed  as a function of `$\mathbf{m}$`.

Therefore, natural-gradients w.r.t. natural parameter `$\eta$` can be efficinetly computed if we  can  easily compute Euclidean gradients w.r.t. its expectation parameter `$\mathbf{m}$`.


to do: add the Gaussian example



# Natural-gradient Descent as Unconstrained Mirror Descent

Now, we show that natural-gradient descent is related to mirror descent in unconstrained cases.

we will assume  natural parametrization `$\eta$` is both regular and minimal.

Since mirror descent is defined by using a Bregman divergence, we first introduce the Bregman divergence.

## Bregman Divergence
Given a strictly convex function `$\Phi(\cdot)$` in its domain, a Bregman divergence equipped with `$\Phi(\cdot)$` is defined as
`$$
\begin{aligned}
\mathrm{B}_\Phi(\mathbf{x},\mathbf{y}):= \Phi(\mathbf{x})- \Phi(\mathbf{y}) - \langle \nabla \Phi(\mathbf{y}), (\mathbf{x}-\mathbf{y}) \rangle
\end{aligned}
$$`

In particular, the Kullback–Leibler (KL) divergence is a Bregman divergence when we use natural parametrization `$\eta$`:
`$$
\begin{aligned}
\, & \,\mathrm{KL} [p(\mathbf{w}|\eta_1) || p(\mathbf{w}|\eta_2)]\\
=& \,E_{p(w|\eta_1)} [ \log \frac{p(\mathbf{w}|\eta_1)} {p(\mathbf{w}|\eta_2)} ] \\
=& \,E_{p(w|\eta_1)} [  \langle \eta_1-\eta_2 , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta(\eta_1) + A_\eta(\eta_2) ] & ( p(\mathbf{w}|\eta) \text{ is an exponential family}) \\
=& \,A_\eta(\eta_2) - A_\eta(\eta_1) - E_{p(w|\eta_1)} [  \langle \eta_2-\eta_1,  \mathbf{T}_\eta (\mathbf{w}) \rangle ] \\
=& \,A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \underbrace{ E_{p(w|\eta_1)} [ \mathbf{T}_\eta (\mathbf{w}) ] }_{ \nabla_\eta A_\eta(\eta_1) }  \rangle    \\
=& \, \mathrm{B}_{A_\eta}(\mathbf{\eta}_2,\mathbf{\eta}_1) & ( A_\eta(\eta) \text{ is strictly convex})
\end{aligned}
$$`

We denote the expectation parameter as `$\mathbf{m}$`.
The convex conjugate (Legendre transformation) of the log-partition function `$A_\eta$` is defined as
`$$
\begin{aligned}
A^*_\eta( \mathbf{m}) &:= \sup_{x} \{ \langle \mathbf{x},\mathbf{m} \rangle - A_\eta(\mathbf{x}) \} \\
&= \langle \mathbf{\eta},\mathbf{m} \rangle - A_\eta(\mathbf{\eta})  \,\,\,\, (\text{the supremum attains at } \mathbf{x}=\eta )\\
\end{aligned}\tag{5}\label{5}
$$` where
`$\eta=\eta( \mathbf{m} )$`
can be viewed  as a function of `$\mathbf{m}$`.



Notice that
`$$
\begin{aligned}
\nabla_{\mathbf{m}} A^*_\eta( \mathbf{m})
&= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - \nabla_{\mathbf{m}} A_\eta(\mathbf{\eta}) \\
&= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - [\nabla_{\mathbf{m}} \eta] \underbrace{ [\nabla_\eta A_\eta(\mathbf{\eta})] }_{ = \mathbf{m}}\\
&= \mathbf{\eta}
\end{aligned}\tag{6}\label{6}
$$`.

The convex conjugate `$A^*_\eta( \mathbf{m})$` is strictly convex w.r.t. `$\mathbf{m}$` since the Hessian `$\nabla_m^2 A^*_\eta( \mathbf{m})$`
is positive-definite as shown below.

`$$
\begin{aligned}
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})
&= \nabla_{\mathbf{m}} \mathbf{\eta}
\end{aligned}
$$` 

Note that due to `$\eqref{3}$`,
the FIM `$\mathbf{F}_\eta(\eta)$` under natural parameter `$\mathbf{\eta}$`  is the Jacobian matrix `$\mathbf{J}= \nabla_{\eta} \mathbf{m}$`
and 
positive-definite.

Therefore, it is easy to see that
`$
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m}) = 
\mathbf{F}^{-1}_\eta(\eta),
$` which is 
positive-definite and therefore strictly convex.


By the [transformation rule]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#transformation-rule-for-the-fisher-information-matrix) of the FIM,  we have the following relationship.
`$$
\begin{aligned}
\mathbf{F}_{\eta} (\eta) & = \mathbf{J}^T \mathbf{F}_{m}(\mathbf{m}) \mathbf{J} \\
&= \mathbf{F}^T_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) \\
&= \mathbf{F}_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) & (\text{the FIM is symmetric})
\end{aligned}
$$`  which implies that
the FIM under expectation parameter `$\mathbf{m}$` is
`$\mathbf{F}_m(\mathbf{m})=\mathbf{F}^{-1}_\eta(\eta) = \nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})= \nabla_{\mathbf{m}} \mathbf{\eta}$`.

 
Moreover, we have the following identity since by `$\eqref{5}$`, `$A_\eta(\eta)=\langle \mathbf{\eta},\mathbf{m} \rangle- A^*_\eta( \mathbf{m}) $`.
`$$
\begin{aligned}
\mathrm{B}_{A_\eta}(\mathbf{\eta}_2, \color{red}{\mathbf{\eta}_1 })
&= A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \overbrace{ \nabla_\eta A_\eta(\eta_1) }^{= \mathbf{m}_1} \rangle    \\
&= [  \langle \mathbf{\eta}_2,\mathbf{m}_2 \rangle- A^*_\eta( \mathbf{m}_2) ]   
-[  \langle \mathbf{\eta}_1,\mathbf{m}_1 \rangle- A^*_\eta( \mathbf{m}_1) ]
-\langle \eta_2-\eta_1, \mathbf{m}_1 \rangle \\
&=  A^*_\eta( \mathbf{m}_1) - A^*_\eta( \mathbf{m}_2) -    
\langle \mathbf{m}_1-\mathbf{m}_2, \underbrace{ \eta_2}_{ = \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_2)} \rangle\\
&= \mathrm{B}_{A^*_\eta}( \color{red}{ \mathbf{m}_1 },\mathbf{m}_2) & (\text{the order is changed})
\end{aligned}
$$`

## Mirror Descent (MD)

Now, we give the definition of mirror descent.

Consider the following optimization problem over a convex domain denoted by `$\Omega_\theta$`.
`$$
\begin{aligned}
\min_{\theta \in \Omega_\theta} \ell(\mathbf{\theta})
\end{aligned}
$$`

Given a strictly convex function `$\Phi(\mathbf{\theta})$` in the domain , mirror
descent with step-size `$\alpha$` is defined as

`$$
\begin{aligned}
\mathbf{\theta}_{k+1} \leftarrow \arg \min_{x \in \Omega_\theta}\{ \langle \nabla_\theta \ell(\mathbf{\theta}_k), \mathbf{x}-\mathbf{\theta}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{\Phi}(\mathbf{x},\mathbf{\theta}_k) \}
\end{aligned}
$$` where `$\mathrm{B}_\Phi(\cdot,\cdot)$` is a Bregman divergence  equipped  with the strictly convex function `$\Phi(\cdot)$`.



## Natural-gradient Descent as Mirror Descent

To show natural-gradient descent as mirror descent, we have to make the following assumption.

**Additional** assumption:
**Natural** parameter space `$\Omega_\eta$`
is unconstrainted (`$\Omega_\eta=\mathcal{R}^K$`), where `$K$` is the number of entries of parameter array `$\eta$`.

The following example illustrates that the expectation space `$\Omega_m$` is constrained even when
`$\Omega_\eta$` is unconstrained.

>Example: Bernoulli family
>
> We consider this family as discussed in [the previous section](#exponential-family)
>  `$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \Big| 0<\pi<1 \}$`
>
> We re-express it in an exponential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})
>&=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \log \frac{\pi}{1-\pi}}_{\eta} , \underbrace{ \mathcal{I}(w=0)}_{T_\eta(w) } \rangle - \log \frac{1}{1-\pi} )
>\end{aligned}
>$$`
> where  `$\pi = \frac{\exp(\eta)}{1+ \exp(\eta) } $` and`$A_\eta(\mathbf{\eta}) =  \log \frac{1}{1-\pi} = \log(1+\exp(\eta))$`.
>
> This natural parametrization is also known as the soft-max representation in categorical cases.
>
> The natural parameter space `$\Omega_\eta= \{ \eta | \eta \in \mathcal{R} \}=\mathcal{R}^1$`.
>
> The corresponding expectation parameter is `$m = E_{q(w|\eta)}[ T_\eta (w) ] = \pi$`
>
> The expectation parameter space `$\Omega_m= \{ m| 0<m<1 \}$` is a constrained open set in $\mathcal{R}^1$.



Now, consider the following mirror descent in the expectation parameter space `$\Omega_m$` as

`$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{x \in \Omega_m}\{ \langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k) \}
\end{aligned}\tag{7}\label{7}
$$` where `$\nabla_m \ell(\mathbf{m}_k):= \nabla_m f(\eta(\mathbf{m}_k))$` and the Bregman divergence `$\mathrm{B}_{A^*_\eta}(\cdot,\cdot)$` is well-defined
since  `$A^*_\eta$` is strcitly convex in `$\Omega_m$`.


Denote 
`$$
\begin{aligned}
u(\mathbf{x}) &:=\langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha} [ \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k)] \\
& = \langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  [A^*_\eta( \mathbf{x}) - A^*_\eta( \mathbf{m}_k) -    
\langle \mathbf{x}-\mathbf{m}_k, \underbrace{ \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_k)}_{ = \eta_k} \rangle ],
\end{aligned}
$$`

A stationary point of `$\eqref{7}$`,  `$\hat{\mathbf{x}}$`,  must satisfy the following
 condition.
`$$
\begin{aligned}
\mathbf{0} &= \nabla_x u(\hat{\mathbf{x}}) & (\mathbf{m}_k \text{ is considered to be a constant}) \\
&= \nabla_m \ell(\hat{\mathbf{x}})+ \frac{1}{\alpha}  [ \nabla_x A^*_\eta( \hat{\mathbf{x}})  -  \eta_k ],
\end{aligned}
$$` which implies that `$\nabla_x A^*_\eta( \hat{\mathbf{x}}) =  \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)$`


We first show that there exists a stationary point in the domain (`$\hat{\mathbf{x}} \in \Omega_m$`).
Let's denote `$\mathbf{\eta}_{k+1}:= \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)$`. Since `$\Omega_\eta$` is unconstrained, it
is obvious that `$\mathbf{\eta}_{k+1} \in \Omega_\eta$`.
By `$\eqref{2}$`, `$\mathbf{m}(\mathbf{\eta}_{k+1}) =\nabla_\eta A_\eta( \mathbf{\eta}_{k+1}) \in
\Omega_m$`. Notice that,  by `$\eqref{6}$`, we have  `$\nabla_m A^*_\eta ( \mathbf{m}(\mathbf{\eta}_{k+1}) ) = \mathbf{\eta}_{k+1}$`, which implies that `$\mathbf{m}(\mathbf{\eta}_{k+1})$` is a stationary point and 
 `$\mathbf{m}(\mathbf{\eta}_{k+1}) \in \Omega_m$`.

Moreover,
`$\mathbf{m}(\mathbf{\eta}_{k+1})$` is 
the unique  solution of `$\eqref{7}$` since `$\nabla_x^2 u(\mathbf{x}) =\nabla_x^2 A^*_\eta(
{\mathbf{x}})$` is positive-definite for any  `$\mathbf{x} \in \Omega_m$` and therefore strictly convex.
In other words, 
`$\mathbf{m}_{k+1} = \mathbf{m}( {\mathbf{\eta}_{k+1}}) $`.


In summary, when `$\Omega_\eta = \mathcal{R}^K$`, the unique solution of `$\eqref{7}$` is 
`$$
\begin{aligned}
\eta_{k+1} & = \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k) \\
\mathbf{m}_{k+1} &= \nabla_\eta A_\eta( {\mathbf{\eta}_{k+1}})
\end{aligned}\tag{8}\label{8}
$$` 

In other words,
mirror descent of `$\eqref{7}$` in **expectation** parameter space `$\Omega_m$` is equivalent to
the following update
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)
= \eta_k - \alpha\nabla_m f( \underbrace{ \eta(\mathbf{m}_k) }_{= \eta_k}),
\end{aligned}\tag{9}\label{9}
$$` which is exactly natural gradient
descent in **natural** parameter space `$\Omega_\eta=\mathcal{R}^K$` since by `$\eqref{4}$`, we have `$\nabla_m \ell(\mathbf{m}_k) = \nabla_m f( \eta_k)= \mathbf{F}
_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$`.

