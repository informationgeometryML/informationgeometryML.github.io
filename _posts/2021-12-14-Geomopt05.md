---
title: 'Part V: Efficient Natural-gradient Methods for Exponential Family'
date: 2021-12-14
permalink: /posts/2021/12/Geomopt05/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
  - Exponential Family
---

Warning: working in Progress (incomplete)

Goal
------
This blog post should show that we can efficiently implement natural-gradient methods in many cases.

We will give an informal introduction with a focus on high level of ideas.


# Exponential Family
------

An exponential family takes the following (canonical) form as
`$$
\begin{aligned}
p(\mathbf{w}|\mathbf{\eta}) = h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta (\mathbf{\eta}) )
\end{aligned}
$$` where   `$C_\eta(\eta) :=  \int h_\eta(\mathbf{w}) \exp( \langle \mathbf{\eta} , \mathbf{T}(\mathbf{w}) \rangle ) d \mathbf{w}
$` is the normalization constant.   `$A_\eta(\mathbf{\eta}):=\log C_\eta(\eta)$`,  `$h_\eta(\mathbf{w})$`, `$\mathbf{T}_\eta(\mathbf{w})$`, and `$\eta$`  are known as the log-partition function, the base measure, the sufficient statistics, and the **natural** parameter,  respectively.

The parameter space of `$\eta$` denoted by `$\Omega_\eta$` is determined so that the normalization constant is well-defined and (strictly) positive.

**Regular** natural parametrization `$\eta$`: parameter space `$\Omega_\eta$` is relatively open.

 In this post, we only consider
regular natural parametrizations since commonly used exponential family distributions have a regular natural parametrization.

This natural parametrization is special since the inner product `$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$` is **linear** in `$\eta$`. As we will discuss later,  this linearity is essential.


Readers should be aware of the following points when using an exponential family.

* The support of `$\mathbf{w}$` should not depend on parametrization `$\eta$`.

* The base measure and the log-partition function are only unique up to a constant as illustrated by the following example. 
    >
>Example: Univariate Gaussian as an exponential family:
>
> Recall that in [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families), we consider this family as
>`$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$`, where `$\mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $`.
>
> We re-express it in an exponential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})  &= \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] \\
>&= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2\sigma} \\ \frac{\mu}{\sigma}  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} w^2 \\ w  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ]     )   \\
>\end{aligned}
>$$`
> Since `$\sigma= -\frac{1}{2\eta_1} $` and `$\mu = -\frac{\eta_2}{2\eta_1}$`,  `$A_\eta(\mathbf{\eta}) = \frac{1} {2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ] = \frac{1}{2} [ \log ( 2\pi ) + \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $`.
>
> It is also valid that $ h_\eta({w}) = \frac{1}{\sqrt{2\pi}} $ and 
> `$A_\eta(\mathbf{\eta}) =  \frac{1}{2} [  \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $`.
>
> We easily to verify that parameter space `$\Omega_\eta= \{ (\eta_1,\eta_2) | \eta_1<0 , \eta_2 \in \mathcal{R} \}$` is open in $\mathcal{R}^2$.

* The log-partition function could be differentiable w.r.t. `$\eta$` even when `$\mathbf{w}$` is discrete.
    >
>Example: Bernoulli family as an exponential family:
>
> Recall that in [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families), we consider this family as
>  `$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \Big| 0<\pi<1 \}$`
>
> We re-express it in an exponential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})
>&=  \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) \\
>&=\underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{ \log \frac{\pi}{1-\pi}}_{\eta} , \underbrace{ \mathcal{I}(w=0)}_{T_\eta(w) } \rangle - \log \frac{1}{1-\pi} )
>\end{aligned}
>$$`
> Since `$\pi = \frac{\exp(\eta)}{1+ \exp(\eta) } $` , we have `$A_\eta(\mathbf{\eta}) =  \log \frac{1}{1-\pi} = \log(1+\exp(\eta))$`. <br />
> We easily to verify that parameter space `$\Omega_\eta= \{ \eta | \eta \in \mathcal{R} \}$` is open in $\mathcal{R}$.

* An invertiable linear reparametrization could also be a natural parametrization. This is one of the reasons using natural-gradient descent since it is linearly invariant.
    >
> For simplicity, let's assume set `$\Omega_\lambda := \{ \mathbf{U}^{-1} \eta | \eta \in \Omega_\eta \} = \Omega_\eta$`, where `$\mathbf{U}$` is a constant invertiable matrix. 
>
>Consider a linear reparametrization such as `$\lambda=\mathbf{U}^{-1} \mathbf{\eta}$`, parametrization `$\lambda$` is also a natural parametrization as
>`$$
>\begin{aligned}
>p(\mathbf{w}|\mathbf{\lambda})
>&= h_\eta(\mathbf{w}) \exp( \langle \mathbf{U}\mathbf{\lambda} , \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta( \mathbf{U}\mathbf{\lambda}) ) \\
>&=  h_\eta(\mathbf{w})\exp( \langle \mathbf{\lambda} , \mathbf{U}^T \mathbf{T}_\eta(\mathbf{w}) \rangle - A_\eta(\mathbf{U}\mathbf{\lambda}) ) \\
>&= h_\lambda(\mathbf{w})  \exp( \langle \mathbf{\lambda} ,  \mathbf{T}_\lambda(\mathbf{w}) \rangle - A_\lambda(\mathbf{\lambda}) ) 
>\end{aligned}
>$$` where `$h_\lambda(\mathbf{w}):= h_\eta(\mathbf{w})$`,  `$\mathbf{T}_\lambda(\mathbf{w}):= \mathbf{U}^T\mathbf{T}_\eta(\mathbf{w})$`, and 
>`$A_\lambda(\mathbf{\lambda}):= A_\eta(\mathbf{U}\mathbf{\lambda})$`.


## Minimal Parametrizations of Exponential Family

Now, we discuss particular parametrizations of an exponential family. We could efficiently compute natural-gradients when
using this class of parametrizations. 

 **Minimal** natural parametrization: the corresponding sufficient stattistics `$\mathcal{T}(\mathbf{w})$` is linearly independent.


A regular, minimal, and natural parametrization `$\eta$` has many nice properties.

* It is an intrinstic parametrization as we discussed in
 [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families).

* The parameter space `$\Omega_\eta$` is a open set in `$\mathcal{R}^K$`, where
`$K$` is the number of entires of this parameter array.

* The log-partition function `$A_\eta(\eta)$` is infinitely differentiable and strictly convex in `$\Omega_\eta$`.

* The FIM `$\mathbf{F}_\eta(\eta) = \nabla_\eta^2 A_\eta(\eta)$` is positive-definite.

We will only show the first property in this post. The remaining properties can be found in the literature.
Note that the linearity in the inner product `$\langle \mathbf{\eta} , \mathbf{T}_\eta(\mathbf{w}) \rangle$`,  plays a key role in showing these properties.


Recall that a parametrization is intrinstic if 
`$K$` partial derivatives 
`$ \{ \partial_{\eta_i} \log p(\mathbf{w}|\eta) \} $`  are linearly independent.
Since the parameter space is open in `$\mathcal{R}^K$` and the log-partition function `$A_\eta(\eta)$` is differentiable,  these partial derivatives are well-defined and can be computed as
`$$
\begin{aligned}
\partial_{\eta_i} \log p(\mathbf{w}|\eta) = \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle - \partial_{\eta_i}A_\eta(\eta) 
\end{aligned}
$$` where `$\mathbf{e}_i$` is an one-hot array which has zero in all entries except the $i$-th entry.

> Proof by contradiction:
>
>If these partial derivatives are linearly dependent, there exist a set of non-zero constant `$c_i$` such that
>`$\sum_i c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta)= 0 $`, where the value of `$c_i$` does not depent on  `$\mathbf{w}$`.
>`$$
>\begin{aligned}
>0 &= \sum_{i=1}^{K} c_i \partial_{\eta_i} \log p(\mathbf{w}|\eta) \\
>&= \sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle -   c_i\partial_{\eta_i}A_\eta(\eta) \\
>\end{aligned}
>$$`
>
>Since  `$c_i$` is a non-zero constant and its value does not depent on  `$\mathbf{w}$`, we must have
> `$$
>\begin{aligned}
> 0 =\sum_{i=1}^{K} c_i \langle \mathbf{e}_i,  \mathbf{T} (\mathbf{w}) \rangle ,
>\end{aligned}
> $$` which implies that
>the sufficient stattistics `$\mathcal{T}(\mathbf{w})$` is linearly dependent. This is a contradiction since `$\eta$` is a minimal natural parametrization.

Now, we give an example of a regular, minimal and natural parametrization.

>Example: Minimal parametrization for multivate Gaussians
>
>Consider a $d$-dimensional Gaussian family
>We specify a parameterization $\mathbf{\tau}$ of the  family as `$ \{ \mathcal{N}(\mathbf{w} |\mathbf{\mu},\mathbf{\Sigma}) \Big| \mathbf{\mu} \in \mathcal{R}^d, \mathbf{\Sigma}   \succ \mathbf{0} \}$`
>
> We re-express it in an exponential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\lambda})  &= \frac{1}{\sqrt{ \mathrm{det}( 2\pi \Sigma )} } \exp [- \frac{1}{2} (\mathbf{w}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{w}-\mathbf{\mu})  ] \\
>&= \underbrace{ \exp(0) }_{  h_\lambda({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \mathrm{vec}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\lambda} }  ,  \underbrace{\begin{bmatrix} \mathrm{vec}( \mathbf{w} \mathbf{w}^T) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\lambda ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
>\end{aligned}
>$$` where `$\mathrm{vec}()$` is the vectorization function
>and  `$\lambda$` is a $(d+d^2)$-dim array.
>
> Parametrization $\lambda$ is a natural and regular parametrization. However, it is NOT a minimal
>parametrization since `$\mathbf{w} \mathbf{w}^T$` is symmetric and therefore the sufficient statistics is linearly dependent.
> It can be shown that $\Omega_\lambda$ is relatively open but not open in `$\mathcal{R}^K$`, where `$K=d+d^2$`.
>
> A minimal natural parametrization $\eta$ should be defined as
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})
>&= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2} \mathrm{vech}( \Sigma^{-1} ) \\ \Sigma^{-1}\mu  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} \mathrm{vech}( \mathbf{w} \mathbf{w}^T) \\ \mathbf{w}  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ d\log ( 2\pi ) + \log \mathrm{det} (\Sigma) + \mu^T \Sigma^{-1} \mu ]     )   \\
>\end{aligned}
>$$` where `$\mathrm{vech}()$` is the [half-vectorization function](https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization) 
>and  `$\eta$` is a $(d+\frac{d(d+1)}{2})$-dim array.
> It can be shown that $\Omega_\eta$ is open in `$\mathcal{R}^K$`, where `$K=d+\frac{d(d+1)}{2}$`.
>
> As we discussed in [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-steepest-direction),  `$\mathrm{vech}(\Sigma^{-1})$` is an intrinstic parameterization while `$\mathrm{vec}(\Sigma^{-1})$` is not.


# Efficient Natural-gradient Computation

In general, natural-gradient computation can be challenging due to the inverse of the Fisher matrix.
In cases of an exponential family,  natural-gradient computation often can be quite efficient without computing the inverse of the Fisher matrix.


We will assume `$\eta$` is a reguar, minimal, natural parametrization.
Now, we introduce a dual parametrization `$\mathbf{m} := E_{p(w|\eta)}[ \mathbf{T}_\eta(\mathbf{w}) ] $`, which is known as the expectation parametrization.

Recall that in
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#the-hessian-is-not-a-valid-manifold-metric),  we use the identity of the score function. This identity also shows us a connection 
between these two parametrizations.
`$$
\begin{aligned}
\mathbf{0} & = E_{p(w|\eta)} [ \nabla_\eta \log p(\mathbf{w}|\eta) ]\\
&=E_{p(w|\eta)} [ \mathbf{T}_\eta(\mathbf{w}) - \nabla_\eta A_\eta(\eta) ] \\
&= \mathbf{m}  - \nabla_\eta A_\eta(\eta)
\end{aligned}
$$`  which is a valid Legendre (dual) transformation since 
`$\nabla_\eta^2 A_\eta(\eta)$` is positive-definite in its domain.

Moreover, the FIM of the exponential family under parametrization `$\eta$` is
`$$
\begin{aligned}
\mathbf{F}_\eta(\eta)=\nabla_\eta^2 A_\eta(\eta) = \nabla_\eta \mathbf{m}
\end{aligned}\tag{1}\label{1}
$$` which means this FIM is a Jacobian matrix `$\mathbf{J}=\nabla_\eta \mathbf{m}$`.

As we discussed in
[Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-steepest-direction),
a natural-gradient w.r.t. `$f(\eta)$` can be computed as below, where `$\mathbf{g}_\eta=\nabla_\eta f(\eta)$` is a Euclidean gradient w.r.t.  natural parameter `$\eta$`.
`$$
\begin{aligned}
\hat{\mathbf{g}}_\eta & = \mathbf{F}^{-1}_\eta(\eta) \mathbf{g}_\eta \\
&= (\nabla_\eta \mathbf{m} )^{-1} [ \nabla_\eta f(\eta) ] \\
&= [\nabla_{m} \eta ] [ \nabla_\eta f(\eta) ] \\
&=  \nabla_{m} f(\eta)  
\end{aligned}\tag{2}\label{2}
$$` where 
`$\nabla_{m} f( \eta )$` is a Euclidean gradient w.r.t. expectation parameter `$\mathbf{m}$` and
`$\eta=\eta( \mathbf{m} )$`
can be viewed  as a function of `$\mathbf{m}$`.

Therefore, natural-gradients w.r.t. natural parameter `$\eta$` can be efficinetly computed if we  can  easily compute Euclidean gradients w.r.t. its expectation parameter `$\mathbf{m}$`.


to do: add the Gaussian example

# Natural-gradient Descent as Unconstrained Mirror Descent

In the  following sections, we will assume a natural parametrization `$\eta$` is also regular and minimal.

Since mirror descent is defined by using a Bregman divergence, we first introduce the Bregman divergence.

## Bregman Divergence
Given a strictly convex function `$\Phi(\cdot)$` in its domain, a Bregman divergence is defined as
`$$
\begin{aligned}
\mathrm{B}_\Phi(\mathbf{x},\mathbf{y}):= \Phi(\mathbf{x})- \Phi(\mathbf{y}) - \langle \nabla \Phi(\mathbf{y}), (\mathbf{x}-\mathbf{y}) \rangle
\end{aligned}
$$`

In particular, the Kullback–Leibler (KL) divergence is a Bregman divergence when we use natural parametrization `$\eta$`:
`$$
\begin{aligned}
\, & \,\mathrm{KL} [p(\mathbf{w}|\eta_1) || p(\mathbf{w}|\eta_2)]\\
=& \,E_{p(w|\eta_1)} [ \log \frac{p(\mathbf{w}|\eta_1)} {p(\mathbf{w}|\eta_2)} ] \\
=& \,E_{p(w|\eta_1)} [  \langle \eta_1-\eta_2 , \mathbf{T}_\eta (\mathbf{w}) \rangle - A_\eta(\eta_1) + A_\eta(\eta_2) ] & ( p(\mathbf{w}|\eta) \text{ is an exponential family}) \\
=& \,A_\eta(\eta_2) - A_\eta(\eta_1) - E_{p(w|\eta_1)} [  \langle \eta_2-\eta_1,  \mathbf{T}_\eta (\mathbf{w}) \rangle ] \\
=& \,A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \underbrace{ E_{p(w|\eta_1)} [ \mathbf{T}_\eta (\mathbf{w}) ] }_{ \nabla_\eta A_\eta(\eta_1) }  \rangle    \\
=& \, \mathrm{B}_{A_\eta}(\mathbf{\eta}_2,\mathbf{\eta}_1) & ( A_\eta(\eta) \text{ is strictly convex})
\end{aligned}
$$`

We denote the expectation parameter as `$\mathbf{m}$`.
The convex conjugate (Legendre transformation) of the log-partition function `$A_\eta$` is defined as
`$$
\begin{aligned}
A^*_\eta( \mathbf{m}) &:= \sup_{x} \{ \langle \mathbf{x},\mathbf{m} \rangle - A_\eta(\mathbf{x}) \} \\
&= \langle \mathbf{\eta},\mathbf{m} \rangle - A_\eta(\mathbf{\eta})  \,\,\,\, (\text{the supremum attains at } \mathbf{x}=\eta )\\
\end{aligned}\tag{3}\label{3}
$$` where
`$\eta=\eta( \mathbf{m} )$`
can be viewed  as a function of `$\mathbf{m}$`.



Notice that
`$$
\begin{aligned}
\nabla_{\mathbf{m}} A^*_\eta( \mathbf{m})
&= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - \nabla_{\mathbf{m}} A_\eta(\mathbf{\eta}) \\
&= \mathbf{\eta} + \langle \nabla_{\mathbf{m}} \mathbf{\eta},\mathbf{m} \rangle - [\nabla_{\mathbf{m}} \eta] \underbrace{ [\nabla_\eta A_\eta(\mathbf{\eta})] }_{ = \mathbf{m}}\\
&= \mathbf{\eta}
\end{aligned}
$$`.

The convex conjugate `$A^*_\eta( \mathbf{m})$` is strictly convex w.r.t. `$\mathbf{m}$` since the Hessian `$\nabla_m^2 A^*_\eta( \mathbf{m})$`
is positive-definite as shown below.

`$$
\begin{aligned}
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})
&= \nabla_{\mathbf{m}} \mathbf{\eta}
\end{aligned}
$$` 

Note that due to `$\eqref{1}$`,
the FIM `$\mathbf{F}_\eta(\eta)$` under natural parameter `$\mathbf{\eta}$`  is the Jacobian matrix `$\mathbf{J}= \nabla_{\eta} \mathbf{m}$`
and 
positive-definite.

Therefore, it is easy to see that
`$
\nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m}) = 
\mathbf{F}^{-1}_\eta(\eta),
$` which is 
positive-definite and therefore strictly convex.


By the [transformation rule]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#transformation-rule-for-the-fisher-information-matrix) of the FIM,  we have the following relationship.
`$$
\begin{aligned}
\mathbf{F}_{\eta} (\eta) & = \mathbf{J}^T \mathbf{F}_{m}(\mathbf{m}) \mathbf{J} \\
&= \mathbf{F}^T_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) \\
&= \mathbf{F}_{\eta} (\eta)  \mathbf{F}_{m}(\mathbf{m})\mathbf{F}_{\eta} (\eta) \\
\end{aligned}
$$`  which implies that
the FIM under expectation parameter `$\mathbf{m}$` is
`$\mathbf{F}_m(\mathbf{m})=\mathbf{F}^{-1}_\eta(\eta) = \nabla_{\mathbf{m}}^2 A^*_\eta( \mathbf{m})= \nabla_{\mathbf{m}} \mathbf{\eta}$`.

 
Moreover, we have the following identity since by `$\eqref{3}$`, `$A_\eta(\eta)=\langle \mathbf{\eta},\mathbf{m} \rangle- A^*_\eta( \mathbf{m}) $`.
`$$
\begin{aligned}
\mathrm{B}_{A_\eta}(\mathbf{\eta}_2, \color{red}{\mathbf{\eta}_1 })
&= A_\eta(\eta_2) - A_\eta(\eta_1) - \langle \eta_2-\eta_1, \overbrace{ \nabla_\eta A_\eta(\eta_1) }^{= \mathbf{m}_1} \rangle    \\
&= [  \langle \mathbf{\eta}_2,\mathbf{m}_2 \rangle- A^*_\eta( \mathbf{m}_2) ]   
-[  \langle \mathbf{\eta}_1,\mathbf{m}_1 \rangle- A^*_\eta( \mathbf{m}_1) ]
-\langle \eta_2-\eta_1, \mathbf{m}_1 \rangle \\
&=  A^*_\eta( \mathbf{m}_1) - A^*_\eta( \mathbf{m}_2) -    
\langle \mathbf{m}_1-\mathbf{m}_2, \underbrace{ \eta_2}_{ = \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_2)} \rangle\\
&= \mathrm{B}_{A^*_\eta}( \color{red}{ \mathbf{m}_1 },\mathbf{m}_2) & (\text{the order is changed})
\end{aligned}
$$`

## Mirror Descent in the Expectation Space
To show natural-gradient descent as unconstrained mirror descent, we have to make the following assumption.

**Additional** assumption:
Natural parameter space `$\Omega_\eta$` and its
expectation parameter space `$\Omega_{m_\eta}$` are both unconstrainted (`$\Omega_\eta=\Omega_{m_\eta} =\mathcal{R}^K$`), where `$K$` is the number of entries of parameter array `$\eta$`.

Now, consider the following mirror descent in the 
**expectation** parameter space `$\Omega_m$` as

`$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{x \in \Omega_m}\{ \langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k) \}
\end{aligned}\tag{4}\label{4}
$$` where `$\nabla_m \ell(\mathbf{m}_k):= \nabla_m f(\eta(\mathbf{m}_k))$`.

Denote 
`$$
\begin{aligned}
u(\mathbf{x}) &:=\langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha} [ \mathrm{B}_{A^*_\eta}(\mathbf{x},\mathbf{m}_k)] \\
& = \langle \nabla_m \ell(\mathbf{m}_k), \mathbf{x}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  [A^*_\eta( \mathbf{x}) - A^*_\eta( \mathbf{m}_k) -    
\langle \mathbf{x}-\mathbf{m}_k, \underbrace{ \nabla_{\mathbf{m}} A^*_\eta( \mathbf{m}_k)}_{ = \eta_k} \rangle ],
\end{aligned}
$$` where $\mathbf{m}_k$ is considered to be fixed.

When `$\Omega_m=\mathcal{R}^k$`, the optimal expectation parameter `$\mathbf{m}_{k+1}=\bar{\mathbf{x}}$` must satisfy the following expression.

`$$
\begin{aligned}
\mathbf{0} &= \nabla_x u(\bar{\mathbf{x}}) \\
&= \nabla_m \ell(\mathbf{m}_k)+ \frac{1}{\alpha}  [\underbrace{ \nabla_x A^*_\eta( \bar{\mathbf{x}})}_{\eta_{\bar{x}}}  -  \eta_k ],
\end{aligned}
$$` which implies that
`$\eta_{\bar{x}} = \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)$`.

Note that `$\eta_{\bar{x}}$` is the corresponding natural parameter for the expectation
parameter
`$\mathbf{m}_{k+1}$`.

Therefore, unconstrained mirror descent in **expectation** parameter space `$\Omega_m$` gives 
the following update
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha \nabla_m \ell(\mathbf{m}_k)
= \eta_k - \alpha\nabla_m f( \underbrace{ \eta(\mathbf{m}_k) }_{= \eta_k}),
\end{aligned}\tag{5}\label{5}
$$` which is exactly natural gradient
descent in **natural** parameter space `$\Omega_\eta$` since by `$\eqref{2}$`, we have `$\nabla_m \ell(\mathbf{m}_k) = \nabla_m f( \eta_k)= \mathbf{F}
_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$`.


# Handling Parameter Constraints
------
Unfortunately, the connection between natural-gradient descent and mirror desecent breaks down when the natural
parameter space `$\Omega_\eta$` is constrained.
Since the Legendre transformation is defined in `$\Omega_\eta$` , which implies the dual (expectation) parameter space `$\Omega_m$` is constrained in general.

The following example illustrate this point
>Example: Univariate Gaussian
>
> we consider this family as discussed in [the previous section](#exponential-family)
>`$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with mean $\mu$ and variance $\sigma$.
>
> It can be re-expressed in an exponential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})
>&= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2\sigma} \\ \frac{\mu}{\sigma}  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} w^2 \\ w  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ]     )   \\
>\end{aligned}
>$$`
> where  `$\sigma= -\frac{1}{2\eta_1} $`,  `$\mu = -\frac{\eta_2}{2\eta_1}$`, and  `$A_\eta(\mathbf{\eta}) = \frac{1} {2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ] = \frac{1}{2} [ \log ( 2\pi ) + \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $`.
>
> The natural parameter space `$\Omega_\eta= \{ (\eta_1,\eta_2) | \eta_1<0 , \eta_2 \in \mathcal{R} \}$` is a constrained open set in $\mathcal{R}^2$, where $K=2$.
>
> The corresponding expectation parameter is `$\mathbf{m} = E_{q(w|\eta)}[ \mathbf{T}_\eta (w) ] = [ \mu^2+\sigma , \mu ] $`. 
>
> The expectation parameter space `$\Omega_m= \{ (m_1,m_2) | m_1 - m_2^2 >0 , m_2 \in \mathcal{R} \}$` is a constrained open set in $\mathcal{R}^2$.



When `$\Omega_m \neq \mathcal{R}^K$`, (constrained) mirror descent in `$\eqref{4}$` in general does not give us the same update in `$\eqref{5}$` as
natural-gradient descent.

> Proof by contradiction:
>
> Suppose when `$\Omega_m \neq \mathcal{R}^K$`,  `$\eqref{4}$` gives the same update in `$\eqref{5}$` in general. 
>
> By the definition of (constrained) mirror descent in `$\eqref{4}$`, the expectation parameter must satisfy `$\mathbf{m}_{k+1} \in \Omega_m$`.
>
> Therefore, the corresponding natural parameter must satisfy `$\mathbf{\eta}_{k+1} \in \Omega_\eta$`.
>
> By our hypothesis, `$\mathbf{\eta}_{k+1}$` is updated according to `$\eqref{5}$` and it must satisfy the natural parameter constraint.
>
> However, it is obvious that `$\eqref{5}$` in general does not satisfy the natural parameter constraint when the step-size $\alpha$ is
> large enough since `$\Omega_\eta$`  is just a proper open subset of `$\mathcal{R}^K$`.
>
> This is a contradiction.

## Proximal NGD,  Projected NGD, and (Constrained) Mirror Descent

As we discussed before, natural-gradient descent and mirror desecent in general are **distinct** methods when the natural parameter space `$\Omega_\eta$` is constrained.

A straightforward approach from natural-gradient descent is the projected natural-gradient descent.
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \arg\min_{ \color{blue} {z} \in \Omega_\eta} \|\eta_k - \alpha
\mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k) -\color{blue} {\mathbf{z}} \|^2_{ \color{red}{ \mathbf{F}_\eta(\eta_k)} }
\end{aligned}\tag{6}\label{6}
$$` where we should use 
the [weighted inner product]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#distance-induced-by-the-fisher-rao-metric) with the same FIM highlighted in red.


On the other hand, the constrained mirror descent in the expectation space remains the same as in  `$\eqref{4}$`. 
`$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{ \color{blue} {x} \in \Omega_m}\{ \langle \nabla_m \ell(\mathbf{m}_k), \color{blue}{\mathbf{x}}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\color{blue}{\mathbf{x}},\mathbf{m}_k) \}
\end{aligned}
$$`
where 
`$\nabla_m \ell(\mathbf{m}_k) = \nabla_m f( \underbrace{ \eta(\mathbf{m}_k)}_{=\eta_k} )=  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$`.

We could also perform the constrained mirror descent in the natural parameter space as
`$$
\begin{aligned}
\mathbf{\eta}_{k+1} \leftarrow \arg \min_{\color{blue}{y} \in \Omega_\eta}\{ \langle \nabla_\eta f(\mathbf{\eta}_k), \color{blue}{\mathbf{y}}-\mathbf{\eta}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A_\eta}(\color{blue}{\mathbf{y}},\mathbf{\eta}_k) \}
\end{aligned}\tag{7}\label{7}
$$`


Recall that in
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#natural-gradient-descent-as-unconstrained-proximal-gradient-descent),
we show that natural-gradient descent can be viewed as unconstrained proximal-gradient method, where we use the
second-order Taylor expansion of `$\mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\mathbf{y})]$` at `$y=\eta_k$`.
We could also obtain proximal natural-gradient descent without the Taylor expansion as

`$$
\begin{aligned}
\eta_{k+1} \leftarrow \arg\min_{\color{blue}{y} \in  \Omega_\eta  } \{ \langle \nabla_\eta f(\eta_k),\color{blue}{\mathbf{y}}-\eta_k \rangle   + \frac{1}{\alpha} \underbrace{ \mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\color{blue}{\mathbf{y}})]}_{ = \mathrm{B}_{A_\eta}(\mathbf{\eta}_k,\color{blue}{\mathbf{y}})}  \} 
\end{aligned}\tag{8}\label{8}
$$` 

These methods could be very difficult to solve since `$\Omega_m$` can be an arbitrary open subset in `$\mathcal{R}^K$`.




## Using an Adaptive Step-size

When the step-size `$\alpha$` is small enough, the connection between natural-gradient descent and mirror desecent could
still hold.

Therefore, one idea is to use an adaptive step-size to satisfy the parameter constraint at each iteration.
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha_k \nabla_m \ell(\mathbf{m}_k)
\end{aligned}\tag{9}\label{9}
$$` where 
`$\nabla_m \ell(\mathbf{m}_k) =  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$` and the step-size `$\alpha_k$` is selected  so that
`$\eta_{k+1} \in \Omega_\eta$`.

Since `$\Omega_m$` is a open set in `$\mathcal{R}^K$`, this update is valid when the step-size `$\alpha_k$` is small enough.

However, for a general parameter constraint `$\Omega_m$`, this approach can be inefficient due to the selection precedure and will often select an extremally small step-size
`$\alpha_k$`,
which greatly slows down the progression of the method.

## Riemannian Gradient Descent

An alternative approach is to use Riemannian gradient descent as we discussed in 
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#riemannian-gradient-descent-and-its-non-linear-invariance), which is a generalization of natural-gradient descent. 
Note that this approach cannot be derived from mirror descent.

To avoid solving the geodeisc ODE to get the manifold exponential map, we could use an (inexact) geodesic, which
induces a retraction map.
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \mathrm{Ret}_{\eta_k} (- \alpha  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k) )  
\end{aligned}\tag{10}\label{10}
$$` 

As mentioned in 
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#natural-gradient-descent-as-inexact-riemannian-gradient-descent),
we have to carefully select a retraction map to handle the parameter constraint.

However, for a general parameter constraint `$\Omega_m$`, it can be difficult to come out an efficient retraction map to satisfy
the constraint.
