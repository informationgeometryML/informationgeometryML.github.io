---
title: 'Part I: Smooth Manifolds with the Fisher-Rao Metric'
date: 2021-09-06
permalink: /posts/2021/09/Geomopt01/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---


Goal
------
This blog post focuses on the Fisher-Rao metric, also known as the Fisher information matrix (FIM). The purpose is to show that,
* The FIM plays an essential role in statistics and machine learning 
* For a parametric distribution, it induces a **Riemannian** geometric-structure 

The discussion here is informal and focuses on more on intuitions, rather than rigor.

<div class="notice--info" markdown="1">
<details>
<summary>click to see how to cite this blog post</summary>
<fieldset class="field-set" markdown="1">

```latex
@misc{lin2021blog01,
  title = {Smooth Manifolds with the Fisher-Rao Metric},
  author = {Lin, Wu and Nielsen, Frank and Khan, Mohammad Emtiyaz and Schmidt, Mark},
  url = {https://informationgeometryml.github.io/posts/2021/09/Geomopt01/}, 
  howpublished = {\url{https://informationgeometryml.github.io/posts/2021/09/Geomopt01/}},
  year = {2021},
  note = {Accessed: 2021-09-06}
}
```
</fieldset>
</details>
</div>



# Motivation
------
Let's start with some motivation: why should we care about geometric structures of probability distributions?

The answer is that by exploiting geometric structures, we could
* design efficient and simple algorithms for machine learning {% cite amari1998natural  %}
* design methods that are robust and invariant to re-parametrization {% cite lin2021tractable %} 
* understand models and algorithms through the lens of differential geometry, information geometry, and invariant theory {% cite  liang2019fisher %}


Probability distributions form the backbone of majority of machine-learning approaches, for example, any approach that uses probabilistic modeling is built upon such distributions. 
For all such cases, we can exploit the underlying geometric structure, induced by the Fisher-Rao metric, which is the topic of this blog. 

Below, we give some common examples from machine learning, where probability distributions naturally arise.
In these cases, we can exploit the structure to design efficient learning algorithms.


> Least Square (Empirical Risk Minimization): 
>
> Given N input-output pairs `$(x_i,y_i)$`,  the least-square loss can be viewed as an expectation under a probability distribution.
>`$$
>\begin{aligned}
>\min_{\tau}  \frac{1}{2n} \sum_{i=1}^{n}   (y_i-x_i^T\tau)^2 + \frac{1}{2}\log(2\pi) 
>  &= - \frac{1}{n}  \sum_{i=1}^{n}  \log \mathcal{N}(y_i | x_i^T\tau,1) \\
>& \approx \int [ -\log p(x,y | \tau) + \log p(x) ]  d p(x,y | \tau) \\
>& =  E_{ \color{red}  { p(x,y | \tau) } } [ - \log  p(x,y | \tau)    ] + \underbrace{ E_{  p(x)  } [  \log  p(x)    ]}
> _{ \text{ constant w.r.t. } \tau }
>\end{aligned} \tag{1}\label{1}
>$$`
>Here `$ p(x,y | \tau) = \mathcal{N}(y | x^T\tau,1) p(x) $` is assumed to be the data-generating distribution, and the least-square loss is the finite-sample approximation of the expectation. The normal distribution is denoted by `$ \mathcal{N} (y | x^T\tau,1) $` with mean `$ x^T\tau $` and variance `$ 1 $`.
>
> Algorithms such as  [**Fisher scoring**](https://en.wikipedia.org/wiki/Scoring_algorithm)  and **(emprical) natural-gradient descent** {% cite   martens2020new %} are commonly used methods that exploit the geometric structure of `$p(x,y | \tau)$`.





> Variational Inference: 
>
> Given a prior `$ p(z) $` and a likelihood `$ p(\mathcal{D} | z ) $` over an latent vector `$z$` and known data `$ \mathcal{D} $`, we can approximate the exact posterior `$ p( z | \mathcal{D} ) =\frac{p(z,\mathcal{D})}{p(\mathcal{D})} $` by optimizing a variational objective with respect to  an approximated distribution `$ q(z | \tau) $`:
> `$$
>\begin{aligned}
>\min_{\tau} \mathrm{KL} [ { q(z | \tau) || p( z | \mathcal{D} ) }   ] = E_{ \color{red}  {q(z | \tau)} } [ \log q(z | \tau) - \log p( z , \mathcal{D} )    ]  + \underbrace{\log p(\mathcal{D} )}_{ \text{ constant w.r.t. } \tau } 
>\end{aligned} \tag{2}\label{2}
>$$`
>where `$ \mathrm{KL} [ q(z) ||  p(z) ] :=  E_{ {q(z)} } [ \log \big(\frac{q(z)}{p(z)}\big) ]$` is the Kullbackâ€“Leibler divergence.
>
>The **natural-gradient variatioal inference** {% cite khan2017conjugate  %} is an algorithm that speeds up the inference by exploiting the geometry of `$q(z|\tau)$` induced by the Fisher-Rao metric.



> Evolution Strategies (Gradient-free Search): 
>
> In gradient-free optimization, we often use a search distribution `$ \pi(a | \tau ) $` to find the optimal solution of an objective funtion `$h(a)$` by solving the following problem:
>`$$
>\begin{aligned}
>\min_{\tau}  E_{ \color{red}  {\pi(a | \tau)} } [ h(a)  ]
>\end{aligned} \tag{3}\label{3}
>$$`
> The **natural evolution strategies** {% cite wierstra2014natural %} is an algorithm that speeds up the search process by exploiting the geometry of `$\pi(a|\tau)$`.
> In the context of reinforcement learning,  `$ \pi(a | \tau ) $` is known as the policy distribution to generate actions and the natural evolution strategies is known as the **natural policy gradient** method {%cite kakade2001natural %}. 


In these examples, we can express the objective function in terms of an expectation under a parametric family highlighted in red. 
By doing so, we can exploit the geometric structure of a parametric family denoted by `$ p(w|\tau) $` induced by the Fisher-Rao metric.


| Example  &nbsp; &nbsp; &nbsp;    | `$w$`   &nbsp; &nbsp; &nbsp;     | `$ p(w|\tau) $`  |
|:------------|:-------------:| -----:|
| Least Square     | observation `$(x,y)$`  | `$p(x,y|\tau)$` |
| Variational Inference     |  latent variable $z$    |  `$q(z|\tau)$` |
| Evolution Strategies     |  decision variable $a$   |   `$\pi(a|\tau)$` |



<div class="notice--success" markdown="1">
**Note**:

In machine learning applications, these algorithms such as {% cite lin2021tractable %}  {% cite   martens2020new %} {% cite khan2017conjugate %} {%cite wierstra2014natural %}  {%cite kakade2001natural %} can be efficiently implemented without
explicitly computing the inverse of the FIM.

See 
[Part V]({{ site.baseurl }}{% post_url 2021-12-14-Geomopt05 %}#efficient-natural-gradient-computation) for an
example.
</div>


We will mainly focus on the geometric structure of (finite-dimensional) parametric families.
For example, let's consider a 1-dimensional Gaussian family.
The following figure illustrates four distributions in a Gaussian family denoted by
`$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$`, where `$ p(w | \tau ): = \mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $`  and `$\tau :=(\mu,\sigma) $`.

![Figure 2]({{ site.baseurl }}/img/gauss1d.png "Source:Wikipedia") 



# Intrinsic Parameterizations[^1]
------
We start by discussing parameterizations for a distribution family. As we will see, a proper parameterization plays an important role in defining the FIM.
We can not use an arbitrary differentiable parameterization to represent a geometric structure.
This structure is also known as a manifold structure {% cite tu2011introduction  %}. A (smooth) manifold should be locally like a "flat" vector space. For example, consider the (curved) Earth surface. It looks like a (locally) flat space to us. 

The main reason of using an intrinsic parameterization is (1) the topology of a parameter space is nice. (2) The (exact) FIM is non-singular and well-defined (finite).
These properties will play a key role in [Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#natural-gradient-descent-in-an-intrinsic-parameter-space) for natural-gradient descent.

We require that a manifold should be locally like a vector space denoted by `$ E $` under a parameterization. Informally, we refer to 
such structure as a local vector-space structure.

<div class="notice--success" markdown="1">
Local **vector-space structure**:

It supports local **vector additions**,  local **real scalar products**, and their algebraic laws (i.e., the distributive law). (see [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional) for the details.) 
</div>

Intuitively, this vector-space structure means that a local (small) perturbation $ E $ at each point should not take out of the parameter space.
Technically, the parameter space denoted by $\Omega_\tau$ should be an **open** set in `$\mathcal{R}^K$`, where `$K$` is the number of entries of a parameter array.
As we will see soon, the FIM is a `$K$`-by-`$K$` matrix.

To illustrate this, let's consider a unit circle in $\mathcal{R}^2$ as shown in the Figure.
Clearly, a point $ (0,1) $ highlighted in green is in the circle, where we consider its center as the origin.

<img src="/img/circle.png" title="Source:Wikipedia" width="300"/>

>Parametrization 1 (Intrinsic parameterization):
>
>A (local) parametrization at $ (0,1) $ highlighted in red for the circle is
>`$\{ (t,\sqrt{1-t^2}) | -h<t<h \} $`, where $h=0.1$. We use **one** (scalar) parameter in this parametrization. 
>
>The manifold is (locally) "flat" since we can always find a small **1-dimensional** perturbation $E$ in the **1-dimensional** parameter space  `$\Omega_t=\{t|-h<t<h \} $`. 
>
><img src="/img/1d-perturbation.png" title="Fig" width="300"/>
>
>This parametrization is called an **intrinsic** parameterization.
>
>We can similarly define a (local) parametrization at each point of the circle. In fact, we can use  four (local) parameterizations to represent the circle as shown below.
>
><img src="/img/charts.png" title="Source:Wikipedia" width="200"/>

>Parametrization 2 (Non-intrinsic parameterization):
>
>Let's define a map `$f : [0,2\pi) \rightarrow \mathcal{S}^1 $` such that `$f(\theta) = (\sin \theta, \cos \theta ) $`, where we use $\mathcal{S}^1$ to denote the circle.
>
>A (global) parametrization of the circle is `$\{ f(\theta) | \theta \in [0,2\pi)  \}$`, where we use one (scalar) parameter. 
>
>This map $f$ is bijective and smooth. However, the parameter space is not open in $\mathcal{R}$. Moreover, its inverse map $f^{-1}$ is **not** continous at point $(0,1) \in  \mathcal{S}^1$.
>
>This smoothness requirement of the inverse map is essential when it comes to reparametrization (A.K.A. parameter transformation). The requirement gives us a way to generate new intrinsic parameterizations. In other words, the Jacobian matrix is non-singular if the requirement is satisfied. Thus, we can safely use the chain rule and inverse function theorem.



>Parametrization 3 (Non-intrinsic parameterization):
>
>The circle does **not** look like a flat space under the following parametrization
>`$\{ (x,y) | x^2+y^2=1, x,y \in \mathcal{R} \} $`. The number of entries in this parameter array is 2. 
>
>The reason is that we cannot find a small **2-dimensional** perturbation $E$ in the **2-dimensional** parameter space `$\Omega_{\tau}=\{(x,y)|x^2+y^2=1 \} $` due to the constraint $x^2+y^2=1$. In other words, $\Omega_\tau$ is not open in $\mathcal{R}^2$.
>
><img src="/img/2d-perturbation.png" title="Fig" width="300"/>





# Intrinsic Parameterizations for Parametric families
------
Now, we discuss how to choose a parameterization given a parametric family so that we can exploit the geometric structure {% cite amari2016information %} induced by the Fisher-Rao metric.

Given a parametric distribution family `$ p(w|\tau) $` indexed by its parameter `$\tau$`, `$ p(w|\tau) $` should be smooth w.r.t. `$ \tau $` by considering `$ w $` to be fixed.
We say a parametrization is **intrinsic** if the following condition for parameter (array) `$\tau $` holds:


<div class="notice--success" markdown="1">
**Regularity Condition**: 

The set of partial derivatives 
`$ \{ \partial_{\tau_i} \log p(w|\tau) \} $`  should be linearly independent. 

In other words, `$\sum_i c_i \partial_{\tau_i} \log p(w|\tau)= 0 $` holds only when constant `$c_i$` is zero and the value of `$c_i$` does not depent on  `$w$`.
</div>

Note that this regularity condition implicitly assumes that the parameter space `$\Omega_\tau$` is an open set in `$\mathcal{R}^K$` due to the definition of the partial derivatives, where K is the number of entries in parameter array `$\tau$`.
We will discuss more about this at [here](#caveats-of-the-fisher-matrix-computation).


We will use the following examples to illustrate this condition.


>Example 1 (Intrinsic parameterization): 
>
>We will show that the regularity condition holds. Consider a 1-dimensional Gaussian family `$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with mean `$\mu$`, variance `$\sigma$`, and parametrization `$\tau = (\mu,\sigma) $`.
>The partial derivatives are
>`$$
>\begin{aligned}
> \partial_{\mu} \log \mathcal{N}(w |\mu,\sigma) = \frac{w-\mu}{\sigma}, \,\,\, \partial_{\sigma} \log \mathcal{N}(w |\mu,\sigma) = \frac{ (w-\mu)^2 }{2 \sigma^2} - \frac{1}{2\sigma} 
>\end{aligned}
>$$`
> 
>`$$
>\begin{aligned}
> \partial_{\mu} \log \mathcal{N}(w |\mu,\sigma) \Big|_{\mu=0,\sigma=1}= w, \,\,\, \partial_{\sigma} \log \mathcal{N}(w |\mu,\sigma) \Big|_{\mu=0,\sigma=1} = \frac{ w^2 -1 }{2}  
\end{aligned}
>$$`
>If `$ c_1 w + c_2  (\frac{ w^2 -1 }{2}) = 0$` holds for any $w$, we have `$c_1=c_2=0$`, which implies  linear independence. 
>
>Similarly, we can show that for any `$\mu \in \mathcal{R}$` and `$\sigma >0$`, 
>the partial derivatives are linearly independent.


>Example 2 (Non-intrinsic parameterization): 
>
>We will show that the regularity condition fails. Consider a Bernoulli family `$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0>0, \pi_1>0  \}$` with parameter `$\tau = (\pi_0,\pi_1) $`, where function `$ \mathcal{I}(\cdot) $` is the indicator function.
>The partial derivatives are
>
>`$$
>\begin{aligned}
> \partial_{\pi_0} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{\pi_1}{(\pi_0+\pi_1)^2}   
\end{aligned}
>$$`
>`$$
>\begin{aligned}
>\partial_{\pi_1} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} 
>\end{aligned}
>$$`
>Note that when `$c_0 = \pi_0 \neq 0 $` and `$c_1= \pi_1 \neq 0$`, we have `$c_0 \frac{\pi_1}{(\pi_0+\pi_1)^2}  + c_1 \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} = 0$`. 
>
>Therefore, we can show that 
>the partial derivatives are linearly dependent.

>Example 3 (Non-intrinsic parameterization):
>
>We will soon show that the  condition fails for Bernoulli family  `$ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \}$` with parameter `$\tau = (\pi_0,\pi_1)$`. 
>The main reason is that the parameter space is not open in $\mathcal{R}^2$. 

>Example 4  (Intrinsic parameterization):
>
>We can show that the condition holds for Bernoulli family `$\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) (1-\pi_0)  \Big| 0<\pi_0<1  \}$` with parameter `$\tau = \pi_0$`.



# Fisher-Rao Metric
------
As we can see from the [previous section](#motivation), we can use the Fisher-Rao metric to design fast and efficient algorithms.
In statistics, this metric is closely related to  maximum likelihood estimation, central limit theorem, and principle of maximum entropy.
These statistical principles  {%cite casella2002statistical %} {%cite  jaynes1957information %} play essential roles in training and estimating machine learning models.
Moreover, the Fisher-Rao metric has been theorically and emprically used in machine learning and statistics. 


Given an intrinsic parameterization, we can define the Fisher-Rao metric under this parameterization as:
`$$
\begin{aligned}
F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau) \Big) ].
\end{aligned}
$$`
Note that the metric could be ill-defined since the expectation may not exist.


Given a parameterization,  we can express the metric in a matrix form as

`$$
\begin{aligned}
\mathbf{F}(\tau) := E_{ p(w|\tau) }  [ \Big( \nabla_{\tau} \log p(w|\tau ) \Big)  \Big(\nabla_{\tau} \log (w|\tau) \Big)^T ],
\end{aligned}
$$`
where $K$ is the number of entries of parameter array $\tau$ and 
`$ \nabla_{\tau} \log p(w|\tau ) := [ \partial_{\tau_1} \log p(w|\tau ), \cdots, \partial_{\tau_K} \log p(w|\tau ) ]^T  $` is a column vector.


The matrix form is also known as the **Fisher information matrix** (FIM). Obviously, the FIM depends on the choice of parameterizations. In many cases,  we could also compute the FIM as
`$ \mathbf{F}(\tau) := - E_{p(w|\tau) }  [  \nabla_{\tau}^2 \log p(w|\tau )  ]$`.



The regularity condition guarantees that the FIM is non-singular if the matrix exists.
The condition is also related to the model identification in maximum likelihood estimation.

In the following discussion, we will assume the metric is well-defined.
In such cases, the Fisher-Rao metric is a valid Riemannian metric {% cite lee2018introduction %} since the corresponding FIM is positive definite everywhere in an **intrinsic** parameter space.

<div class="notice--danger" markdown="1">
**Warning**:
An arbitrary Riemannian metric often is NOT useful for applications in machine learning and statistics.
</div>



Recall that when we perform a parameter transformation, we require both the transformation map and its inverse map to be smooth.
If this smoothness requirement[^2] is satisfied, we can obtain a new intrinsic parameterization via the transformation.
In [Part III]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#Pparameter-transform-and-invariance),
we will show the FIM is also positive definite under this new intrinsic parameterization.




# Caveats of the Fisher matrix computation
------
There are some caveats when it comes to the Fisher matrix computation. In particular, the regularity condition should be satisfied.
It is possible to define the FIM under a non-intrinsic parameterization. However, the FIM often is singular or ill-defined under a non-intrinsic  parameterization as shown below.

>Example 1 (Ill-defined FIM):
>
>Consider Bernoulli family  `$ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \}$` with non-intrinsic parameter `$\tau = (\pi_0,\pi_1)$`.
> The following computation is not correct. Do you make similar mistakes like this?
>
>Let `$  p(w|\tau ) =  \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1$`, where `$\tau = (\pi_0,\pi_1)$`. The derivative is
> `$$  \begin{aligned} \nabla_{\tau} \log p(w|\tau ) = \frac{1}{p(w|\tau )} [\mathcal{I}(w=0) ,  \mathcal{I}(w=1)]^T  \end{aligned} \tag{4}\label{4}$$`
>Thus, by Eq. `$\eqref{4}$`, the FIM under this  parameterization is
>
>`$$  \begin{aligned} F(\tau) = E_{p(w|\tau) } [ \frac{1}{p^2(w|\tau)} \begin{bmatrix} \mathcal{I}^2(w=0) &  \mathcal{I}(w=1)  \mathcal{I}(w=0) \\  \mathcal{I}(w=0)  \mathcal{I}(w=1) &  \mathcal{I}^2(w=1) \end{bmatrix} ]   = \begin{bmatrix} \frac{1}{\pi_0} &  0 \\ 0 &  \frac{1}{\pi_1} \end{bmatrix} \end{aligned}$$`
><div class="notice--danger" markdown="1">
>This computation is not correct. Do you know why?
></div>
>
><div class="notice--info" markdown="1">
><details>
><summary>Reason: (Click to expand)</summary>
><fieldset class="field-set" markdown="1">
>The key reason is the equality constraint `$ \pi_0+\pi_1=1 $`. Thus, Eq. `$\eqref{4}$` is **incorrect**.
>
>By definition, a Bernoulli distribution is valid only when the constraint holds. 
Thus, the constraint `$ \pi_0+\pi_1=1 $` must be satisifed when we compute the Fisher matrix since the computation involves computing the expectation w.r.t. this distribution.
>
> Note that the gradient is defined as `$ \nabla_{\tau} \log p(w|\tau ) := [ \partial_{\pi_0} \log p(w|\tau ), \partial_{\pi_1} \log p(w|\tau ) ]^T $`.
>
>Unfortunately, these partial derivatives do not exist. By the definition of  partial derivative `$ \partial_{\pi_0} \log p(w|\tau )$`, we fix `$\pi_1$` and allow `$\pi_0$` to change.
However, given that `$\pi_1$` is fixed and `$ \pi_0 $` is fully determined by `$\pi_1$` due to the equality constraint `$ \pi_0+\pi_1=1 $`. 
>Therefore, `$  \partial_{\pi_0} \log p(w|\tau ) $` is not well-defined.
>In other words, the above Fisher matrix computation is not correct since `$ \nabla_{\tau} \log p(w|\tau ) $` does not exist. 
></fieldset>
></details>
></div>


 
>Example 2  (Singular FIM):
>
>Consider Bernoulli family `$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0>0, \pi_1>0  \}$` with  non-intrinsic parameter `$\tau = (\pi_0,\pi_1) $`.
>
>The FIM under this  parameterization is singular as shown below. 
>
>Let `$  p(w|\tau ) =  \mathcal{I}(w=0)  \frac{\pi_0}{\pi_0+\pi_1}+ \mathcal{I}(w=1)  \frac{\pi_1}{\pi_0+\pi_1}$`, where `$\tau = (\pi_0,\pi_1)$`. The derivative is
>
> `$$ 
> \begin{aligned}
> \nabla_{\tau} \log p(w|\tau ) = \frac{ \mathcal{I}(w=0) - \mathcal{I}(w=1)  }{p(w|\tau )} [\frac{\pi_1}{(\pi_0+\pi_1)^2} , \frac{-\pi_0}{(\pi_0+\pi_1)^2}]^T 
> \end{aligned}
> $$`
>
>
>Thus, the FIM under this  parameterization is
>`$$
>\begin{aligned} 
>F(\tau) = E_{p(w|\tau) } [ \frac{( \mathcal{I}(w=0) - \mathcal{I}(w=1))^2 }{p^2(w|\tau)} \begin{bmatrix} \frac{\pi_1^2}{(\pi_0+\pi_1)^4}  & \frac{-\pi_0 \pi_1}{(\pi_0+\pi_1)^4} \\ \frac{-\pi_0 \pi_1}{(\pi_0+\pi_1)^4}  &  \frac{\pi_0^2}{(\pi_0+\pi_1)^4} \end{bmatrix} ]   =\frac{1}{(\pi_0+\pi_1)^2} \begin{bmatrix} \frac{\pi_1}{\pi_0} &  -1 \\ -1 &  \frac{\pi_0}{\pi_1} \end{bmatrix}
>\end{aligned}
>$$`
> where this FIM is singular since the matrix determinant is zero as shown below. `$$
> \begin{aligned}
> \mathrm{det}\big( \begin{bmatrix} \frac{\pi_1}{\pi_0} &  -1 \\ -1 &  \frac{\pi_0}{\pi_1} \end{bmatrix}\big) = 0.
> \end{aligned}
> $$`

Now, we give an example to show that the FIM of a Bernoulli family can be non-singular when we use an instrinsic parameterization.

>Example 3  (Non-singular FIM):
>
>Consider Bernoulli family `$ \{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi)  \Big| 0<\pi<1 \}$` with  **intrinsic** parameter `$\tau = \pi $`.
>
>The FIM under this parameterization is non-singular as shown below.
>
>Let `$  p(w|\tau ) =  \mathcal{I}(w=0) \pi+ \mathcal{I}(w=1) (1-\pi)$`, where `$\tau = \pi$`. The derivative is
> `$$
> \begin{aligned} \nabla_{\tau} \log p(w|\tau ) = \frac{\mathcal{I}(w=0) - \mathcal{I}(w=1)}{ \mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) }  \end{aligned}
> $$`
>
>Thus, the FIM under this  parameterization is
>`$$
>\begin{aligned}
>F(\tau) &= E_{p(w|\tau) } [ \frac{ (\mathcal{I}(w=0) - \mathcal{I}(w=1) )^2 }{ (\mathcal{I}(w=0) \pi + \mathcal{I}(w=1) (1-\pi) )^2 }  ]\\
> & = \pi \frac{1^2}{ \pi^2} + (1-\pi) \frac{(-1)^2}{(1-\pi)^2} \\
> &= \frac{1}{\pi } + \frac{1}{1-\pi} = \frac{1}{\pi(1-\pi)}> 0
> \end{aligned}
> $$`
>




# Dimensionality of a manifold
------
We can define the dimension of a manifold by using the  degrees of freedom of an intrinsic parametrization.
Mathematically speaking, we can show that any intrinsic parametrization of a manifold has the same degrees of freedom {% cite tu2011introduction %}.
This also gives us a tool to help identify non-manifold cases.
We now illustrate this by examples.


unit circle | open unit ball  | closed unit ball
:-------------------------:|:-------------------------:|:-------------------------:
<img src="/img/circle-org.png" alt="Source:Wikipedia" width="200"/> | <img src="/img/open-ball.png" alt="Source:Wikipedia" width="200"/> | <img src="/img/closed-ball.png" alt="Source:Wikipedia" width="200"/>
1-dim manifold    |  2-dim manifold | non-manifold, which is indeed a manifold with (closed) boundary

As we shown in [the previous section](#intrinsic-parameterizations), a unit circle is a 1-dimensional manifold.  We can similarly show that an open unit ball is a 2-dimensional manifold.

However, a closed
unit ball is NOT a manifold since its interior is  an open unit ball and its boundary is a unit circle.
The  circle and the open unit ball do not have the same dimensionality.



For statistical  manifolds, 
 consider the following  manifolds. We will discuss more about them in [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-steepest-direction).

1-dim Gaussian with zero mean  | $d$-dimensional Gaussian with zero mean
:-------------------------:|:-------------------------:
 `$ \{ \mathcal{N}(w |0,s^{-1}) \Big|  s>0 \}$` with precision `$s$` <br /> under intrinsic parameterization `$\tau = s $` | `$ \{ \mathcal{N}(\mathbf{w} |\mathbf{0},\mathbf{S}^{-1}) \Big| \mathrm{MatH}(\tau) = \mathbf{S}   \succ \mathbf{0} \}$` with precision `$\mathbf{S}$`  <br /> under intrinsic parameterization `$\tau = \mathrm{vech}(\mathbf{S})$` is  a $\frac{d(d+1)}{2}$-dim array. 
1-dim statistical manifold | `$\frac{d(d+1)}{2}$`-dim statistical  manifold
 

We  use 
the [half-vectorization map](https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization) `$\mathrm{vech}()$`.
The map `$\mathrm{vech}(\mathbf{S})$`  returns  a `$\frac{d(d + 1)}{2}$`-dim array obtained by vectorizing only the lower triangular part of a (symmetric) `$d$`-by-`$d$` matrix `$\mathbf{S}$`.
The map `$\mathrm{MatH}()$` is the inverse map of  `$\mathrm{vech}()$`.

<div class="notice--info" markdown="1">
<details>
<summary>Illustration of these maps (click to expand)</summary>
<fieldset class="field-set" markdown="1">
> Consider the following symmetric 2-by-2 matrix
> 
> `$$
> \mathbf{S} = 
>\begin{aligned}
> \begin{bmatrix} 2 &  -1 \\  -1  & 3  \end{bmatrix}
>\end{aligned}
> $$`
> The output of map `$\mathrm{vech}(\mathbf{S})$` is
> `$$
> \mathbf{v}:=\mathrm{vech}(\mathbf{S}) = 
>\begin{aligned}
> \begin{bmatrix} 2  \\  -1  \\ 3  \end{bmatrix}
>\end{aligned}
> $$`
>
> The output of  map `$\mathrm{MatH}(\mathbf{v})$` is
> `$$
> \mathrm{MatH}(\mathbf{v}) = 
>\begin{aligned}
> \begin{bmatrix} 2 &  -1 \\  -1  & 3  \end{bmatrix}
>\end{aligned}
> $$`
</fieldset>
</details>
</div>


------
# References
{% bibliography --cited %}


## Footnotes:
[^1]: In differential geometry, an intrinsic parametrization is known as a coordinate chart.

[^2]: In differential geometry, the smoothness requirement is known as a diffeomorphism.

