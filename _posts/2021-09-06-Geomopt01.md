---
title: 'Information Geometry in Machine Learning (1)'
date: 2021-09-06
permalink: /posts/2021/09/Geomopt01/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Goal
------
This blog post should help readers to understand the Fisher-Rao metric also known as the Fisher information matrix (FIM).
* FIM plays an essential role in statistics and machine learning 
* FIM introduces geometric structures of a (parametric) distribution family

# Motivation
------
Probabilistic modeling is a popular approach in machine learning by using distribution families.
We want to exploit (hidden) structures of distribution families induced by the Fisher-Rao metric.
Applications of the metric in machine learning are natural-gradient methods as shown in the following examples.

> Example 1: 
> given a set of features $X$ and labels $Y$, we can view a linear regression problem 
as 
> `$$
\begin{aligned}
\min_{\tau} \frac{1}{2n} \sum_{i=1}^{n}   (y_i-x_i^T\tau)^2   + \mathrm{Constant} & = - \frac{1}{n}  \sum_{i=1}^{n}  \log \mathcal{N}(y_i | x_i^T\tau,1) \\
& \approx - E_{ \color{red} {p(x,y | \tau)} } [ \log p(x,y | \tau) ] 
\end{aligned} \tag{1}\label{1}
$$`
>where `$ p(x,y | \tau) = \mathcal{N}(y | x^T\tau,1) p(x) $` and  `$ \mathcal{N} (y | x^T\tau,1) $`  is a Gaussian distribution with mean `$ x^T\tau $` and variance `$ 1 $`.
>
>The **Fisher scoring algorithm**  can be derived from Example 1. The **emprical natural-gradient descent** could also be obtained in this example.

<!--- \stackrel{\eqref{1}} (Eq. `$\eqref{1}$`) --->


> Example 2: 
> given a prior $$ p(z) $$ and a likelihood `$ p(\mathcal{D} | z ) $` with observations $ \mathcal{D} $, we can approximate the exact posterior `$ p( z | \mathcal{D} ) =\frac{p(z,\mathcal{D})}{p(\mathcal{D})} $` by solving a variational inference problem with 
 an approximated distribution `$ q(z | \tau) $` as
> `$$
\begin{aligned}
\min_{\tau} \mathrm{KL} [ { q(z | \tau) || p( z | \mathcal{D} ) }   ] = E_{ \color{red}  {q(z | \tau)} } [ \log q(z | \tau) - \log p( z , \mathcal{D} )    ]  + \log p(\mathcal{D} ) 
\end{aligned} \tag{2}\label{2}
$$`
>where `$ \mathrm{KL} [ q(z) ||  p(z) ] :=  E_{ {q(z)} } [ \log \big(\frac{q(z)}{p(z)}\big) ]$` is the Kullbackâ€“Leibler divergence.
>
>The **natural-gradient variatioal inference** can be derived from Example 2 for the Bayesian inference. 



> Example 3: 
> in gradient-free optimization, we often use a search distribution `$ \pi(a | \tau ) $` to find the optimal solution of an objective funtion `$h(a)$` by solving
>`$$
\begin{aligned}
\min_{\tau}  E_{ \color{red}  {\pi(a | \tau)} } [ h(a)  ]  
\end{aligned} \tag{3}\label{3}
$$`
> The **natural evolution strategies** can be derived from Example 3. In the context of reinforcement learning, we often use a policy distribution `$ \pi(a | \tau ) $` to generate actions.
> The **natural policy gradient** method can be derived for reinforcement learning.


In these examples, we express the objective function in terms of an expectation under a parametric family highlighted in red. 
We will use the Fisher-Rao metric to build a manifold structure on a parametric family denoted by `$ p(w|\tau) $`. 


| Example  &nbsp; &nbsp; &nbsp;    | `$w$`   &nbsp; &nbsp; &nbsp;     | `$ p(w|\tau) $`  |
|:------------|:-------------:| -----:|
| Example 1     | observation $(x,y)$  | `$p(x,y|\tau)$` |
| Example 2     |  latent variable $z$    |  `$q(z|\tau)$` |
| Example 3     |  decision variable $a$   |   `$\pi(a|\tau)$` |



Parametric families
------
We only consider (finite-dimensional) parametric families with a Riemannian manifold structure induced by the Fisher-Rao metric in these blog posts.


Parametric families are important since they are
* widely used in machine learning
* well-studied and suitable for high-dimensional tasks compared to non-parametric counterparts such as Stein variational gradient methods
* flexible by using a mixture such as a Gaussian mixture


The following figure illustrates four distributions in a 1-dimensional Gaussian family denoted by
`$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$`, where `$ p(w | \tau ): = \mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $`  and $\tau :=(\mu,\sigma) $.

![Figure 2](/img/gauss1d.png "Source:Wikipedia") 




# Intrinsic Parameterizations
------
Unfortunately, we can not use an arbitrary differentiable parameterization to build a manifold structure in a parametric distribution family.
A (smooth) manifold should be locally like a (flat) vector space. For example, consider the (curved) Earth surface looks like a (locally) flat space to us.

Thus, we require that a manifold should be locally like a vector space denoted by $ E $ under a parameterization.
In other words, we should allow a small perturbation $ E $ such as an open ball in a given parameter space.

To illustrate this, let's consider a unit circle in $\mathcal{R}^2$ as shown in the Figure.
Clearly, a point $ (0,1) $ highlighted in green is in the circle, where we consider its center as the origin.

<img src="/img/circle.png" title="Source:Wikipedia" width="300"/>

>Parametrization 1:
>
>A (local) parametrization at $ (0,1) $ highlighted in red for the circle is
>`$\{ (t,\sqrt{1-t^2}) | -h<t<h \} $`, where $h=0.1$.
>
>The manifold is (locally) flat since we can always find a small **1-dimensional** perturbation $E$ in the **1-dimensional** parameter space  `$\Omega_t=\{t|-h<t<h \} $`. 
>
><img src="/img/1d-perturbation.png" title="Fig" width="300"/>
>
>This kind of parametrization is called an **intrinsic** parameterization.
>
>We can similarly define a (local) parametrization at each point of the circle. In fact, we only need to use  four (local) parameterizations to represent the circle as shown below.
>
><img src="/img/charts.png" title="Source:Wikipedia" width="200"/>

>Parametrization 2:
>
>Let's define a map `$f : [0,2\pi) \rightarrow \mathcal{S}^1 $` such that `$f(\theta) = (\sin \theta, \cos \theta ) $`, where we use $\mathcal{S}^1$ to denote the circle.
>
>A (global) parametrization of the circle is `$\{ f(\theta) | \theta \in [0,2\pi)  \}$`.
>
>This map $f$ is bijective and smooth. However, its inverse map $f^{-1}$ is **not** continous at point $(0,1) \in  \mathcal{S}^1$.
>
>This smoothness requirement of the inverse map is essential when it comes to reparametrization (A.K.A. parameter transformation). The requirement gives us a way to generate new intrinsic parameterizations. In other words, the Jacobian matrix is non-singular if the requirement is satisfied. Thus, we can safely use the chain rule and inverse function theorem.
>
>We will **not** consider this parametrization as an intrinsic parameterization.


>Parametrization 3:
>
>The circle does **not** look like a flat space under the following parametrization
>`$\{ (x,y) | x^2+y^2=1, x,y \in \mathcal{R} \} $`
>
>The reason is that we cannot find a small **2-dimensional** perturbation $E$ in the **2-dimensional** parameter space `$\Omega_{xy}=\{(x,y)|x^2+y^2=1 \} $` due to the constraint $x^2+y^2=1$. 
>
><img src="/img/2d-perturbation.png" title="Fig" width="300"/>
>
>This parametrization is **not** an intrinsic parameterization.




# Intrinsic Parameterizations for Parametric families
------
When it comes to parametric distribution family $ p(w|\tau) $ indexed by its parameter $\tau$, the smoothness w.r.t. parameter $ \tau $ should hold for each $ w $ pointwisely.

We say a parametrization is intrinsic if the following **regularity condition** for parameter $\tau $ holds.
The set of partial derivatives 
$ \\{ \partial_{\tau_i} \log p(w\|\tau) \\}  $ should be linearly independent for all $ w $.


We will use two examples to illustrate this condition.


>Example 1: 
>
>We will show that the regularity condition holds. Consider a 1-dimensional Gaussian family `$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with parameter $\tau = (\mu,\sigma) $.
>The partial derivatives are
>`$$
\begin{aligned}
 \partial_{\mu} \log \mathcal{N}(w |\mu,\sigma) = \frac{w-\mu}{\sigma}, \,\,\, \partial_{\sigma} \log \mathcal{N}(w |\mu,\sigma) = \frac{ (w-\mu)^2 }{2 \sigma^2} - \frac{1}{2\sigma} 
\end{aligned} \tag{4}\label{4}
$$`
>For simplicity,  let $\mu=0$ and $\sigma=1$. We can simplify the partial derivatives as below.
> 
>`$$
\begin{aligned}
 \partial_{\mu} \log \mathcal{N}(w |\mu,\sigma) \Big|_{\mu=0,\sigma=1}= w, \,\,\, \partial_{\sigma} \log \mathcal{N}(w |\mu,\sigma) \Big|_{\mu=0,\sigma=1} = \frac{ w^2 -1 }{2}  
\end{aligned} \tag{5}\label{5}
$$`
>If $ c_1 w + c_2  (\frac{ w^2 -1 }{2}) = 0$ holds for any $w$, we have $c_1=c_2=0$, which implies  linear independence. 
>
>Similarly, we can show that for any $\mu \in \mathcal{R}$ and $\sigma >0$, 
the partial derivatives are linearly independent.


>Example 2: 
>
>We will show that the regularity condition fails. Consider a Bernoulli family `$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0>0, \pi_1>0  \}$` with parameter $\tau = (\pi_0,\pi_1) $, where function $ \mathcal{I}(\cdot) $ is the indicator function.
>The partial derivatives are
>
>`$$
\begin{aligned}
 \partial_{\pi_0} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{\pi_1}{(\pi_0+\pi_1)^2}   
\end{aligned} \tag{6}\label{6}
$$`
>`$$
\begin{aligned}
\partial_{\pi_1} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} 
\end{aligned} \tag{7}\label{7}
$$`
>Note that when $c_0 = \pi_0 \neq 0 $ and $c_1= \pi_1 \neq 0$, we have $c_0 \frac{\pi_1}{(\pi_0+\pi_1)^2}  + c_1 \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} = 0$. 
>
>Therefore, we can show that 
the partial derivatives are linearly dependent.

>Example 3:
>
>We will soon show that the  condition fails for Bernoulli family  `$ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \}$` with parameter $\tau = (\pi_0,\pi_1)$.  

>Example 4:
>
>The condition holds for Bernoulli family `$\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) (1-\pi_0)  \Big| 0<\pi_0<1  \}$` with parameter $\tau = \pi_0$.



# Fisher-Rao Metric
------
Given an intrinstic parameterization, we can define the Fisher-Rao metric under this parameterization as:
$ F_{ij}(\tau) := E_{p(w\|\tau) }  [ \Big( \partial_{\tau_i} \log p(w\|\tau ) \Big)  \Big(\partial_{\tau_j} \log (w\|\tau) \Big) ]$.
Note that the metric could be ill-defined since the expectation may not exist.



Given a parameterization,  we can express the metric in a matrix form as
$ \mathbf{F}(\tau) := E_{p(w\|\tau) }  [ \Big( \nabla_{\tau} \log p(w\|\tau ) \Big)  \Big(\nabla_{\tau} \log (w\|\tau) \Big)^T ]$,
where $K$ is the length of parameter vector $\tau$ and 
$ \nabla_{\tau} \log p(w\|\tau ) := [ \partial_{\tau_1} \log p(w\|\tau ), \cdots, \partial_{\tau_K} \log p(w\|\tau ) ]^T  $ is a column vector.
The matrix form is also known as the Fisher information matrix. Obviously, the Fisher matrix depends on the choice of parameterizations.


The regularity condition guarantees that the Fisher information matrix is non-singular if the matrix exists.
The condition is also related to the model identification in maximum likelihood estimation.

In the following statement, we will assume the metric is well-defined.
The Fisher-Rao metric is a valid Riemannian metric since the corresponding Fisher matrix is positive definite everywhere in an **intrinsic** parameter space.
The Fisher-Rao metric is special  since it is closely related to  maximum likelihood estimation, central limit theorem, and differential entropy.

<span style="color:red">**Warning**</span>: an arbitrary Riemannian metric often is NOT useful for applications in machine learning. 


Recall that when we perform a parameter transformation, we require both the transformation map and its inverse map to be smooth.
If this smoothness requirement is satisfied, we can obtain a new intrinsic parameterization via the transformation.
We can show the Fisher information matrix is positive definite under this new intrinsic parameterization due to the chain rule.



In many cases, we could also compute the Fisher matrix as
$ \mathbf{F}(\tau) := - E_{p(w\|\tau) }  [  \nabla_{\tau}^2 \log p(w\|\tau )  ]$.



Caveats of the Fisher matrix computation
------
There are some caveats when it comes to the Fisher matrix computation. In particular, the regularity condition should be satisfied.
It is possible to define the Fisher matrix under a non-intrinstic parameterization. However, the Fisher matrix can be singular or ill-defined under this parameterization as shown below.

>Example 1:
>
>Consider Bernoulli family  $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big\| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \\}$ with parameter $\tau = (\pi_0,\pi_1)$.
The following computation is not correct. Do you make similar mistakes like this?
>
>Let $  p(w\|\tau ) =  \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1$, where $\tau = (\pi_0,\pi_1)$. The derivative is
> `$$  
\begin{aligned}
\nabla_{\tau} \log p(w|\tau ) = \frac{1}{p(w|\tau )} [\mathcal{I}(w=0) ,  \mathcal{I}(w=1)]^T 
\end{aligned} \tag{8}\label{8}
$$`
>Thus, by Eq. `$\eqref{8}$`, the Fisher matrix is
>`$$ 
\begin{aligned}
F(\tau) = E_{p(w|\tau) } [ \frac{1}{p^2(w|\tau)} \begin{bmatrix} \mathcal{I}^2(w=0) &  \mathcal{I}(w=1)  \mathcal{I}(w=0) \\ 
\mathcal{I}(w=0)  \mathcal{I}(w=1) &  \mathcal{I}^2(w=1) \end{bmatrix} ]   = \begin{bmatrix} \frac{1}{\pi_0} &  0 \\ 0 &  \frac{1}{\pi_1} \end{bmatrix}
\end{aligned} \tag{9}\label{9}
$$`
>This computation is not correct. Do you know why it is not correct?
>
>The key reason is the equality constraint $ \pi_0+\pi_1=1 $. Thus, Eq. `$\eqref{8}$` is **incorrect**.
>
>By definition, a Bernoulli distribution is valid only when the constraint holds. 
Thus, the constraint $ \pi_0+\pi_1=1 $ must be satisifed when we compute the Fisher matrix since the computation involves the expectation of this distribution.
>
> Note that the gradient is defined as $ \nabla_{\tau} \log p(w\|\tau ) := [ \partial_{\pi_0} \log p(w\|\tau ), \partial_{\pi_1} \log p(w\|\tau ) ]^T $.
>
>Unfortunately, these partial derivatives do not exist. By the definition of  partial derivative $ \partial_{\pi_0} \log p(w\|\tau )$, we fix $\pi_1$ and allow $\pi_0$ to change.
However, given that $\pi_1$ is fixed and $ \pi_0 $ is fully determined by $\pi_1$ due to the equality constraint $ \pi_0+\pi_1=1 $. 
>
>Therefore, $  \partial_{\pi_0} \log p(w\|\tau ) $ is not well-defined.
>In other words, the above Fisher matrix computation is not correct since $ \nabla_{\tau} \log p(w\|\tau ) $ does not exist. 
 
>Example 2:
>
>Consider Bernoulli family `$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0>0, \pi_1>0  \}$` with parameter $\tau = (\pi_0,\pi_1) $.
>
>We can show that the Fisher information matrix is singular. 


(optional) Manifolds with (Closed) Boundary
------
We define the dimension of a manifold by using the dimension of an intrinsic parametrization. Formally, we can show that any intrinsic parametrization of a manifold has the same degrees of freedom. 
We now illustrate this by examples.

>Example 1:
>
>The unit circle dicussed before is a 1-dimensional manifold.
>
><img src="/img/circle-org.png" alt="Source:Wikipedia" width="200"/>

>Example 2:
>
>The open unit ball is a 2-dimensional manifold.
>
><img src="/img/open-ball.png" alt="Source:Wikipedia" width="200"/>

>Example 3:
>
>The closed unit ball is **not** a manifold. 
>
><img src="/img/closed-ball.png" alt="Source:Wikipedia" width="200"/>
>
>The main reason is that its boundary  does not have the same degrees of freedom as its interior.
>
>The closed ball is indeed a manifold with (closed) boundary. We will not consider this case in these blog posts.



