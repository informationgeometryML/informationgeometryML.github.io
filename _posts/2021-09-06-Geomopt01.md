---
title: 'Part I: Manifolds with the Fisher-Rao Metric'
date: 2021-09-06
permalink: /posts/2021/09/Geomopt01/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Goal
------
This blog post should help readers to understand the Fisher-Rao metric also known as the Fisher information matrix (FIM).
* FIM plays an essential role in statistics and machine learning 
* FIM induces a (Riemannian) geometric structure of a (parametric) distribution family

We will give an informal introduction with a focus on intuitions.

# Motivation
------
Probabilistic modeling is a popular approach in machine learning and often relies on (parametric) families of probability distributions.
In such cases, we can exploit the (hidden) geometric structure induced by the Fisher-Rao metric. 
Below, we give some examples of  probability  distributions that naturally arise in machine learning.


> Least Square (Empirical Risk Minimization): 
>
> Given N input-output pairs `$(x_i,y_i)$`,  the least-square loss can be viewed as expectation under a probability distribution.
> `$$
\begin{aligned}
\min_{\tau} \frac{1}{2n} \sum_{i=1}^{n}   (y_i-x_i^T\tau)^2   + \mathrm{Constant} & = - \frac{1}{n}  \sum_{i=1}^{n}  \log \mathcal{N}(y_i | x_i^T\tau,1) \\
& \approx - E_{ \color{red} {p(x,y | \tau)} } [ \log p(x,y | \tau) ] 
\end{aligned} \tag{1}\label{1}
$$`
>where we assume `$(x_i,y_i)$` is generated by a distribution `$ p(x,y | \tau) $` and  `$ p(x,y | \tau) = \mathcal{N}(y | x^T\tau,1) p(x) $`. `$ \mathcal{N} (y | x^T\tau,1) $`  is a Gaussian distribution with mean `$ x^T\tau $` and variance `$ 1 $`.
>
> Algorithms such as  **Fisher scoring** and **(emprical) natural-gradient descent** can be seen as methods that exploit the geometric structure of `$p(x,y | \tau)$` induced by the Fisher-Rao metric.

<!--- \stackrel{\eqref{1}} (Eq. `$\eqref{1}$`) --->


> Variational Inference: 
>
> Given a prior $$ p(z) $$ and a likelihood `$ p(\mathcal{D} | z ) $` over an latent vector `$z$` and known data $ \mathcal{D} $, we can approximate the exact posterior `$ p( z | \mathcal{D} ) =\frac{p(z,\mathcal{D})}{p(\mathcal{D})} $` by optimizing a variational objective with respect to  an approximated distribution `$ q(z | \tau) $`:
> `$$
\begin{aligned}
\min_{\tau} \mathrm{KL} [ { q(z | \tau) || p( z | \mathcal{D} ) }   ] = E_{ \color{red}  {q(z | \tau)} } [ \log q(z | \tau) - \log p( z , \mathcal{D} )    ]  + \log p(\mathcal{D} ) 
\end{aligned} \tag{2}\label{2}
$$`
>where `$ \mathrm{KL} [ q(z) ||  p(z) ] :=  E_{ {q(z)} } [ \log \big(\frac{q(z)}{p(z)}\big) ]$` is the Kullbackâ€“Leibler divergence.
>
>The **natural-gradient variatioal inference** is an algorithm that speeds up the inference by exploiting the geometry of `$q(z|\tau)$` induced by the Fisher-Rao metric.



> Evolution Strategies (Gradient-free Search): 
>
> In gradient-free optimization, we often use a search distribution `$ \pi(a | \tau ) $` to find the optimal solution of an objective funtion `$h(a)$` by solving the following problem:
>`$$
\begin{aligned}
\min_{\tau}  E_{ \color{red}  {\pi(a | \tau)} } [ h(a)  ]
\end{aligned} \tag{3}\label{3}
$$`
> The **natural evolution strategies** is an algorithm that speeds up the search process by exploiting the geometry of `$\pi(a|\tau)$`.
> In the context of reinforcement learning,  `$ \pi(a | \tau ) $` is known as the policy distribution to generate actions and the algorithm becomes the **natural policy gradient** method. 


In these examples, we can express the objective function in terms of an expectation under a parametric family highlighted in red. 
By doing so, we can exploit the geometric structure of a parametric family denoted by `$ p(w|\tau) $` induced by the Fisher-Rao metric.


| Example  &nbsp; &nbsp; &nbsp;    | `$w$`   &nbsp; &nbsp; &nbsp;     | `$ p(w|\tau) $`  |
|:------------|:-------------:| -----:|
| Least Square     | observation $(x,y)$  | `$p(x,y|\tau)$` |
| Variational Inference     |  latent variable $z$    |  `$q(z|\tau)$` |
| Evolution Strategies     |  decision variable $a$   |   `$\pi(a|\tau)$` |



We will mainly focus on the geometric structure of (finite-dimensional) parametric families.
As an example of a parametric family, let's consider a Gaussian family.
The following figure illustrates four distributions in a 1-dimensional Gaussian family denoted by
`$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$`, where `$ p(w | \tau ): = \mathcal{N}(w |\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $`  and $\tau :=(\mu,\sigma) $.

![Figure 2](/img/gauss1d.png "Source:Wikipedia") 




# Intrinsic Parameterizations
------
We start by discussing parameterizations for a distribution family. As we will see, a proper parameterization plays an important role in defining FIM.
We can not use an arbitrary differentiable parameterization to represent a geometric structure.
This structure is also known as a manifold structure. A (smooth) manifold should be locally like a "flat" vector space. For example, consider the (curved) Earth surface. It looks like a (locally) flat space to us. 

We require that a manifold should be locally like a vector space denoted by $ E $ under a parameterization.
A local vector-space structure means that we can do local vector additions and local real scalar products.
Intuitively, this vector-space structure means that a local (small) perturbation $ E $ at each point should not take out of the parameter space.
Technically, the parameter space denoted by $\Omega_\tau$ should be an open set in `$\mathcal{R}^K$`, where `$K$` is the number of (scalar) parameters.
As we will see soon, FIM is a `$K$`-by-`$K$` matrix.

To illustrate this, let's consider a unit circle in $\mathcal{R}^2$ as shown in the Figure.
Clearly, a point $ (0,1) $ highlighted in green is in the circle, where we consider its center as the origin.

<img src="/img/circle.png" title="Source:Wikipedia" width="300"/>

>Parametrization 1:
>
>A (local) parametrization at $ (0,1) $ highlighted in red for the circle is
>`$\{ (t,\sqrt{1-t^2}) | -h<t<h \} $`, where $h=0.1$. We use **one** (scalar) parameter in this parametrization. 
>
>The manifold is (locally) "flat" since we can always find a small **1-dimensional** perturbation $E$ in the **1-dimensional** parameter space  `$\Omega_t=\{t|-h<t<h \} $`. 
>
><img src="/img/1d-perturbation.png" title="Fig" width="300"/>
>
>This parametrization is called an **intrinsic** parameterization.
>
>We can similarly define a (local) parametrization at each point of the circle. In fact, we can use  four (local) parameterizations to represent the circle as shown below.
>
><img src="/img/charts.png" title="Source:Wikipedia" width="200"/>

>Parametrization 2:
>
>Let's define a map `$f : [0,2\pi) \rightarrow \mathcal{S}^1 $` such that `$f(\theta) = (\sin \theta, \cos \theta ) $`, where we use $\mathcal{S}^1$ to denote the circle.
>
>A (global) parametrization of the circle is `$\{ f(\theta) | \theta \in [0,2\pi)  \}$`. We use one (scalar) parameter in this parametrization. 
>
>This map $f$ is bijective and smooth. However, the parameter space is not open in $\mathcal{R}$. Moreover, its inverse map $f^{-1}$ is **not** continous at point $(0,1) \in  \mathcal{S}^1$.
>
>This smoothness requirement of the inverse map is essential when it comes to reparametrization (A.K.A. parameter transformation). The requirement gives us a way to generate new intrinsic parameterizations. In other words, the Jacobian matrix is non-singular if the requirement is satisfied. Thus, we can safely use the chain rule and inverse function theorem.
>
>We will **not** consider this parametrization as an intrinsic parameterization.


>Parametrization 3:
>
>The circle does **not** look like a flat space under the following parametrization
>`$\{ (x,y) | x^2+y^2=1, x,y \in \mathcal{R} \} $`. We use **two** (scalar) parameters in this parametrization. 
>
>The reason is that we cannot find a small **2-dimensional** perturbation $E$ in the **2-dimensional** parameter space `$\Omega_{xy}=\{(x,y)|x^2+y^2=1 \} $` due to the constraint $x^2+y^2=1$. In other words, $\Omega_\tau$ is not open in $\mathcal{R}^2$.
>
><img src="/img/2d-perturbation.png" title="Fig" width="300"/>
>
>This parametrization is **not** an intrinsic parameterization.




# Intrinsic Parameterizations for Parametric families
------
Now, we discuss how to choose a parameterization for a parametric family so that we can exploit the geometric structure induced by the Fisher-Rao metric.

Given a parametric distribution family $ p(w|\tau) $ indexed by its parameter $\tau$, $ p(w|\tau) $ should be smooth w.r.t. $ \tau $ by considering $ w $ to be fixed.
We say a parametrization is intrinsic if the following condition for parameter $\tau $ holds:


**Regularity Condition**: The set of partial derivatives 
$ \\{ \partial_{\tau_i} \log p(w\|\tau) \\}  $ should be linearly independent for any $ w $ and any $\tau$.


We will use two examples to illustrate this condition.


>Example 1: 
>
>We will show that the regularity condition holds. Consider a 1-dimensional Gaussian family `$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with parameter $\tau = (\mu,\sigma) $.
>The partial derivatives are
>`$$
\begin{aligned}
 \partial_{\mu} \log \mathcal{N}(w |\mu,\sigma) = \frac{w-\mu}{\sigma}, \,\,\, \partial_{\sigma} \log \mathcal{N}(w |\mu,\sigma) = \frac{ (w-\mu)^2 }{2 \sigma^2} - \frac{1}{2\sigma} 
\end{aligned} \tag{4}\label{4}
$$`
>For simplicity,  let $\mu=0$ and $\sigma=1$. We can simplify the partial derivatives as below.
> 
>`$$
\begin{aligned}
 \partial_{\mu} \log \mathcal{N}(w |\mu,\sigma) \Big|_{\mu=0,\sigma=1}= w, \,\,\, \partial_{\sigma} \log \mathcal{N}(w |\mu,\sigma) \Big|_{\mu=0,\sigma=1} = \frac{ w^2 -1 }{2}  
\end{aligned} \tag{5}\label{5}
$$`
>If $ c_1 w + c_2  (\frac{ w^2 -1 }{2}) = 0$ holds for any $w$, we have $c_1=c_2=0$, which implies  linear independence. 
>
>Similarly, we can show that for any $\mu \in \mathcal{R}$ and $\sigma >0$, 
the partial derivatives are linearly independent.


>Example 2: 
>
>We will show that the regularity condition fails. Consider a Bernoulli family `$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0>0, \pi_1>0  \}$` with parameter $\tau = (\pi_0,\pi_1) $, where function $ \mathcal{I}(\cdot) $ is the indicator function.
>The partial derivatives are
>
>`$$
\begin{aligned}
 \partial_{\pi_0} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{\pi_1}{(\pi_0+\pi_1)^2}   
\end{aligned} \tag{6}\label{6}
$$`
>`$$
\begin{aligned}
\partial_{\pi_1} \log \mathcal{B}(w |\pi_0, \pi_1) = \mathcal{B}(w |\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} 
\end{aligned} \tag{7}\label{7}
$$`
>Note that when $c_0 = \pi_0 \neq 0 $ and $c_1= \pi_1 \neq 0$, we have $c_0 \frac{\pi_1}{(\pi_0+\pi_1)^2}  + c_1 \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} = 0$. 
>
>Therefore, we can show that 
the partial derivatives are linearly dependent.

>Example 3:
>
>We will soon show that the  condition fails for Bernoulli family  `$ \{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \}$` with parameter $\tau = (\pi_0,\pi_1)$. 
>The main reason is that the parameter space is not open in $\mathcal{R}^2$. 

>Example 4:
>
>We can show that the condition holds for Bernoulli family `$\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) (1-\pi_0)  \Big| 0<\pi_0<1  \}$` with parameter $\tau = \pi_0$.



# Fisher-Rao Metric
------
Given an intrinstic parameterization, we can define the Fisher-Rao metric under this parameterization as:
`$ F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau) \Big) ]$`.
Note that the metric could be ill-defined since the expectation may not exist.



Given a parameterization,  we can express the metric in a matrix form as
$ \mathbf{F}(\tau) := E_{p(w\|\tau) }  [ \Big( \nabla_{\tau} \log p(w\|\tau ) \Big)  \Big(\nabla_{\tau} \log (w\|\tau) \Big)^T ]$,
where $K$ is the length of parameter vector $\tau$ and 
$ \nabla_{\tau} \log p(w\|\tau ) := [ \partial_{\tau_1} \log p(w\|\tau ), \cdots, \partial_{\tau_K} \log p(w\|\tau ) ]^T  $ is a column vector.
The matrix form is also known as the **Fisher information matrix**. Obviously, FIM depends on the choice of parameterizations.


The regularity condition guarantees that FIM is non-singular if the matrix exists.
The condition is also related to the model identification in maximum likelihood estimation.

In the following discussion, we will assume the metric is well-defined.
The Fisher-Rao metric is a valid Riemannian metric since the corresponding FIM is positive definite everywhere in an **intrinsic** parameter space.
The Fisher-Rao metric is special since it is closely related to  maximum likelihood estimation, central limit theorem, and principle of maximum entropy.

<span style="color:red">**Warning**</span>: an arbitrary Riemannian metric often is NOT useful for applications in machine learning. 


Recall that when we perform a parameter transformation, we require both the transformation map and its inverse map to be smooth.
If this smoothness requirement is satisfied, we can obtain a new intrinsic parameterization via the transformation.
In Part II,
we will show FIM is also positive definite under this new intrinsic parameterization.


In many cases, we could also compute FIM as
$ \mathbf{F}(\tau) := - E_{p(w\|\tau) }  [  \nabla_{\tau}^2 \log p(w\|\tau )  ]$.



Caveats of the Fisher matrix computation
------
There are some caveats when it comes to the Fisher matrix computation. In particular, the regularity condition should be satisfied.
It is possible to define FIM under a non-intrinstic parameterization. However, FIM often is singular or ill-defined under a non-intrinstic  parameterization as shown below.

>Example 1:
>
>Consider Bernoulli family  $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big\| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \\}$ with parameter $\tau = (\pi_0,\pi_1)$.
The following computation is not correct. Do you make similar mistakes like this?
>
>Let $  p(w\|\tau ) =  \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1$, where $\tau = (\pi_0,\pi_1)$. The derivative is
> `$$  
\begin{aligned}
\nabla_{\tau} \log p(w|\tau ) = \frac{1}{p(w|\tau )} [\mathcal{I}(w=0) ,  \mathcal{I}(w=1)]^T 
\end{aligned} \tag{8}\label{8}
$$`
>Thus, by Eq. `$\eqref{8}$`, FIM under this  parameterization is
>`$$ 
\begin{aligned}
F(\tau) = E_{p(w|\tau) } [ \frac{1}{p^2(w|\tau)} \begin{bmatrix} \mathcal{I}^2(w=0) &  \mathcal{I}(w=1)  \mathcal{I}(w=0) \\ 
\mathcal{I}(w=0)  \mathcal{I}(w=1) &  \mathcal{I}^2(w=1) \end{bmatrix} ]   = \begin{bmatrix} \frac{1}{\pi_0} &  0 \\ 0 &  \frac{1}{\pi_1} \end{bmatrix}
\end{aligned} \tag{9}\label{9}
$$`
>This computation is not correct. Do you know why it is not correct?
>
>The key reason is the equality constraint $ \pi_0+\pi_1=1 $. Thus, Eq. `$\eqref{8}$` is **incorrect**.
>
>By definition, a Bernoulli distribution is valid only when the constraint holds. 
Thus, the constraint $ \pi_0+\pi_1=1 $ must be satisifed when we compute the Fisher matrix since the computation involves the expectation of this distribution.
>
> Note that the gradient is defined as $ \nabla_{\tau} \log p(w\|\tau ) := [ \partial_{\pi_0} \log p(w\|\tau ), \partial_{\pi_1} \log p(w\|\tau ) ]^T $.
>
>Unfortunately, these partial derivatives do not exist. By the definition of  partial derivative $ \partial_{\pi_0} \log p(w\|\tau )$, we fix $\pi_1$ and allow $\pi_0$ to change.
However, given that $\pi_1$ is fixed and $ \pi_0 $ is fully determined by $\pi_1$ due to the equality constraint $ \pi_0+\pi_1=1 $. 
>Therefore, $  \partial_{\pi_0} \log p(w\|\tau ) $ is not well-defined.
>In other words, the above Fisher matrix computation is not correct since $ \nabla_{\tau} \log p(w\|\tau ) $ does not exist. 
 
>Example 2:
>
>Consider Bernoulli family `$ \{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big| \pi_0>0, \pi_1>0  \}$` with parameter $\tau = (\pi_0,\pi_1) $.
>
>We can show that FIM under this  parameterization is singular. 


>Example 3:
>
>Consider Bernoulli family `$\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) (1-\pi_0)  \Big| 0<\pi_0<1  \}$` with parameter `$\tau = \pi_0$`.
>
>We can show that FIM under this  parameterization is non-singular. 


# Manifold Dimension
------
We can define the dimension of a manifold by using the dimension of an intrinsic parametrization. Formally, we can show that any intrinsic parametrization of a manifold has the same degrees of freedom. 
We now illustrate this by examples.

>Example 1:
>
>The unit circle dicussed before is a 1-dimensional manifold.
>
><img src="/img/circle-org.png" alt="Source:Wikipedia" width="200"/>

>Example 2:
>
>The open unit ball is a 2-dimensional manifold.
>
><img src="/img/open-ball.png" alt="Source:Wikipedia" width="200"/>

>Example 3:
>
>The closed unit ball is **not** a manifold. 
>
><img src="/img/closed-ball.png" alt="Source:Wikipedia" width="200"/>
>
>The main reason is that its boundary  does not have the same degrees of freedom as its interior.
>
>The closed ball is indeed a manifold with (closed) boundary. We will not consider this case in these blog posts.


>Example 4:
>
>Consider a 1-dimensional  Gaussian family.
>We specify an intrinsic parameterization $\mathbf{\tau}$  as `$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with `$\tau = (\mu,\sigma) $`. <br /> 
>
>This Gaussian family is a  2-dimensional manifold.


>Example 5:
>
>Consider a $d$-dimensional Gaussian family with zero mean.
>We specify an intrinsic parameterization $\mathbf{\tau}$  as `$ \{ \mathcal{N}(\mathbf{w} |\mathbf{0},\mathbf{\Sigma}) \Big| \mathrm{MatH}(\tau) = \mathbf{\Sigma}   \succ \mathbf{0} \}$` with `$\tau = \mathrm{vech}(\mathbf{\Sigma})$`, where `$\tau$` is a $\frac{d(d+1)}{2}$-dim vector and map $\mathrm{MatH}()$ is the inverse map of the half-vectorization function `$\mathrm{vech}()$`. 
> The half-vectorization, `$\mathrm{vech}(\mathbf{\Sigma})$`, of a symmetric `$d \times d$` matrix `$\mathbf{\Sigma}$` is the `$\frac{d(d + 1)}{2}$`-dim  vector obtained by vectorizing only the lower triangular part of `$\mathbf{\Sigma}$`.
>
>This Gaussian family is a $\frac{d(d+1)}{2}$-dimensional manifold. We will disucss more about this family in Part II.





