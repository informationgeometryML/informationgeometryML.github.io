---
title: 'Information Geometry in Machine Learning (2)'
date: 2021-09-06
permalink: /posts/2021/10/Geomopt02/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Goal
------
This blog post should help readers to understand natural-gradients also known as Riemannian gradients with the Fisher-Rao metric.


Euclidean steepest direction and directional derivative
------
Given a smooth scalar funcion $\min_{\tau \in \mathcal{R}^n } f(\mathbf{\tau})$, in a flat (Euclidean) space we can define its (Euclidean) steepest direction at current `$\mathbf{\tau}_0$` as the following optimization problem in terms of a directional derivative w.r.t. vector $\mathbf{v}$, where we assume `$\nabla_\tau f(\mathbf{\tau}_0)  \neq \mathbf{0}$`.
`$$
\begin{aligned}
\min_{\|v\|^2=1} \lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t} = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0) 
\end{aligned}\tag{1}\label{1}
$$` 

It is easy to see that the optimal solution of Eq. `$\eqref{1}$` is `$\mathbf{v}_{\text{opt}}= -\frac{\nabla_\tau f(\mathbf{\tau}_0) }{\|\nabla_\tau f(\mathbf{\tau}_0) \|}$`, which is the (Euclidean) steepest direction.

To generalize  the steepest direction at a given point (e.g., `$\mathbf{\tau}_0$`) in a Riemannian manifold, we first have to define the length of a vector in the manifold.


Distance induced by the Fisher-Rao metric 
------
As mentioned at [the previous post]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#fisher-rao-metric), the Fisher information matrix $\mathbf{F}$ should be positive definite. We can use it to define the length/norm of a vector (e.g., a Riemannian gradient) $\mathbf{v}$ at a point in a manifold. 
`$$
\begin{aligned}
\|\mathbf{v}\|_F := \sqrt{\mathbf{v}^T \mathbf{F} \mathbf{v}}
\end{aligned}
$$`

The positive-definiteness of the Fisher matrix is essential since we do not want a non-zero vector has a zero length.

The orthogonality and distance between two <span style="color:red">vectors at the same point</span> are also induced by the Fisher matrix since we can define them by using the length.
The subtle difference in the manifold case is that we use the distance (and the orthogonality) induced by the manifold metric instead of using the Euclidean counterpart. 


**Warning**:  We do NOT define the distance between two points in the manifold, which is shown in [this figure.](#riemannian-gradients-as-tangent-vectors)
We also do NOT define the distance between a vector at one point and another vector at another distinct point.


Directional derivatives in a manifold
------
The next step is to generalize directional derivatives  in a manifold. 

Note that a manifold is locally like a flat (Euclidean) space under [**intrinsic** parameterization]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations) `$\mathbf{\tau}$`.
Using this parameterization, consider an optimization problem $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$, where the parameter space $\Omega_\tau$ is determined by the parameterization and the manifold. For simplicity, we assume $\mathbf{\tau}$ is a **global** intrinsic parameterization, which is often possible for [parametric families]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families). 

>For example, consider a 1-dimensional  Gaussian family as a manifold. <br />
>We specify an intrinsic parameterization $\mathbf{\tau}$ of the manifold as `$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with `$\tau = (\mu,\sigma) $`. <br />
>The parameter space  is $\Omega_\tau = \mathcal{R} \times \mathcal{R}_{++}$, where `$\mathcal{R}_{++}$` denotes the set of positive real numbers. This parameterization is global since we can use **one** coordinate system to cover the whole manifold.
>
>A possible optimization problem in machie learning is `$f(\mathbf{\tau}) =  E_{  p(w |\tau)  } [ \ell(w)  ]$` (see [ML examples]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#motivation)) 


Therefore, we can similarly define a directional derivative at `$\mathbf{\tau}_0$` w.r.t. Riemannian vector $\mathbf{v}$ as `$\lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t}$`. 


Recall that we allow a [small perturbation]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations) $E$ in the parameter space of $\mathbf{\tau}$ since  $\mathbf{\tau}$ is an intrinsic parameterization.
Therefore, when $t$ is small enough, `$\mathbf{\tau}_0+t\mathbf{v} $` stays in the manifold and `$f(\mathbf{\tau}_0+t\mathbf{v})$` is well-defined since  `$\mathbf{\tau}_0+t\mathbf{v} \in \Omega_\tau$`.


Under an **intrinsic** parameterization $\mathbf{\tau}$, the directional derivative remains the same as in the Euclidean case.
`$\lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t} = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0)$`

As shown in the figure, consider the unit sphere as a manifold, where `$\tau$` is an (local) **intrinsic** parameterization. 
The line segment from `$\mathbf{\tau}_0$` to `$\mathbf{\tau}_0+t\mathbf{v} $`  is shown in blue, which is the parameter representation of the yellow curve `$\gamma(t)$` in the manifold.
We will show later that Riemannian vector `$\mathbf{v}$` at `$\mathbf{\tau}_0$` is the **parameter representation** of the tangent vector of curve `$\gamma(t)$` at point `$\mathbf{x}_0$`.

<img src="/img/sphere_simple.png"  width="500"/>


**Warning**: Curve `$\gamma(t)$` is not the shortest curve in the manifold between `$\mathbf{x}_0$` and `$\mathbf{x}_1$`. The shortest curve is not a straight line in curved manifold cases.




The following figure illustrates that a directional derivative can be ill-defined under a **non-intrinsic** parameterization (see parameterization 3 at the [previous post]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations)) for  unit circle `$\mathcal{S}^1$` , where the red line segment passes through `$\tau_0=(0,1) \in \mathcal{S}^1 $`.

![Figure 1](/img/tangent_non.png) 

Any other point `$\tau_0 + t\mathbf{v}$` in the line segment leaves the manifold for `$t>0$` and thus, `$f(\mathbf{\tau}_0+t\mathbf{v})$` is not well-defined.
The main reason is that `$\tau$` is not an intrinsic parameterization.


Riemannian steepest direction
------
Now, we will define the  Riemannian steepest direction.

Given  a smooth scalar funcion defined in a manifold $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$ under an intrinsic parameterization $\mathbf{\tau}$. We can define the Riemannian steepest direction as the following optimization problem in terms of a directional derivative w.r.t. Riemannian vector $\mathbf{v}$, where we assume `$\nabla_\tau f(\mathbf{\tau}_0)  \neq \mathbf{0}$`.
`$$
\begin{aligned}
\min_{ \color{red} {\|v\|_{F}^2=1} }  \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0) 
\end{aligned} \tag{2}\label{2}
$$` 
where all possible $\mathbf{v}$ are vectors at current point `$\mathbf{\tau}_0$`.

We can write down the Lagrangian function, where $\lambda$ is a Lagrange multiplier. 
`$$
\begin{aligned}
L(\mathbf{v},\lambda) =  \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0) + \lambda (\|v\|_{F}^2 - 1) = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0) + \lambda (\mathbf{v}^T \mathbf{F}(\mathbf{\tau}_0) \mathbf{v}  - 1) 
\end{aligned}
$$` where `$\mathbf{F}(\mathbf{\tau}_0)$` is the Fisher information matrix.

One of the KKT necessary conditions implies that
`$$
\begin{aligned}
\mathbf{0} = \nabla_{v} L(\mathbf{v}_{\text{opt}},\lambda) = \nabla_\tau f(\mathbf{\tau}_0) + 2 \lambda \mathbf{F}(\mathbf{\tau}_0) \mathbf{v}_{\text{opt}}
\end{aligned}
$$`
When $\lambda \neq 0$, vector 	`$\mathbf{v}_{\text{opt}}$` should be proportional to `$\mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0)$`. The Fisher matrix `$\mathbf{F}(\mathbf{\tau}_0)$` is invertible since it is positive definite.
 

We can show that the optimal solution of Eq. `$\eqref{2}$` is `$\mathbf{v}_{\text{opt}}= -\frac{ \mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0) }{\| \mathbf{F}^{-1}(\mathbf{\tau}_0)\nabla_\tau f(\mathbf{\tau}_0) \|_F}$`, which gives us the Riemannian steepest direction at current `$\mathbf{\tau}_0$`. 

The **Euclidean** steepest direction `$\mathbf{v}_{\text{euclid}}= -\frac{ \nabla_\tau f(\mathbf{\tau}_0) }{\| \nabla_\tau f(\mathbf{\tau}_0) \|_F}$` is **not** the optimal solution of  Eq. `$\eqref{2}$` when `$\mathbf{F}(\tau_0) \neq \mathbf{I}$`.
We will illustrate this by using an example.

>Example
>
>Consider `$\mathbf{F}(\tau_0)=\begin{bmatrix} 1 & 0 \\ 0 & \frac{1}{2} \end{bmatrix}$` and `$\nabla_\tau f(\mathbf{\tau}_0)=\begin{bmatrix} 1\\1 \end{bmatrix}$`.
>We have the following results
> `$$
\begin{aligned}
\| F^{-1} \nabla_\tau f(\mathbf{\tau}_0) \|_F^2  =  \nabla_\tau^T f(\mathbf{\tau}_0) \mathbf{F}^{-1}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) = 3; \,\,\,
\| \nabla_\tau f(\mathbf{\tau}_0) \|_F^2  =  \nabla_\tau^T f(\mathbf{\tau}_0) \mathbf{F}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) = \frac{3}{2}
\end{aligned}
$$`
> `$$
\begin{aligned}
\mathbf{v}_{\text{opt}} = -\begin{bmatrix} \frac{1}{\sqrt{3}} \\ \frac{2}{\sqrt{3}} \end{bmatrix}; \,\,\,
\mathbf{v}_{\text{euclid}}=
-\begin{bmatrix} \sqrt{\frac{2}{3}} \\ \sqrt{\frac{2}{3}} \end{bmatrix}\end{aligned}
$$`
>`$$
\begin{aligned}
 \mathbf{v}_{\text{opt}}^T \nabla_\tau f(\mathbf{\tau}_0)= -\sqrt{3}  <  -\frac{2\sqrt{2}}{\sqrt{3}} = \mathbf{v}_{\text{euclid}}^T \nabla_\tau f(\mathbf{\tau}_0) 
\end{aligned}
$$`
>
>Therefore, the Euclidean steepest direction `$\mathbf{v}_{\text{euclid}}$` is not the optimal solution of  Eq. `$\eqref{2}$`.


For manifold optimization, given a scalar function $f(\mathbf{\tau})$, we denote its **Euclidean** gradient by $\nabla_\tau f(\mathbf{\tau})$ and the **Riemannian** gradient by $ \mathbf{F}^{-1}(\mathbf{\tau}) \nabla_\tau f(\mathbf{\tau})$. Since we use the Fisher-Rao metric, the Riemannian gradient is also known as the **natural** gradient. We use a learning-rate to control the length of a gradient instead of normalizing its length. 




# Riemannian gradients as tangent vectors
------
A Riemannian gradient denoted by $\mathbf{v}$ is indeed a tangent vector in the manifold under the parametrization $\tau$. 
The set of tangent vectors is called the tangent space at the point $\mathbf{\tau}=\mathbf{\tau}_0$. 


Let's denote the unit sphere by $\mathcal{M}$, where we set the origin to be the center of the sphere. Point $\mathbf{x_0}=(0,0,1)$ is the north pole.
We use the following parameterization, where the manifold can be locally expressed as `$\{(\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2})|  \tau_x^2 + \tau_y^2 <1 \}$` with parameter $\mathbf{\tau}=(\tau_x,\tau_y)$. 
Under the parametrization $\mathbf{\tau}$, we have the following parametric representations.  

|   &nbsp; &nbsp; &nbsp;    | Parametric representation     | 
|:------------|:-------------:|
| North pole  $\mathbf{x_0}$   | $\mathbf{\tau}_0=(0,0)$  |  
| Intrinsic parameter space     |  red space `$\Omega_\tau:=\{ (\tau_x,\tau_y)| \tau_x^2 + \tau_y^2 <1 \}$`   |
| Tangent space at $\mathbf{x_0}$     |  green space  `$\mathcal{R}^2$` at `$\mathbf{\tau}_0$`   |
| Yellow curve from $\mathbf{x_0}$ to $\mathbf{x_1}$    |  blue line segment from `$\mathbf{\tau}_0$` to `$\mathbf{\tau}_0+h\mathbf{v}(\tau_0)$`   |  



<img src="/img/sphere.png"  width="500"/>

**Warning**: Be aware of the differences shown in the table.

|   &nbsp; &nbsp; &nbsp;    |   parametric representation of   |       properties   |      distance  discussed in this post  |
|:------------|:-------------:|:-------------:|
|  `$\mathcal{R}^2$` space |   tangent vector space at `$\mathbf{x}_0$`  | real scalar product, vector addition  | defined |
|   `$\Omega_\tau$` space | top half of the manifold  |   **local/bounded**  scalar product and vector addition   |  undefined |

Under **intrinsic** parametrization $\tau$, we have `$\Omega_\tau \subset \mathcal{R}^2$`. Thus, we can perform the operation in $\Omega_\tau$ space: `$\tau_0 +t\mathbf{v}(\tau_0)$` when step-size $t$ is small enough. However, we only define the distance between two vectors in the `$\mathcal{R}^2$` space. The distance between two points in the `$\Omega_\tau$` space is undefined.



The tangent vector $\mathbf{v}$ at point $\mathbf{x_0}$  can be viewed the **tangent direction** of a (1-dimensional) smooth curve $\gamma(t) \in \mathcal{M}$, where $\gamma(0)=\mathbf{x_0}$. 
Given parametrization $\tau$, we can define the parametric representation of the curve denoted by $\bar{\gamma}(t)$. 

>Example
>
>Consider the yellow curve $\gamma(t) = (t v_{x}, t v_{y}, \sqrt{1 - t^2(v_{x}^2 + v_{y}^2) } ) \in \mathcal{M} $ 
>and the blue line segment $\bar{\gamma}(t)= (t v_{x} , t v_y  ) \in \Omega_\tau $, where $t$ must be small enough. 
>
>The parametric  representation of the vector is defined as `$\mathbf{v}(\mathbf{\tau}_0):= \frac{d \bar{\gamma}(t) }{d t} \Big|_{t=0} =(v_x,v_y)$`, where `$\bar{\gamma}(0)=\tau_0$`. 

We can also dervie  the parametric  representation of the tangent vector from directional derivatives.

>
>Consider a smooth scalar function defined in the manifold $h: \mathcal{M} \to \mathcal{R}$. In this example, consider `$h(\mathbf{x})$` subject to `$\mathbf{x}^T \mathbf{x}=1$`.
>Under parameterization $\mathbf{\tau}$, we can locally re-expressed the function as `$f(\mathbf{\tau}):=h( (\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2}) )$` where `$\tau \in \Omega_\tau$`.
>
>The parametric  representation of the vector should hold for the following identity for any smooth scalar function $h$: `$[\nabla_\tau^T f(\mathbf{\tau}_0)] \mathbf{v}(\mathbf{\tau}_0) =\frac{d f(\bar{\gamma}(t)) }{d t} \Big|_{t=0}$`, where $f$ is the parametric representation of  $h$ . 
>By the chain rule, we have `$\frac{d f(\bar{\gamma}(t)) }{d t} \Big|_{t=0}=[\nabla_\tau^T f(\mathbf{\tau}_0)]  \frac{d \bar{\gamma}(t) }{d t} \Big|_{t=0}$`, where `$\bar{\gamma}(0)=\tau_0$`. Thus,
> `$\mathbf{v}(\mathbf{\tau}_0) =  \frac{d \bar{\gamma}(t) }{d t} \Big|_{t=0}$`.
>
>From Eq. `$\eqref{2}$` , we can see that a Riemannian gradient `$\mathbf{v}(\mathbf{\tau}_0)$` can be viewed as a parametric representation of the tangent vector of curve $\gamma(t)$ at $\mathbf{x}_0$ since  $\bar{\gamma}(t)$ is the parametric representation of $\gamma(t)$.
 

Euclidean gradients as Cotangent vectors
------
(W.I.P.)


Parametric Invariance
------
(W.I.P.)

Notice that a curve in the manifold is a geometric object, which is invariance to the choice of parametrization.
It may be easy to understand the parametric invariance via a parameter transformation.

(new type of auto-diff) Contravariant transformation for vector components (Riemannian gradients)


(auto-diff) Covariant transformation for covector components  (Euclidean gradients)


Note that gradient invariance does not imply gradient update is invaiant




