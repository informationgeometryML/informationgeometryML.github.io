---
title: 'Information Geometry in Machine Learning (2)'
date: 2021-10-04
permalink: /posts/2021/10/Geomopt02/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Goal
------
This blog post should help readers to understand natural-gradients also known as Riemannian gradients with the Fisher-Rao metric.


Euclidean steepest direction and directional derivative
------
Given a smooth scalar funcion $\min_{\tau \in \mathcal{R}^n } f(\mathbf{\tau})$, in a flat (Euclidean) space we can define its (Euclidean) steepest direction at current `$\mathbf{\tau}_0$` as the following optimization problem in terms of a directional derivative w.r.t. vector $\mathbf{v}$, where we assume `$\nabla_\tau f(\mathbf{\tau}_0)  \neq \mathbf{0}$`.
`$$
\begin{aligned}
\min_{\|v\|^2=1} \lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t} = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0) 
\end{aligned}\tag{1}\label{1}
$$` 

It is easy to see that the optimal solution of Eq. `$\eqref{1}$` is `$\mathbf{v}_{\text{opt}}= -\frac{\nabla_\tau f(\mathbf{\tau}_0) }{\|\nabla_\tau f(\mathbf{\tau}_0) \|}$`, which is the (Euclidean) steepest direction.

To generalize  the steepest direction at a given point (e.g., `$\mathbf{\tau}_0$`) in a Riemannian manifold, we first have to define the length of a vector in the manifold.


Distance induced by the Fisher-Rao metric 
------
As mentioned at [the previous post]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#fisher-rao-metric), the Fisher information matrix $\mathbf{F}$ should be positive definite. We can use it to define the length/norm of a vector (e.g., a Riemannian gradient) $\mathbf{v}$ at a point in a manifold. 
`$$
\begin{aligned}
\|\mathbf{v}\|_F := \sqrt{\mathbf{v}^T \mathbf{F} \mathbf{v}}
\end{aligned}
$$`

The positive-definiteness of the Fisher matrix is essential since we do not want a non-zero vector has a zero length.

The distance (and orthogonality) between two <span style="color:red">vectors at the same point</span> is also induced by the Fisher matrix since we can define them by using the length as
`$$
\begin{aligned}
d(\mathbf{v},\mathbf{w}) := \|\mathbf{v}-\mathbf{w}\|_F
\end{aligned}
$$`
where we can consider `$\mathbf{v}$` and `$\mathbf{w}$` are two vectors evaluted at point `$\tau_0$`.


**Warning**:  We do NOT define the distance between two points in the manifold, which will be discussed [here](#riemannian-gradients-as-tangent-vectors).
We also do NOT define the distance between a vector at one point and another vector at a distinct point.

# Directional derivatives in a manifold
------
The next step is to generalize directional derivatives  in a manifold. 

Note that a manifold is locally like a flat (Euclidean) space under [**intrinsic** parameterization]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations) `$\mathbf{\tau}$`.
Using this parameterization, consider an optimization problem $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$, where the parameter space $\Omega_\tau$ is determined by the parameterization and the manifold. 

Therefore, we can similarly define a directional derivative at `$\mathbf{\tau}_0$` w.r.t. Riemannian vector $\mathbf{v}$ as `$\lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t}$`. 


Recall that we allow a [small perturbation]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations) $E$ in the parameter space of $\mathbf{\tau}$ since  $\mathbf{\tau}$ is an intrinsic parameterization.
Therefore, when $t$ is small enough, `$\mathbf{\tau}_0+t\mathbf{v} $` stays in the manifold and `$f(\mathbf{\tau}_0+t\mathbf{v})$` is well-defined since  `$\mathbf{\tau}_0+t\mathbf{v} \in \Omega_\tau$`.


Under an **intrinsic** parameterization $\mathbf{\tau}$, the directional derivative remains the same as in the Euclidean case.
`$\lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t} = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0)$`

>Example 1:
>
>`$\tau$` is a **local intrinsic** parameterization for the unit sphere.
>  
>The line segment from `$\mathbf{\tau}_0$` to `$\mathbf{\tau}_0+t\mathbf{v} $`  is shown in blue, which is the parameter representation of the yellow curve `$\gamma(t)$` in the manifold.
>We will show later that Riemannian vector `$\mathbf{v}$` at `$\mathbf{\tau}_0$` is the **parameter representation** of the tangent vector of curve `$\gamma(t)$` at point `$\mathbf{x}_0$`.
>
><img src="/img/sphere_simple.png"  width="500"/>
>
>**Warning**: Curve `$\gamma(t)$` is not the shortest curve in the manifold between `$\mathbf{x}_0$` and `$\mathbf{x}_1$`. The shortest curve is not a straight line in curved manifold cases.



>Example 2:
>
>A directional derivative can be ill-defined under a **non-intrinsic** parameterization.
>
>We use [parameterization 3]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations) for unit circle `$\mathcal{S}^1$`, where the red line segment passes through `$\tau_0=(0,1) \in \mathcal{S}^1 $`.
>
>![Figure 1](/img/tangent_non.png) 
>
>Any other point `$\tau_0 + t\mathbf{v}$` in the line segment leaves the manifold for `$t>0$` and thus, `$f(\mathbf{\tau}_0+t\mathbf{v})$` is not well-defined.
>The main reason is that `$\tau$` is not an intrinsic parameterization.

>Example 3:
>
>For [parametric families]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families), we often can find a **global** intrinsic parameterization $\mathbf{\tau}$.
>
>Consider a 1-dimensional  Gaussian family as a manifold. <br />
>We specify an intrinsic parameterization $\mathbf{\tau}$ of the manifold as `$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with `$\tau = (\mu,\sigma) $`. <br />
>The parameter space  is $\Omega_\tau = \mathcal{R} \times \mathcal{R}_{++}$, where `$\mathcal{R}_{++}$` denotes the set of positive real numbers. This parameterization is global since we can use **one** coordinate system to cover the whole manifold.
>
>A possible optimization problem in machie learning is `$f(\mathbf{\tau}) =  E_{  p(w |\tau)  } [ \ell(w)  ]$` (see [ML examples]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#motivation)) 


>Example 4:
>
>Consider a $d$-dimensional Gaussian family with zero mean as a manifold. <br />
>We specify an intrinsic parameterization $\mathbf{\tau}$ of the manifold as `$ \{ \mathcal{N}(\mathbf{w} |\mathbf{0},\mathbf{\Sigma}) \Big| \mathrm{MatH}(\tau) = \mathbf{\Sigma}   \succ \mathbf{0} \}$` with `$\tau = \mathrm{vech}(\mathbf{\Sigma})$`, where `$\tau$` is a $\frac{d(d+1)}{2}$-dim vector and map $\mathrm{MatH}()$ is the inverse map of the lower-triangular half $\mathrm{vech}()$. 
>
>Technically speaking, $\mathbf{\Sigma}$ is NOT an intrinsic parameter due to the symmetry constraint. In other words, the Fisher information matrix w.r.t. $\mathbf{\Sigma}$ will be singular if  `$\mathbf{\Sigma}$` is considered as a matrix parameter with $d^2$ degrees of freedom.
>
>In literature, a natural gradient for $\mathbf{\Sigma}$ is defined by $\mathrm{MatH}(\mathbf{v})$, where $\mathbf{v}$ is a natural-gradient w.r.t. `$\mathrm{vech}(\Sigma)$`. 



Riemannian steepest direction
------
Now, we will define the  Riemannian steepest direction.

Given  a smooth scalar funcion defined in a manifold $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$ under an intrinsic parameterization $\mathbf{\tau}$. We can define the Riemannian steepest direction as the following optimization problem in terms of a directional derivative w.r.t. Riemannian vector $\mathbf{v}$, where we assume `$\nabla_\tau f(\mathbf{\tau}_0)  \neq \mathbf{0}$`.
`$$
\begin{aligned}
\min_{ \color{red} {\|v\|_{F}^2=1} }  \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0) 
\end{aligned} \tag{2}\label{2}
$$` 
where all possible $\mathbf{v}$ are vectors at current point `$\mathbf{\tau}_0$`.

The Lagrangian function is given below, where $\lambda$ is a Lagrange multiplier. 
`$$
\begin{aligned}
L(\mathbf{v},\lambda) =  \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0) + \lambda (\|v\|_{F}^2 - 1) = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0) + \lambda (\mathbf{v}^T \mathbf{F}(\mathbf{\tau}_0) \mathbf{v}  - 1) 
\end{aligned}
$$` where `$\mathbf{F}(\mathbf{\tau}_0)$` is the Fisher information matrix.

One of the KKT necessary conditions implies that
`$$
\begin{aligned}
\mathbf{0} = \nabla_{v} L(\mathbf{v}_{\text{opt}},\lambda) = \nabla_\tau f(\mathbf{\tau}_0) + 2 \lambda \mathbf{F}(\mathbf{\tau}_0) \mathbf{v}_{\text{opt}}
\end{aligned}
$$`
When $\lambda \neq 0$, vector 	`$\mathbf{v}_{\text{opt}}$` should be proportional to `$\mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0)$`, where  `$\mathbf{F}^{-1}(\mathbf{\tau}_0)$` is well-defined since the Fisher matrix `$\mathbf{F}(\mathbf{\tau}_0)$` is positive definite.
 

We can show that the optimal solution of Eq. `$\eqref{2}$` is `$\mathbf{v}_{\text{opt}}= -\frac{ \mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0) }{\| \mathbf{F}^{-1}(\mathbf{\tau}_0)\nabla_\tau f(\mathbf{\tau}_0) \|_F}$`, which gives us the Riemannian steepest direction at current `$\mathbf{\tau}_0$`. 

The **Euclidean** steepest direction `$\mathbf{v}_{\text{euclid}}= -\frac{ \nabla_\tau f(\mathbf{\tau}_0) }{\| \nabla_\tau f(\mathbf{\tau}_0) \|_F}$` is **not** the optimal solution of  Eq. `$\eqref{2}$` when `$\mathbf{F}(\tau_0) \neq \mathbf{I}$`.
We will illustrate this by using an example.

>Example
>
>Consider `$\mathbf{F}(\tau_0)=\begin{bmatrix} 1 & 0 \\ 0 & \frac{1}{2} \end{bmatrix}$` and `$\nabla_\tau f(\mathbf{\tau}_0)=\begin{bmatrix} 1\\1 \end{bmatrix}$`.
>We have the following results
> `$$
\begin{aligned}
\| F^{-1} \nabla_\tau f(\mathbf{\tau}_0) \|_F^2  =  \nabla_\tau^T f(\mathbf{\tau}_0) \mathbf{F}^{-1}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) = 3; \,\,\,
\| \nabla_\tau f(\mathbf{\tau}_0) \|_F^2  =  \nabla_\tau^T f(\mathbf{\tau}_0) \mathbf{F}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) = \frac{3}{2}
\end{aligned}
$$`
> `$$
\begin{aligned}
\mathbf{v}_{\text{opt}} = -\begin{bmatrix} \frac{1}{\sqrt{3}} \\ \frac{2}{\sqrt{3}} \end{bmatrix}; \,\,\,
\mathbf{v}_{\text{euclid}}=
-\begin{bmatrix} \sqrt{\frac{2}{3}} \\ \sqrt{\frac{2}{3}} \end{bmatrix}\end{aligned}
$$`
>`$$
\begin{aligned}
 \mathbf{v}_{\text{opt}}^T \nabla_\tau f(\mathbf{\tau}_0)= -\sqrt{3}  <  -\frac{2\sqrt{2}}{\sqrt{3}} = \mathbf{v}_{\text{euclid}}^T \nabla_\tau f(\mathbf{\tau}_0) 
\end{aligned}
$$`
>
>Therefore, the Euclidean steepest direction `$\mathbf{v}_{\text{euclid}}$` is not the optimal solution of  Eq. `$\eqref{2}$`.


Given a scalar function $f(\mathbf{\tau})$, if its **Euclidean** (steepest) gradient is $\nabla_\tau f(\mathbf{\tau})$, its **Riemannian** (steepest) gradient is defined as $ \mathbf{F}^{-1}(\mathbf{\tau}) \nabla_\tau f(\mathbf{\tau})$ in literature.
We use a learning-rate to control the length of a gradient instead of normalizing its length. 
Since we use the Fisher-Rao metric, the Riemannian gradient is also known as the **natural** gradient.







# Riemannian gradients as tangent vectors
------
A Riemannian gradient denoted by $\mathbf{v}$ is indeed a tangent vector in the manifold under the parametrization $\tau$. 
The set of tangent vectors is called the tangent space at the point $\mathbf{\tau}=\mathbf{\tau}_0$. 


Let's denote the unit sphere by $\mathcal{M}$, where we set the origin to be the center of the sphere. Point $\mathbf{x_0}=(0,0,1)$ is the north pole.
We use the following parameterization, where the top half of the sphere can be locally expressed as `$\{(\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2})|  \tau_x^2 + \tau_y^2 <1 \}$` with parameter $\mathbf{\tau}=(\tau_x,\tau_y)$. 
Under parametrization $\mathbf{\tau}$, we have the following parametric representations.  

|   &nbsp; &nbsp; &nbsp;    | Parametric representation     | 
|:------------|:-------------:|
| North pole  $\mathbf{x_0}$   | $\mathbf{\tau}_0=(0,0)$  |  
| Intrinsic parameter space     |  red space `$\Omega_\tau:=\{ (\tau_x,\tau_y)| \tau_x^2 + \tau_y^2 <1 \}$`   |
| Tangent space at $\mathbf{x_0}$     |  green space  `$\mathcal{R}^2$` at `$\mathbf{\tau}_0$`   |
| Yellow curve from $\mathbf{x_0}$ to $\mathbf{x_1}$    |  blue line segment from `$\mathbf{\tau}_0$` to `$\mathbf{\tau}_0+t\mathbf{v}(\tau_0)$`   |  



<img src="/img/sphere.png"  width="500"/>

**Warning**: Be aware of the differences shown in the table.

|   &nbsp; &nbsp; &nbsp;    |   parametric representation of   |     supported operations   |      distance  discussed in this post  |
|:------------|:-------------:|:-------------:|
|  `$\mathcal{R}^2$` space |   tangent vector space at `$\mathbf{x}_0$`  | real scalar product, vector addition  | defined |
|   `$\Omega_\tau$` space | top half of the manifold  |   **local**  scalar product, **local** vector addition   |  undefined |

Under **intrinsic** parametrization $\tau$, we have `$\Omega_\tau \subset \mathcal{R}^2$`. Thus, we can perform the operation in $\Omega_\tau$ space: `$\tau_0 +t\mathbf{v}(\tau_0) \in \Omega_\tau$` when scalar `$|t|$` is small enough. Note that we only define the distance between two vectors in the `$\mathcal{R}^2$` space. The distance between two points in the `$\Omega_\tau$` space is undefined.



The tangent vector $\mathbf{v}$ at point $\mathbf{x_0}$  can be viewed as the **tangent direction** of a (1-dimensional) smooth curve $\gamma(t) \in \mathcal{M}$, where $\gamma(0)=\mathbf{x_0}$ and the support of $t$ is an open interval containing 0. 
Given parametrization $\tau$, we can define the parametric representation of the curve denoted by ${\gamma}_\tau(t)$. 
The parametric representation of the vector is defined as `$\mathbf{v}(\mathbf{\tau}_0):= \frac{d {\gamma}_{\tau}(t) }{d t} \Big|_{t=0}$`, where `${\gamma}_{\tau}(0)=\tau_0$`. 
We can also define vector additions and real scalar products by using tangent directions of curves in the manifold. 

>Example
>
>Consider the yellow curve $\gamma(t) = (t v_{x}, t v_{y}, \sqrt{1 - t^2(v_{x}^2 + v_{y}^2) } ) \in \mathcal{M} $ 
>and the blue line segment `${\gamma}_{\tau}(t)= (t v_{x} , t v_y  ) \in \Omega_\tau $`, where `$|t|$` must be small enough. 
>
>The parametric  representation of the vector is `$\mathbf{v}(\mathbf{\tau}_0):= \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}=(v_x,v_y)$`.

The parametric  representation of the tangent vector can also be derived from directional derivatives as shown below.

>
>Consider a smooth scalar function defined in the manifold $h: \mathcal{M} \to \mathcal{R}$. In the unit sphere case, consider `$h(\mathbf{x})$` subject to `$\mathbf{x}^T \mathbf{x}=1$`.
>Under parameterization $\mathbf{\tau}$, we can locally re-expressed the function as `$h_\tau(\mathbf{\tau}):=h( (\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2}) )$` where `$\tau \in \Omega_\tau$`.
>
>The parametric  representation of the vector should hold for the following identity for any smooth scalar function $h$: `$[\nabla h_\tau(\mathbf{\tau}_0)]^T \mathbf{v}(\mathbf{\tau}_0) =\frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0}$`, where $h_\tau$ is the parametric representation of  $h$ . 
>By the chain rule, we have `$\frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0}=[\nabla h_\tau(\mathbf{\tau}_0)]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$`, where `${\gamma}_\tau(0)=\tau_0$`. Thus,
> `$\mathbf{v}(\mathbf{\tau}_0) =  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$` since `$\nabla h_\tau(\mathbf{\tau}_0)$` can be arbitrary.
>
>Thus, a Riemannian gradient `$\mathbf{v}(\mathbf{\tau}_0)$` can be viewed as a parametric representation of the tangent vector of curve $\gamma(t)$ at $\mathbf{x}_0$ since  `${\gamma}_\tau(t)$` is the parametric representation of $\gamma(t)$.
 



# Parameter Transform and Invariance
------

>to be removed: key idea to avoid defining the covariant derivative:
>A transport of a scalar field can be expressed using a chart without defining the covariant derivative
>
>to be removed: (new type of auto-diff) Contravariant transformation for vector components (Riemannian gradients)
>
>to be removed: (auto-diff) Covariant transformation for covector components  (Euclidean gradients)
>
>to be removed:  Note that gradient invariance does not imply gradient update is invaiant



Recall that a Riemannian gradient is a parametric representation of the tangent direction of a curve in the manifold.
Since a curve and its tangent direction are geometric obejects, they should be invariant to the choice of parametrization.
In other words, geometric properties should be persevered in any valid coordinate system.


We now discuss one geometric property: the directional derivative should be same if we perform a parameter transform.

In the previous, we consider a point $\mathbf{x}_0$ in a manifold $\mathcal{M}$, a (1-dimensional) curve $\gamma(t)$, and a smooth scalar function $h: \mathcal{M} \to \mathcal{R}$.

Given an intrinsic parametrization $\tau$ containing the point, we consider the following parametric representations.
 
|   geometric object   |   parametric representation  |
|:------------|:-------------:|
| point `$\mathbf{x}_0$` |  `$\tau_0$`   | 
| curve  `$\gamma(t)$`  | `$\gamma_\tau(t) $`  | 
| function  `$h$`  | `$h_\tau(\tau) $`  |   




We want the following indentity holds for any two (intrinsic) parametrizations $\tau$ and $\lambda$.
`$$
\begin{aligned}
h(\gamma(t)) = h_\tau(\gamma_\tau(t)) = h_\lambda(\gamma_\lambda(t))
\end{aligned}
$$`

From the above expression, we can see that directional derivatives should be the same. 
`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} = \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} 
\end{aligned}\tag{3}\label{3}
$$`

In the previous section, we have shown that 
`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &=  [\nabla h_\tau(\mathbf{\tau}_0)  ]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}   \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} & = [\nabla h_\tau(\mathbf{\tau}_0) ]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}  
\end{aligned}
$$` where $\nabla$ is the standard (coordinate) derivate.


To simplify the notation, we follow the notations.

|  Notation   |   Meanings  |
|:------------|:-------------:|
| Euclidean gradient `$(g_\tau)_i$`  |  $i$-th entry  under parametrization $\tau$  | 
| Riemannian gradient `$(\hat{g}_\tau)^j$` |  $j$-th entry under parametrization $\tau$  | 
   
The derivational derivatives can be re-expressed as

`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &= \sum_{i}  (g_\tau)_i  (\hat{g}_\tau)^i  = {\mathbf{g}_\tau} \hat{\mathbf{g}}_\tau \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} & =\sum_{i}  (g_\lambda)_i (\hat{g}_\lambda)^i =  {\mathbf{g}}_\lambda \hat{\mathbf{g}}_\lambda
\end{aligned}\tag{4}\label{4}
$$`
where `$\mathbf{g}_\lambda$` and `$\mathbf{g}_\tau$` are **row** vectors (Euclidean gradients) while `$\hat{\mathbf{g}}_\lambda$` and `$\hat{\mathbf{g}}_\tau$` are **column** vectors (Riemannian gradients). 

By `$\eqref{3}$` and `$\eqref{4}$`, we the key invariance property for the directional derivative.
`$$
\begin{aligned}
 \mathbf{g}_\tau \hat{\mathbf{g}}_\tau  =  \mathbf{g}_\lambda \hat{\mathbf{g}}_\lambda 
\end{aligned}\tag{5}\label{5}
$$`


In the previous section, we have shown that `$\frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$` is a  parametric representation of a Riemannian vector, which is a Riemannian gradient.
Notice that `$\nabla h_\lambda(\mathbf{\lambda}_0)$` is a Euclidean gradient. 

Now, we discuss the parameter transform between $\tau$ and $\lambda$, where we can express $\lambda$ in terms of $\tau$ denoted by $\lambda(\tau)$.


By the chain rule for the Euclidean gradient, we has
`$$
\begin{aligned}
(g_\tau)_i = \sum_{k} \frac{\partial \lambda^k(\tau) }{ \partial \tau^i } (g_\lambda)_k
\end{aligned}
\tag{6}\label{6}
$$` 


Let $J_{ki}:=\frac{\partial \lambda^k(\tau) }{ \partial \tau^i }$ denotes the $(k,i)$ entry of the Jacobian matrix. We can express the Jacobian matrix as below.

`$$
\begin{aligned}
\begin{matrix}
& \\
\mathbf{J} = 
    \left ( \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    i=1 & i=2 \\ \hline
    J_{11} & J_{12}  \\
   J_{21} & J_{22} \\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right )
    \begin{matrix}
    k=1 \\ k=2 
    \end{matrix}
\end{matrix}
\end{aligned}
$$`

Eq. `$\eqref{6}$` gives us the transformation rule for Eulcidean gradients as

`$$
\begin{aligned}
\mathbf{g}_\tau =  \mathbf{g}_\lambda \mathbf{J}
\end{aligned},
$$`
where `${\mathbf{g}}_\tau$` can be computed via the vector-Jacobian product in any standard auto-diff toolbox given that `${\mathbf{g}}_\lambda$` is pre-computed.



By Eq `$\eqref{5}$`, we obtain the transformation rule for Riemannian gradients as
`$$
\begin{aligned}
 \hat{\mathbf{g}}_\tau= \mathbf{J}^{-1}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$` 
In other words,
`$$
\begin{aligned}
\mathbf{J}
 \hat{\mathbf{g}}_\tau=   \hat{\mathbf{g}}_\lambda 
\end{aligned},
$$` where `$\hat{\mathbf{g}}_\lambda$` can be computed via a Jacobian-vector product used in forward-mode differentiation given that `$\hat{\mathbf{g}}_\tau$` is pre-computed.


The elementwise expression is
`$$
\begin{aligned}
(g_\lambda)^k
 = \sum_{i} (g_\tau)^i  \frac{\partial \lambda^k(\tau)  }{\partial  \tau^i}
\end{aligned},
$$` 
Recall that these transformation rules are valid  when the Jacobian matrix is square and non-singular.


As we discussed in the previous post about [intrinsic parameterizations]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations), the transformation map between $\tau$ and $\lambda$ must be bi-jective, which implies the Jacoabian matrix is square. 
Moreover, the map and its inverse map should be smooth, which implies that Jacobian matrix is well-defined and non-singular.


Finally, we give a transformation rule for the Fisher information matrix as defined at [the previous post]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#fisher-rao-metric).
We will use this transformation rule  to show that another geometric property---the length of a Riemannian vector---is preserved.

`$$
 F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau) \Big) ]
$$`
Since $ \log p(w|\tau )$ can be considered as a scalar function $h$ defined on the manifold for any valid $w$, we have 
`$$
 \log p(w|\tau_0  ) = h_\tau(\tau_0) = h_\lambda(\lambda_0) =  \log p(w|\lambda_0  )
$$`

Thus, we have 
`$$
\begin{aligned}
 F_{ij}(\tau_0) &= E_{p(w|\tau_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ] \\
&=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ]\\
\end{aligned}
$$`

Recall that 
`$$
\partial_{\tau_i} \log p(w|\tau_0 ) = \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 )
$$`
 
Moreover, the Jacobian matrix does not depent on $w$. Therefore, we have
`$$
\begin{aligned}
 F_{ij}(\tau_0) 
&=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ]\\
&=  E_{p(w|\lambda_0) }  [ \Big( \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \sum_l \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  E_{p(w|\lambda_0) }  [ \Big(  \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } F_{kl}(\lambda_0)
\end{aligned}
$$`

We can re-express it in a matrix form as

`$$
\begin{aligned}
\mathbf{F}_{\tau} (\tau_0) = \underbrace{\mathbf{J}^T}_{  \frac{ \partial \lambda^i(\tau_0) }{ \partial \tau^k }  } \mathbf{F}_{\lambda} (\lambda_0) \underbrace{\mathbf{J}}_{  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  } 
\end{aligned}
$$`

We can see that the length of a Riemannian vector is also perseversed.
`$$
\begin{aligned}
\| \hat{\mathbf{g}}_\tau \|^2_F &= [\hat{\mathbf{g}}_\tau]^T \mathbf{F}_{\tau} (\tau_0) \hat{\mathbf{g}}_\tau \\
&= [\mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda]^T \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda \\
&= [\hat{\mathbf{g}}_\lambda]^T [ \mathbf{J}^{-T}  \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} ] \hat{\mathbf{g}}_\lambda \\
&= [\hat{\mathbf{g}}_\lambda]^T  \mathbf{F}_{\lambda} (\lambda_0)  \hat{\mathbf{g}}_\lambda = \| \hat{\mathbf{g}}_\lambda \|^2_F
\end{aligned}
$$`

Thus, the optimal solution of Eq. `$\eqref{2}$` under parametrization $\tau$ and $\lambda$ are equivalent since both the length and the directional derivative remain the same.






 
