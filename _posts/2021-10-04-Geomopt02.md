---
title: 'Basics of Information Geometry (2)'
date: 2021-09-06
permalink: /posts/2021/10/Geomopt02/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Goal
------
This blog post should help readers to understand natural-gradients also known as Riemannian gradients with the Fisher-Rao metric.


Euclidean steepest direction and directional derivative
------
Given a smooth scalar funcion $\min_{\tau \in \mathcal{R}^n } f(\mathbf{\tau})$, in a flat (Euclidean) space we can define its (Euclidean) steepest direction as the following optimization problem in terms of a directional derivative w.r.t. vector $\mathbf{v}$, where we assume $\nabla_\tau f(\mathbf{\tau})  \neq \mathbf{0}$.
`$$
\begin{aligned}
\min_{\|v\|^2=1} \lim_{h \to 0} \frac{f(\mathbf{\tau}+h\mathbf{v}) - f(\mathbf{\tau}) }{h} = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}) 
\end{aligned}\tag{1}\label{1}
$$` 

It is easy to see that the optimal solution of Eq. `$\eqref{1}$` is `$\mathbf{v}_{\text{opt}}= -\frac{\nabla_\tau f(\mathbf{\tau}) }{\|\nabla_\tau f(\mathbf{\tau}) \|}$`, which is the (Euclidean) steepest direction.

To generalize  the steepest direction in a Riemannian manifold, we first have to define the length of a vector in the manifold.


Distance induced by the Fisher-Rao metric 
------
As mentioned at [the previous post]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#fisher-rao-metric), the Fisher information matrix $\mathbf{F}$ should be positive definite. We can use it to define a distance between two vectors (e.g., Riemannian gradients) $\mathbf{v}$ and $\mathbf{w}$ in a manifold. 
`$$
\begin{aligned}
d_F(\mathbf{v},\mathbf{w}) := \mathbf{v}^T \mathbf{F} \mathbf{w} 
\end{aligned}
$$`

The positive-definiteness of the Fisher matrix is essential since we use it to define the length of a Riemannian vector $\mathbf{v}$.
`$$
\begin{aligned}
\|\mathbf{v}\|_F := d_F(\mathbf{v},\mathbf{v})
\end{aligned}
$$`

Note that the orthogonality is also induced by the Fisher matrix since we can define an angle between two vectors.


Directional derivatives in a manifold
------
The next step is to generalize directional derivatives in a manifold. 

Note that a manifold is locally like a flat (Euclidean) space under [**intrinsic** parameterization]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations) `$\mathbf{\tau}$`.
Using this parameterization, consider an optimization problem $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$, where the parameter space $\Omega_\tau$ is determined by the parameterization and the manifold. For simplicity, we assume $\mathbf{\tau}$ is a **global** intrinsic parameterization, which is often possible for [parametric families]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families). 

>For example, consider a 1-dimensional  Gaussian family as a manifold. <br />
>We specify an intrinsic parameterization $\mathbf{\tau}$ of the manifold as `$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with `$\tau = (\mu,\sigma) $`. <br />
>The parameter space  is $\Omega_\tau = \mathcal{R} \times \mathcal{R}_{++}$, where `$\mathcal{R}_{++}$` denotes the set of positive real numbers. This parameterization is global since we can use **one** coordinate system to cover the whole manifold.
>
>A possible optimization problem in machie learning is `$f(\mathbf{\tau}) =  E_{  p(w |\tau)  } [ \ell(w)  ]$` (see [ML examples]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#motivation)) 


Therefore, we can similarly define a directional derivative w.r.t. Riemannian vector $\mathbf{v}$ as `$\lim_{h \to 0} \frac{f(\mathbf{\tau}+h\mathbf{v}) - f(\mathbf{\tau}) }{h}$`. The subtle difference in the manifold case is that we use the distance (and the orthogonality) induced by the manifold metric instead of using the Euclidean counterpart. 



Recall that we allow a [small perturbation]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations) $E$ in the parameter space of $\mathbf{\tau}$ since  $\mathbf{\tau}$ is an intrinsic parameterization.
Therefore, when $h$ is small enough, $\mathbf{\tau}+h\mathbf{v} $ stays in the manifold and `$f(\mathbf{\tau}+h\mathbf{v})$` is well-defined since  $\mathbf{\tau}+h\mathbf{v} \in \Omega_\tau$

Under **intrinsic** parameterization $\mathbf{\tau}$, the directional derivative remains the same as in the Euclidean case.
`$\lim_{h \to 0} \frac{f(\mathbf{\tau}+h\mathbf{v}) - f(\mathbf{\tau}) }{h} = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau})$`


Riemannian steepest direction
------
Now, we will define the  Riemannian steepest direction.

Given  a smooth scalar funcion defined in a manifold $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$ under an intrinsic parameterization $\mathbf{\tau}$. We can define the Riemannian steepest direction as the following optimization problem in terms of a directional derivative w.r.t. Riemannian vector $\mathbf{v}$, where we assume $\nabla_\tau f(\mathbf{\tau})  \neq \mathbf{0}$.
`$$
\begin{aligned}
\min_{ \color{red} {\|v\|_{F}^2=1} }  \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}) 
\end{aligned} \tag{2}\label{2}
$$` 

We can write down the Lagrangian function, where $\lambda$ is a Lagrange multiplier. 
`$$
\begin{aligned}
L(\mathbf{v},\lambda) =  \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}) + \lambda (\|v\|_{F}^2 - 1) = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}) + \lambda (\mathbf{v}^T \mathbf{F}(\mathbf{\tau}) \mathbf{v}  - 1) 
\end{aligned}
$$`

One of the KKT necessary conditions implies that
`$$
\begin{aligned}
\mathbf{0} = \nabla_{v} L(\mathbf{v}_{\text{opt}},\lambda) = \nabla_\tau f(\mathbf{\tau}) + 2 \lambda \mathbf{F}(\mathbf{\tau}) \mathbf{v}_{\text{opt}}
\end{aligned}
$$`

We can show that the optimal solution of Eq. `$\eqref{2}$` is `$\mathbf{v}_{\text{opt}}= -\frac{ \mathbf{F}^{-1}(\mathbf{\tau}) \nabla_\tau f(\mathbf{\tau}) }{\| \mathbf{F}^{-1}(\mathbf{\tau})\nabla_\tau f(\mathbf{\tau}) \|_F}$`, which gives us the Riemannian steepest direction. The Fisher matrix $\mathbf{F}$ is invertible since it is positive definite.

The **Euclidean** steepest direction `$\mathbf{v}_{\text{euclid}}= -\frac{ \nabla_\tau f(\mathbf{\tau}) }{\| \nabla_\tau f(\mathbf{\tau}) \|_F}$` is **not** the optimal solution of  Eq. `$\eqref{2}$` since it does not satisfy the KKT condition.

For manifold optimization, given a scalar function $f(\mathbf{\tau})$, we denote its **Euclidean** gradient by $\nabla_\tau f(\mathbf{\tau})$ and the **Riemannian** gradient by $ \mathbf{F}^{-1}(\mathbf{\tau}) \nabla_\tau f(\mathbf{\tau})$. Since we use the Fisher-Rao metric, the Riemannian gradient is also known as the **natural** gradient. We use a learning-rate to control the length of a gradient instead of normalizing its length. 




Riemannian gradients as tangent vectors
------
A Riemannian gradient denoted by $\mathbf{v}$ is indeed a tangent vector in the manifold under the parametrization $\tau$. 
The set of tangent vectors is called the tangent space at the point $\mathbf{\tau}=\mathbf{\tau_0}$. 


Let's denote the unit sphere by $\mathcal{M}$, where we set the origin to be the center of the sphere. Point $\mathbf{x_0}=(0,0,1)$ is the north pole.
We use the following parameterization, where the manifold can be locally expressed as `$\{(\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2})|  \tau_x^2 + \tau_y^2 <1 \}$` with parameter $\mathbf{\tau}=(\tau_x,\tau_y)$. 
Under the parametrization $\mathbf{\tau}$, we have the following parametric representations.  

|   &nbsp; &nbsp; &nbsp;    | Parametric representation     | 
|:------------|:-------------:|
| North pole  $\mathbf{x_0}$   | $\mathbf{\tau_0}=(0,0)$  |  
| Tangent space at $\mathbf{x_0}$     |  red flat space `$\Omega_\tau:=\{ (\tau_x,\tau_y)| \tau_x^2 + \tau_y^2 <1 \}$`   |
| Yellow curve from $\mathbf{x_0}$ to $\mathbf{x_1}$    |  blue line segment from $\mathbf{\tau_0}$   |  



<img src="/img/sphere.png"  width="1000"/>



The tangent vector $\mathbf{v}$ at point $\mathbf{x_0}$  can be viewed the **tangent direction** of a (1-dimensional) smooth curve $\gamma(t) \in \mathcal{M}$, where $\gamma(0)=\mathbf{x_0}$. 
Given parametrization $\tau$, we can define the parametric representation of the curve denoted by $\bar{\gamma}(t)$. For example, consider $\gamma(t) = (t v_{x}, t v_{y}, \sqrt{1 - t^2(v_{x}^2 + v_{y}^2) } ) \in \mathcal{M} $ and $\bar{\gamma}(t)= (t v_{x} , t v_y  ) $, where $t$ is small enough. The parametric  representation of the vector is defined as `$\mathbf{v}(\mathbf{\tau_0}):= \frac{d \bar{\gamma}(t) }{d t} \Big|_{t=0} $`, where `$\bar{\gamma}(0)=\tau_0$`. 

We can also obtain the parametric  representation of the vector via directional derivatives.
Consider a smooth scalar function defined in the manifold $h: \mathcal{M} \to \mathcal{R}$. In this example, consider `$h(\mathbf{x})$` subject to `$\mathbf{x}^T \mathbf{x}=1$`.
Under parameterization $\tau$, we can locally re-expressed the function as `$f(\mathbf{\tau}):=h( (\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2}) )$` where `$\tau \in \Omega_\tau$`.
The parametric  representation of the vector should hold for the following identity for any smooth scalar function $h$: `$\mathbf{v}(\mathbf{\tau_0})^T \nabla_\tau f(\mathbf{\tau_0})=\frac{d f(\bar{\gamma}(t)) }{d t} \Big|_{t=0} $`, where $f$ is the parametric representation of  $h$. From Eq. `$\eqref{2}$` , we can see that a Riemannian gradient can be viewed as a parametric representation of the corresponing tangent vector.
 

Euclidean gradients as Cotangent vectors
------
(W.I.P.)


Parametric Invariance
------
(W.I.P.)

Notice that a curve in the manifold is a geometric object, which is invariance to the choice of parametrization.
It may be easy to understand the parametric invariance via a parameter transformation.

(new type of auto-diff) Contravariant transformation for vector components (Riemannian gradients)


(auto-diff) Covariant transformation for covector components  (Euclidean gradients)


Note that gradient invariance does not imply gradient update is invaiant




