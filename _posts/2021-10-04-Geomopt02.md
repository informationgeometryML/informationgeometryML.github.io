---
title: 'Part II: the Tangent Space at one Point'
date: 2021-10-04
permalink: /posts/2021/10/Geomopt02/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Goal
------
This blog post should help readers to understand the (tangent) space of natural-gradients also known as Riemannian gradients with the Fisher-Rao metric.

We will give an informal introduction with a focus on intuitions.

Euclidean steepest direction and directional derivative
------
We first show how to obtain a Euclidean gradient as the steepest direction by solving an optimization problem. We will extend the steepest direction in Riemannian cases and show that the steepest direction is indeed a natural-gradient.

Given a smooth scalar funcion $\min_{\tau \in \mathcal{R}^n } f(\mathbf{\tau})$ in a vector space, we can define the (Euclidean) steepest direction at current `$\mathbf{\tau}_0$` as the solution to the following optimization problem in terms of a directional derivative w.r.t. vector $\mathbf{v}$, where we assume `$\nabla_\tau f(\mathbf{\tau}_0)  \neq \mathbf{0}$`.
`$$
\begin{aligned}
\min_{\|v\|^2=1} \lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t} = ( \nabla_\tau f(\mathbf{\tau}_0) )^T \mathbf{v} 
\end{aligned}\tag{1}\label{1}
$$` 

It is easy to see that the optimal solution of Eq. `$\eqref{1}$` is `$\mathbf{v}_{\text{opt}}= -\frac{\nabla_\tau f(\mathbf{\tau}_0) }{\|\nabla_\tau f(\mathbf{\tau}_0) \|}$`, which is the (Euclidean) steepest direction at point `$\mathbf{\tau}_0$`.


Distance induced by the Fisher-Rao metric 
------

To generalize  the steepest direction at a given point (e.g., `$\mathbf{\tau}_0$`) in a Riemannian manifold, we first formulate a similar optimization problem like Eq. `$\eqref{1}$` in the manifold case.
To do so, we have to define the length of a vector in manifold cases.


As mentioned at [the previous post]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#fisher-rao-metric), the FIM $\mathbf{F}$ should be positive definite. We can use it to define the length/norm of a vector (e.g., a Riemannian gradient) $\mathbf{v}$ at a point in a manifold via a weighted inner product.
`$$
\begin{aligned}
\|\mathbf{v}\|_F := \sqrt{\mathbf{v}^T \mathbf{F} \mathbf{v}}
\end{aligned}
$$`

The positive-definiteness of FIM is essential since we do not want a non-zero vector has a zero length.

The distance (and orthogonality) between two <span style="color:red">vectors at the same point</span> is also induced by FIM since we can define them by the inner product as
`$$
\begin{aligned}
d(\mathbf{v},\mathbf{w}) := \|\mathbf{v}-\mathbf{w}\|_F
\end{aligned}
$$`
where we can consider `$\mathbf{v}$` and `$\mathbf{w}$` are two vectors evaluted at point `$\tau_0$`.


**Warning**:  We do NOT define the distance between two points in the manifold, which will be discussed [here](#riemannian-gradients-as-tangent-vectors).
We also do NOT define the distance between a vector at one point and another vector at a distinct point.

# Directional derivatives in a manifold
------
As we shown before, the objective function in Eq. `$\eqref{1}$` is a directional derivative in Euclidean cases.
The next step is to generalize the concept of directional derivatives in a manifold. 


Recall that a manifold should be locally like a vector space under [**intrinsic** parameterization]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations) `$\mathbf{\tau}$`.
Using this parameterization, consider an optimization problem $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$, where the parameter space $\Omega_\tau$ is determined by the parameterization and the manifold. 

Therefore, we can similarly define a directional derivative at `$\mathbf{\tau}_0$` w.r.t. Riemannian vector $\mathbf{v}$ as `$\lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t}$`, where $t$ is a scalar real number. The main question is how we can show `$\mathbf{\tau}_0+t\mathbf{v}$` stays in the parameter space `$\Omega_\tau$`.  


Recall that we allow a [small perturbation]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations) $E$ in the parameter space of $\mathbf{\tau}$ since  $\mathbf{\tau}$ is an intrinsic parameterization.
Therefore, when $|t|$ is small enough, `$\mathbf{\tau}_0+t\mathbf{v} $` stays in the parameter space and `$f(\mathbf{\tau}_0+t\mathbf{v})$` is well-defined.


Under **intrinsic** parameterization $\mathbf{\tau}$, the directional derivative remains the same as in the Euclidean case.
`$\lim_{t \to 0} \frac{f(\mathbf{\tau}_0+t\mathbf{v}) - f(\mathbf{\tau}_0) }{t} = ( \nabla_\tau f(\mathbf{\tau}_0))^T \mathbf{v} $`. Note that we only require `$\mathbf{\tau}_0+t\mathbf{v} \in \Omega_\tau$` when $|t|$ is small enough. This is possible since $\Omega_\tau$ is an open set in $\mathcal{R}^k$, where $k$ is the number of parameters. 

>Example 1:
>
>`$\tau$` is a **local intrinsic** parameterization for the unit sphere.
>  
>The line segment from `$\mathbf{\tau}_0$` to `$\mathbf{\tau}_0+t\mathbf{v} $`  is shown in blue, which is the parameter representation of the yellow curve `$\gamma(t)$` in the manifold.
>We will show later that Riemannian vector `$\mathbf{v}$` at `$\mathbf{\tau}_0$` is the **parameter representation** of the tangent vector of curve `$\gamma(t)$` at point `$\mathbf{x}_0$`.
>
><img src="/img/sphere_simple.png"  width="500"/>
>
>**Warning**: Curve `$\gamma(t)$` is not the shortest curve in the manifold between `$\mathbf{x}_0$` and `$\mathbf{x}_1$`. The shortest curve is not a straight line in curved manifold cases.



>Example 2:
>
>A directional derivative can be ill-defined under a **non-intrinsic** parameterization.
>
>We use [parameterization 3]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations) for unit circle `$\mathcal{S}^1$`, where the red line segment passes through `$\tau_0=(0,1) \in \mathcal{S}^1 $`.
>
>![Figure 1](/img/tangent_non.png) 
>
>Any other point `$\tau_0 + t\mathbf{v}$` in the line segment leaves the manifold for `$t>0$` and thus, `$f(\mathbf{\tau}_0+t\mathbf{v})$` is not well-defined.
>The main reason is that `$\tau$` is not an intrinsic parameterization.

>Example 3:
>
>For [parametric families]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations-for-parametric-families), we often can find a **global** intrinsic parameterization $\mathbf{\tau}$.
>
>Consider a 1-dimensional  Gaussian family as a manifold. <br />
>We specify an intrinsic parameterization $\mathbf{\tau}$ of the manifold as `$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with `$\tau = (\mu,\sigma) $`. <br />
>The parameter space  is $\Omega_\tau = \mathcal{R} \times \mathcal{R}_{++}$, where `$\mathcal{R}_{++}$` denotes the set of positive real numbers. This parameterization is global since we can use **one** coordinate system to cover the whole manifold.
>
>A possible optimization problem in machie learning is `$f(\mathbf{\tau}) =  E_{  p(w |\tau)  } [ \ell(w)  ]$` (see [ML examples]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#motivation)) 


>Example 4:
>
>Consider a $d$-dimensional Gaussian family with zero mean as a manifold. <br />
>We specify an intrinsic parameterization $\mathbf{\tau}$ of the manifold as `$ \{ \mathcal{N}(\mathbf{w} |\mathbf{0},\mathbf{\Sigma}) \Big| \mathrm{MatH}(\tau) = \mathbf{\Sigma}   \succ \mathbf{0} \}$` with `$\tau = \mathrm{vech}(\mathbf{\Sigma})$`, where `$\tau$` is a $\frac{d(d+1)}{2}$-dim vector and map $\mathrm{MatH}()$ is the inverse map of the lower-triangular half $\mathrm{vech}()$. 
>
>Technically speaking, $\mathbf{\Sigma}$ is NOT an intrinsic parameter due to the symmetry constraint. In other words, FIM w.r.t. $\mathbf{\Sigma}$ will be singular if  `$\mathbf{\Sigma}$` is considered as a matrix parameter with $d^2$ degrees of freedom.
>
>In literature, a natural gradient for $\mathbf{\Sigma}$ is defined by $\mathrm{MatH}(\mathbf{v})$, where $\mathbf{v}$ is a natural-gradient w.r.t. `$\mathrm{vech}(\Sigma)$`. 
 



Riemannian steepest direction
------
Now, we will define the  Riemannian steepest direction.

Given  a smooth scalar funcion defined in a manifold $\min_{\tau \in \Omega_\tau } f(\mathbf{\tau})$ under an intrinsic parameterization $\mathbf{\tau}$. We can define the Riemannian steepest direction as the solution to the following optimization problem in terms of a directional derivative w.r.t. Riemannian vector $\mathbf{v}$, where we assume `$\nabla_\tau f(\mathbf{\tau}_0)  \neq \mathbf{0}$`.
`$$
\begin{aligned}
\min_{ \color{red} {\|v\|_{F}^2=1} } ( \nabla_\tau f(\mathbf{\tau}_0) )^T  \mathbf{v} 
\end{aligned} \tag{2}\label{2}
$$` 
where all possible $\mathbf{v}$ are vectors at current point `$\mathbf{\tau}_0$`.

The Lagrangian function is given below, where $\lambda$ is a Lagrange multiplier. 
`$$
\begin{aligned}
L(\mathbf{v},\lambda) =  ( \nabla_\tau f(\mathbf{\tau}_0))^T \mathbf{v} + \lambda (\|v\|_{F}^2 - 1) = \mathbf{v}^T \nabla_\tau f(\mathbf{\tau}_0) + \lambda (\mathbf{v}^T \mathbf{F}(\mathbf{\tau}_0) \mathbf{v}  - 1) 
\end{aligned}
$$` where `$\mathbf{F}(\mathbf{\tau}_0)$` is FIM.

One of the KKT necessary conditions implies that
`$$
\begin{aligned}
\mathbf{0} = \nabla_{v} L(\mathbf{v}_{\text{opt}},\lambda) = \nabla_\tau f(\mathbf{\tau}_0) + 2 \lambda \mathbf{F}(\mathbf{\tau}_0) \mathbf{v}_{\text{opt}}
\end{aligned}
$$`
When $\lambda \neq 0$, vector 	`$\mathbf{v}_{\text{opt}}$` should be proportional to `$\mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0)$`, where  `$\mathbf{F}^{-1}(\mathbf{\tau}_0)$` is well-defined since FIM `$\mathbf{F}(\mathbf{\tau}_0)$` is positive definite.
 

We can show that the optimal solution of Eq. `$\eqref{2}$` is `$\mathbf{v}_{\text{opt}}= -\frac{ \mathbf{F}^{-1}(\mathbf{\tau}_0) \nabla_\tau f(\mathbf{\tau}_0) }{\| \mathbf{F}^{-1}(\mathbf{\tau}_0)\nabla_\tau f(\mathbf{\tau}_0) \|_F}$`, which gives us the Riemannian steepest direction at current `$\mathbf{\tau}_0$`. 

The **Euclidean** steepest direction `$\mathbf{v}_{\text{euclid}}= -\frac{ \nabla_\tau f(\mathbf{\tau}_0) }{\| \nabla_\tau f(\mathbf{\tau}_0) \|_F}$` is **not** the optimal solution of  Eq. `$\eqref{2}$` when `$\mathbf{F}(\tau_0) \neq \mathbf{I}$`.
We will illustrate this by using an example.

>Example
>
>Consider `$\mathbf{F}(\tau_0)=\begin{bmatrix} 1 & 0 \\ 0 & \frac{1}{2} \end{bmatrix}$` and `$\nabla_\tau f(\mathbf{\tau}_0)=\begin{bmatrix} 1\\1 \end{bmatrix}$`.
>We have the following results
> `$$
\begin{aligned}
\| F^{-1} \nabla_\tau f(\mathbf{\tau}_0) \|_F^2  =  \nabla_\tau^T f(\mathbf{\tau}_0) \mathbf{F}^{-1}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) = 3; \,\,\,
\| \nabla_\tau f(\mathbf{\tau}_0) \|_F^2  =  \nabla_\tau^T f(\mathbf{\tau}_0) \mathbf{F}(\tau_0) \nabla_\tau f(\mathbf{\tau}_0) = \frac{3}{2}
\end{aligned}
$$`
> `$$
\begin{aligned}
\mathbf{v}_{\text{opt}} = -\begin{bmatrix} \frac{1}{\sqrt{3}} \\ \frac{2}{\sqrt{3}} \end{bmatrix}; \,\,\,
\mathbf{v}_{\text{euclid}}=
-\begin{bmatrix} \sqrt{\frac{2}{3}} \\ \sqrt{\frac{2}{3}} \end{bmatrix}\end{aligned}
$$`
>`$$
\begin{aligned}
 \mathbf{v}_{\text{opt}}^T \nabla_\tau f(\mathbf{\tau}_0)= -\sqrt{3}  <  -\frac{2\sqrt{2}}{\sqrt{3}} = \mathbf{v}_{\text{euclid}}^T \nabla_\tau f(\mathbf{\tau}_0) 
\end{aligned}
$$`
>
>Therefore, the Euclidean steepest direction `$\mathbf{v}_{\text{euclid}}$` is not the optimal solution of  Eq. `$\eqref{2}$`.


Given a scalar function $f(\mathbf{\tau})$, if its **Euclidean** (steepest) gradient is $\nabla_\tau f(\mathbf{\tau})$, its **Riemannian** (steepest) gradient is defined as $ \mathbf{F}^{-1}(\mathbf{\tau}) \nabla_\tau f(\mathbf{\tau})$ in literature.
We use a learning-rate to control the length of a gradient instead of normalizing its length. 
Since we use the Fisher-Rao metric, the Riemannian gradient is also known as the **natural** gradient.







# Riemannian gradients as tangent vectors
------
A Riemannian gradient denoted by $\mathbf{v}$ is indeed a tangent vector in the manifold under the parametrization $\tau$. 
The set of tangent vectors is called the tangent space at the point $\mathbf{\tau}=\mathbf{\tau}_0$. 


Let's denote the unit sphere by $\mathcal{M}$, where we set the origin to be the center of the sphere. Point $\mathbf{x_0}=(0,0,1)$ is the north pole.
We use the following parameterization, where the top half of the sphere can be locally expressed as `$\{(\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2})|  \tau_x^2 + \tau_y^2 <1 \}$` with parameter $\mathbf{\tau}=(\tau_x,\tau_y)$. 
Under parametrization $\mathbf{\tau}$, we have the following parametric representations.  

|   &nbsp; &nbsp; &nbsp;    | Parametric representation     | 
|:------------|:-------------:|
| North pole  $\mathbf{x_0}$   | $\mathbf{\tau}_0=(0,0)$  |  
| Intrinsic parameter space     |  red space `$\Omega_\tau:=\{ (\tau_x,\tau_y)| \tau_x^2 + \tau_y^2 <1 \}$`   |
| Tangent space at $\mathbf{x_0}$     |  green space  `$\mathcal{R}^2$` at `$\mathbf{\tau}_0$`   |
| Yellow curve from $\mathbf{x_0}$ to $\mathbf{x_1}$    |  blue line segment from `$\mathbf{\tau}_0$` to `$\mathbf{\tau}_0+t\mathbf{v}(\tau_0)$`   |  



<img src="/img/sphere.png"  width="500"/>

**Warning**: Be aware of the differences shown in the table.

|   &nbsp; &nbsp; &nbsp;    |   parametric representation of   |     supported operations   |      distance  discussed in this post  |
|:------------|:-------------:|:-------------:|
|  `$\mathcal{R}^2$` space |   tangent vector space at `$\mathbf{x}_0$`  | real scalar product, vector addition  | defined |
|   `$\Omega_\tau$` space | top half of the manifold  |   **local**  scalar product, **local** vector addition   |  undefined |

Under **intrinsic** parametrization $\tau$, we have `$\Omega_\tau \subset \mathcal{R}^2$`. Thus, we can perform the operation in $\Omega_\tau$ space: `$\tau_0 +t\mathbf{v}(\tau_0) \in \Omega_\tau$` when scalar `$|t|$` is small enough. Note that we only define the distance between two vectors in the `$\mathcal{R}^2$` space. The distance between two points in the `$\Omega_\tau$` space is undefined.



The tangent vector $\mathbf{v}$ at point $\mathbf{x_0}$  can be viewed as the **tangent direction** of a (1-dimensional) smooth curve $\gamma(t) \in \mathcal{M}$, where $\gamma(0)=\mathbf{x_0}$ and the support of $t$ is an open interval containing 0. 
Given parametrization $\tau$, we can define the parametric representation of the curve denoted by ${\gamma}_\tau(t)$. 
The parametric representation of the vector is defined as `$\mathbf{v}(\mathbf{\tau}_0):= \frac{d {\gamma}_{\tau}(t) }{d t} \Big|_{t=0}$`, where `${\gamma}_{\tau}(0)=\tau_0$`. 
We can also define vector additions and real scalar products by using tangent directions of curves in the manifold. 

>Example
>
>Consider the yellow curve $\gamma(t) = (t v_{x}, t v_{y}, \sqrt{1 - t^2(v_{x}^2 + v_{y}^2) } ) \in \mathcal{M} $ 
>and the blue line segment `${\gamma}_{\tau}(t)= (t v_{x} , t v_y  ) \in \Omega_\tau $`, where `$|t|$` must be small enough. 
>
>The parametric  representation of the vector is `$\mathbf{v}(\mathbf{\tau}_0):= \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}=(v_x,v_y)$`.

The parametric  representation of the tangent vector can also be derived from directional derivatives as shown below.

>
>Consider a smooth scalar function defined in the manifold $h: \mathcal{M} \to \mathcal{R}$. In the unit sphere case, consider `$h(\mathbf{x})$` subject to `$\mathbf{x}^T \mathbf{x}=1$`.
>Under parameterization $\mathbf{\tau}$, we can locally re-expressed the function as `$h_\tau(\mathbf{\tau}):=h( (\tau_x,\tau_y,\sqrt{1-\tau_x^2-\tau_y^2}) )$` where `$\tau \in \Omega_\tau$`.
>
>The parametric  representation of the vector should hold for the following identity for any smooth scalar function $h$: `$[\nabla h_\tau(\mathbf{\tau}_0)]^T \mathbf{v}(\mathbf{\tau}_0) =\frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0}$`, where $h_\tau$ is the parametric representation of  $h$ . 
>By the chain rule, we have `$\frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0}=[\nabla h_\tau(\mathbf{\tau}_0)]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$`, where `${\gamma}_\tau(0)=\tau_0$`. Thus,
> `$\mathbf{v}(\mathbf{\tau}_0) =  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$` since `$\nabla h_\tau(\mathbf{\tau}_0)$` can be arbitrary.
>
>Thus, a Riemannian gradient `$\mathbf{v}(\mathbf{\tau}_0)$` can be viewed as a parametric representation of the tangent vector of curve $\gamma(t)$ at $\mathbf{x}_0$ since  `${\gamma}_\tau(t)$` is the parametric representation of $\gamma(t)$.
 



# Parameter Transform and Invariance
------

>to be removed: key idea to avoid defining the covariant derivative:
>A transport of a scalar field can be expressed using a chart without defining the covariant derivative
>
>to be removed: (new type of auto-diff) Contravariant transformation for vector components (Riemannian gradients)
>
>to be removed: (auto-diff) Covariant transformation for covector components  (Euclidean gradients)
>
>to be removed:  Note that gradient invariance does not imply gradient update is invaiant



Recall that a Riemannian gradient is a parametric representation of the tangent direction of a curve in the manifold.
Since a curve and its tangent direction are geometric obejects, they should be invariant to the choice of parametrization.
In other words, geometric properties should be persevered in any valid coordinate system.


We now discuss one **geometric property**: the directional derivative should be same if we perform a parameter transform.

In the previous, we consider a point $\mathbf{x}_0$ in a manifold $\mathcal{M}$, a (1-dimensional) curve $\gamma(t)$, and a smooth scalar function $h: \mathcal{M} \to \mathcal{R}$.

Given an intrinsic parametrization $\tau$ containing the point, we consider the following parametric representations.
 
|   geometric object   |   parametric representation  |
|:------------|:-------------:|
| point `$\mathbf{x}_0$` |  `$\tau_0$`   | 
| curve  `$\gamma(t)$`  | `$\gamma_\tau(t) $`  | 
| function  `$h$`  | `$h_\tau(\tau) $`  |   




We want the following indentity holds for any two (intrinsic) parametrizations $\tau$ and $\lambda$.
`$$
\begin{aligned}
h(\gamma(t)) = h_\tau(\gamma_\tau(t)) = h_\lambda(\gamma_\lambda(t))
\end{aligned}
$$`

From the above expression, we can see that directional derivatives should be the same. 
`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} = \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} 
\end{aligned}\tag{3}\label{3}
$$`

In the previous section, we have shown that 
`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &=  [\nabla h_\tau(\mathbf{\tau}_0)  ]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}   \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} & = [\nabla h_\lambda(\mathbf{\lambda}_0) ]^T  \frac{d {\gamma}_\lambda(t) }{d t} \Big|_{t=0}  
\end{aligned}
$$` where $\nabla$ is the standard (coordinate) derivate.


We use the following the notations to simplify the expressions.

|  Notation   |   Meanings  |
|:------------|:-------------:|
| Euclidean gradient `$(g_\tau)_i$`  |  $i$-th entry  under parametrization $\tau$  | 
| Riemannian gradient `$(\hat{g}_\tau)^j$` |  $j$-th entry under parametrization $\tau$  | 
| Parameter `$\tau^j$` |  $j$-th parameter under parametrization   $\tau$  | 
   
The derivational derivatives can be re-expressed as

`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &= \sum_{i}  (g_\tau)_i  (\hat{g}_\tau)^i  = {\mathbf{g}_\tau} \hat{\mathbf{g}}_\tau \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} & =\sum_{i}  (g_\lambda)_i (\hat{g}_\lambda)^i =  {\mathbf{g}}_\lambda \hat{\mathbf{g}}_\lambda
\end{aligned}\tag{4}\label{4}
$$`
where `$\mathbf{g}_\lambda$` and `$\mathbf{g}_\tau$` (e.g.,  `$\mathbf{g}_\tau=(\nabla h_\tau(\tau_0) )^T$`) are **row** vectors (Euclidean gradients) while `$\hat{\mathbf{g}}_\lambda$` and `$\hat{\mathbf{g}}_\tau$` (e.g.,`$\hat{\mathbf{g}}_\tau=\mathbf{v}_\tau(\tau_0) =\frac{d \gamma_\tau(0) }{d t}$`) are **column** vectors (Riemannian gradients). 

By `$\eqref{3}$` and `$\eqref{4}$`, we have  one invariance property for the directional derivative.
`$$
\begin{aligned}
 \mathbf{g}_\tau \hat{\mathbf{g}}_\tau  =  \mathbf{g}_\lambda \hat{\mathbf{g}}_\lambda 
\end{aligned}\tag{5}\label{5}
$$`


In the previous section, we have shown that `$\frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$` is a  parametric representation of a Riemannian vector, which is a Riemannian gradient.
Notice that `$\nabla h_\lambda(\mathbf{\lambda}_0)$` is a Euclidean gradient. 

Now, we discuss the parameter transform between $\tau$ and $\lambda$, where we can express $\lambda$ in terms of $\tau$ denoted by $\lambda(\tau)$.


By the chain rule for the Euclidean gradient, we has
`$$
\begin{aligned}
(g_\tau)_i = \sum_{k}  \color{red}{ (g_\lambda)_k} \frac{\color{red}{ \partial \lambda^k(\tau) }}{ \partial \tau^i } 
\end{aligned}
\tag{6}\label{6}
$$` 


Let $J_{ki}:=\frac{\partial \lambda^k(\tau) }{ \partial \tau^i }$ denotes the $(k,i)$ entry of the Jacobian matrix. We can express the Jacobian matrix as below.

`$$
\begin{aligned}
\begin{matrix}
& \\
\mathbf{J} = 
    \left ( \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    i=1 & i=2 \\ \hline
    J_{11} & J_{12}  \\
   J_{21} & J_{22} \\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right )
    \begin{matrix}
    k=1 \\ k=2 
    \end{matrix}
\end{matrix}
\end{aligned}
$$`

Eq. `$\eqref{6}$` gives us the transformation rule for Eulcidean gradients as

`$$
\begin{aligned}
\mathbf{g}_\tau =  \mathbf{g}_\lambda \mathbf{J}
\end{aligned},
$$`
where `${\mathbf{g}}_\tau$` can be computed via a vector-Jacobian product in any standard Auto-Diff toolbox given that `${\mathbf{g}}_\lambda$` is pre-computed.



By Eq `$\eqref{5}$`, we obtain the transformation rule for Riemannian gradients as
`$$
\begin{aligned}
 \hat{\mathbf{g}}_\tau= \mathbf{J}^{-1}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$` where $\mathbf{Q}:=\mathbf{J}^{-1}$ is also a Jacobian matrix and $Q_{ki}=\frac{\partial \tau^k(\lambda)}{\partial \lambda^i}$ is the $(k,i)$ entry of the matrix.

Note that `$\hat{\mathbf{g}}_\tau$` can be computed via a Jacobian-vector product used in forward-mode differentiation given that `$\hat{\mathbf{g}}_\lambda$` is pre-computed.


The elementwise expression of the transformation rule for Riemannian gradients  is
`$$
\begin{aligned}
(g_\tau)^k
 = \sum_{i}  \frac{ \partial \tau^k(\lambda)  }{\color{red} {\partial  \lambda^i} }  \color{red} {(g_\lambda)^i}
\end{aligned},
$$` 
Recall that these transformation rules are valid  when the Jacobian matrix is square and non-singular.


As we discussed in the previous post about [intrinsic parameterizations]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations), the transformation map between $\tau$ and $\lambda$ must be bi-jective, which implies the Jacoabian matrix is square. 
Moreover, the map and its inverse map should be smooth, which implies that Jacobian matrix is well-defined and non-singular.


Finally, we give a transformation rule for the Fisher information matrix as defined at [the previous post]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#fisher-rao-metric).
We will use this transformation rule  to show that another **geometric property**---the length of a Riemannian vector---is preserved.

`$$
\begin{aligned}
 F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau) \Big) ]
\end{aligned}
$$`
Since $ \log p(w|\tau )$ can be considered as a scalar function $h$ defined on the manifold for any valid $w$, we have 
`$$
\begin{aligned}
 \log p(w|\tau_0  ) = h_\tau(\tau_0) = h_\lambda(\lambda_0) =  \log p(w|\lambda_0  )
\end{aligned}
$$`

Thus, we have 
`$$
\begin{aligned}
 F_{ij}(\tau_0) &= E_{p(w|\tau_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ] \\
&=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ]\\
\end{aligned}
$$`

Recall that 
`$$
\begin{aligned}
\partial_{\tau_i} \log p(w|\tau_0 ) = \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 )
\end{aligned}
$$`
 
Moreover, the Jacobian matrix does not depent on $w$. Therefore, we have
`$$
\begin{aligned}
 F_{ij}(\tau_0) 
&=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ]\\
&=  E_{p(w|\lambda_0) }  [ \Big( \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \sum_l \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  E_{p(w|\lambda_0) }  [ \Big(  \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } F_{kl}(\lambda_0)
\end{aligned}
$$`

We can re-express the above expression in a matrix form as

`$$
\begin{aligned}
\mathbf{F}_{\tau} (\tau_0) = \underbrace{\mathbf{J}^T}_{  \frac{ \partial \lambda^i(\tau_0) }{ \partial \tau^k }  } \mathbf{F}_{\lambda} (\lambda_0) \underbrace{\mathbf{J}}_{  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  } 
\end{aligned}
$$`

We can see that the length of a Riemannian vector is also invariant.
`$$
\begin{aligned}
\| \hat{\mathbf{g}}_\tau \|^2_F &= [\hat{\mathbf{g}}_\tau]^T \mathbf{F}_{\tau} (\tau_0) \hat{\mathbf{g}}_\tau \\
&= [\mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda]^T \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda \\
&= [\hat{\mathbf{g}}_\lambda]^T [ \mathbf{J}^{-T}  \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} ] \hat{\mathbf{g}}_\lambda \\
&= [\hat{\mathbf{g}}_\lambda]^T  \mathbf{F}_{\lambda} (\lambda_0)  \hat{\mathbf{g}}_\lambda = \| \hat{\mathbf{g}}_\lambda \|^2_F
\end{aligned}
$$`

Thus, the optimal solution of Eq. `$\eqref{2}$` under parametrization $\tau$ and $\lambda$ are equivalent since both the length and the directional derivative remain the same.
`$$
\begin{aligned}
(\nabla  f_\tau(\mathbf{\tau}_0)  )^T   \mathbf{v}^{(opt)}_{\tau}  &=   (\nabla  f_\lambda(\mathbf{\lambda}_0) )^T \mathbf{v}^{(opt)}_{\lambda} \\
\|  \mathbf{v}^{(opt)}_{\tau} \|^2_{F_\tau} & = \|  \mathbf{v}^{(opt)}_{\lambda} \|^2_{F_\lambda} \\
\mathbf{v}^{(opt)}_{\tau} & = \mathbf{J}^{-1}  \mathbf{v}^{(opt)}_{\lambda} \\
[\nabla  f_\tau(\mathbf{\tau}_0)]^T & =  [\nabla  f_\lambda(\mathbf{\lambda}_0)]^T \mathbf{J}
\end{aligned}.
$$` 

todo: show to GD case is not invariant and how to fix it. Connect to the invarinace of Newton's method

todo: the distance between two points is the manifold is not invariant unless we use the exact geodesic.




 
