---
title: 'Part IV: Natural and Riemannian  Gradient Descent'
date: 2021-11-15
permalink: /posts/2021/11/Geomopt04/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Warning: working in Progress (incomplete)

Goal
------
This blog post should help readers to understand natural-gradient descent and Riemannian gradient descent.

We will give an informal introduction with a focus on high level of ideas.




# Two kinds of Spaces
------
As we disucssed in [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional), the parameter space $\Omega_\tau$ and the tangent space denoted by `$T\mathcal{M}_{\tau_0}$` at point $\tau_0$ are different spaces. Recall that the tangent space is a vector space and `$T\mathcal{M}_{\tau_0}=\mathcal{R}^K$` while the parameter space $\Omega_\tau$ is like a local vector space in `$\mathcal{R}^K$`, where $K$ is the dimension of the manifold. Moreover, `$\Omega_\tau$` is often an open (proper) subset of `$T\mathcal{M}_{\tau_0}$` since  $\tau$ is an [intrinsic parametrization]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations).

In a manifold case, we have to explicitly distinguish the difference between the representation of a point/parameter and a (Riemannian) vector/gradient.
The following figure illustrates the difference between the **domain** of these two spaces. Moreover, the **norm/distance** in each of these two spaces is defined differently. 


<img src="/img/sphere.png"  width="500"/>
 


# Natural-gradient Descent in an Intrinsic Parameter Space
------

Using intrinstic parametrization $\tau$, we can perform a natural-gradient update known as natural-gradient descent (NGD). 
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{1}\label{1}
$$` where `$\hat{\mathbf{g}}_{\tau_k}$` is a natural/Riemannian gradient evaluated at point `$\tau_{k}$` and $\alpha>0$ is a step-size.

The update in Eq. `$\eqref{1}$` is valid since the parameter space $\Omega_\tau$  has a local vector-space structure thanks to the use of an intrinsic parametrization.
However, when $\Omega_\tau$ is a proper subset of $T\mathcal{M}_{\tau_k}$ (i.e., `$\Omega_\tau \neq T\mathcal{M}_{\tau_k} $`), the update in Eq. `$\eqref{1}$` is valid only when the step-size $\alpha$ is small enough so that  `$\tau_{k+1} \in \Omega_\tau$`.

<span style="color:red">**Warning**</span>:
Using a small step-size could be an issue since it can greatly slow down the progression of natural-gradient
descent in practice.

>Example: (NGD in a constrained space)
>
>Consider a 1-dimensional  Gaussian family.
>We specify an intrinsic parameterization $\mathbf{\tau}$  as `$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with `$\tau = (\mu,\sigma) $`. <br /> 
>
>We have to properly select the step-size $\alpha$ for natural-gradient descent in  `$\eqref{1}$` due to the positivity constraint in $\sigma$.
>
>In multivariate Gaussian cases, we have to handle a positive-definite constraint.



# Natural-gradient Descent is Linearly Invariant
------

Recall that in [Part III]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#parameter-transform-and-invariance), we have shown that natural-gradients are invaraint under any intrinsic parameter transformation.
The parameter transformation can be non-linear.

It is natural to expect that natural-gradient descent has a similar property. However, natural-gradient descent is only invariant under  an intrinsic **linear** transformation. Note that Newton's method is also linearly invariant while Euclidean gradient descent is not.


<span style="color:red">**Warning**</span>: Be aware of the difference of the invaraince property between natural-gradient and natural-gradient descent. 


Let's consider the following (scalar) optimization problem on a manifold $\mathcal{M}$ with the Fisher-Rao metric $F$.
`$$
\begin{aligned}
\min_{x \in \mathcal{M}} h(x)
\end{aligned}\tag{2}\label{2}
$$` 

Note that $\mathcal{M}$ in general does not have a vector-space structure. 
We consider an intrinstic parameterization $\tau$ so that the parameter space $\Omega_\tau$ at least has a local  vector-space structure.
The problem in `$\eqref{2}$` can be re-expressed as below.
`$$
\begin{aligned}
\min_{\tau \in \Omega_\tau} h_\tau(\tau)
\end{aligned}
$$` where $h_\tau$ is the parameter representation of scalar smooth function $h$.

Natural gradient descent in this parameter space $\Omega_\tau$ is
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{3}\label{3}
$$` where `$\hat{\mathbf{g}}_{\tau_k} := [\mathbf{F}_\tau(\tau_k) ]^{-1} \nabla_\tau h_\tau(\tau_k)$` and the step-size $\alpha$ is small enough so that  $\tau_{k+1} \in \Omega_\tau$. 

Consider another intrinstic parameterization $\lambda$ so that $\lambda=\mathbf{U} \tau$, where $\mathbf{U}$ is a constant (square) invertible matrix. 
When $\lambda$ is a valid parameterization, we know that `$\{ \mathbf{U}\tau |\tau \in\Omega_\tau \} 	\cap \Omega_\lambda \neq \emptyset$`.
For simplicity,  we further assume `$\{ \mathbf{U}\tau |\tau \in\Omega_\tau \} = \Omega_\lambda$`, where $\Omega_\lambda$ is the parameter space of $\lambda$. In general, we could use a smaller parameter space either $\Omega_\lambda$ or $\Omega_\tau$ so that this additional assumption holds.

Natural gradient descent in this parameter space $\Omega_\lambda$ is
`$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} -  \alpha  \hat{\mathbf{g}}_{\lambda_k} 
\end{aligned}\tag{4}\label{4}
$$` where `$\hat{\mathbf{g}}_{\lambda_k} := [\mathbf{F}_\lambda(\lambda_k) ]^{-1} \nabla_\lambda h_\lambda(\lambda_k)$` 

Recall that we have the [transformation rule]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#transformation-rules-for-natural-gradients-and-euclidean-gradients) for natural gradients as
`$$
\begin{aligned}
\hat{\mathbf{g}}_\tau= \mathbf{Q}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$` where $Q_{ji}=\frac{\partial \tau^j(\lambda)}{\partial \lambda^i}$.

We can verify that $\mathbf{Q} = \mathbf{U}^{-1}$. Notice that $\tau_0 = \mathbf{U}^{-1} \lambda_0$ by construction.
The update in  `$\eqref{3}$` at iteration $k=1$ then can be re-expressed as
`$$
\begin{aligned}
\tau_{1} \leftarrow \tau_{0} -  \alpha  \hat{\mathbf{g}}_{\tau_0} = \mathbf{U}^{-1} \lambda_0 -  \alpha  \mathbf{U}^{-1}  \hat{\mathbf{g}}_{\lambda_0} = \mathbf{U}^{-1} \lambda_1
\end{aligned}
$$`

When `$\alpha$` is small enough, we have  `$\tau_1 \in \Omega_\tau$` and `$\lambda_1 \in \Omega_\lambda$`.
It is easy to show that `$\tau_k = \mathbf{U}^{-1} \lambda_k$` by induction.
Therefore, updates in `$\eqref{3}$` and `$\eqref{4}$` are equivalent.






# Riemannian Gradient Descent and its (Non-linear) Invariance
------
Now we discuss a gradient-based method that is invariant to any intrinsic parameter transformation.
We will first introduce the concept of a (one-dimensional) geodesic $\gamma(t)$, which is the "shortest curve" on a manifold with a Riemannian metric (e.g., the Fisher-Rao metric).
Recall that in  [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional) we only define a distance between two Riemannian gradients evaluated at the same point. We can use the length of a geodesic to define the distance between two points on the manifold, which is known as the Rao distance in statistics.

Consider an intrinsic parametrization $\tau$, where $\gamma_\tau(t)$ is the parameter representation of the geodesic.
To specify a geodesic, we need to provide a starting point $\tau_0$ on the manifold and a Riemannian gradient $\mathbf{v}_{\tau_0}$ evluated at point $\tau_0$.
The geodeisc is the solution of a system of second-order non-linear ordinary differential equations (ODE) with the following initial conditions.
`$$
\begin{aligned}
\gamma_\tau(0) = \tau_0; \,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$` where the geodesic is determined by the initial conditions and the domain `$\mathbf{I}_\tau$` of the geodesic contains 0 and 1.


We will use the following map to define Riemannian gradient descent without specifying complicated  differential equations (e.g., Christoffel symbols) in the geodesic.
We define a manifold expoential map at point $\tau_0$ for a manifold  via the geodesic as 
`$$
\begin{aligned}
\mathrm{Exp}_{\tau_0}\colon  T\mathcal{M}_{\tau_0} & \mapsto \mathcal{M}\\
\mathbf{v}_{\tau_0} & \mapsto \gamma_\tau(1) \,\,\,\, \textrm{s.t.} \,\,\,\,\,\, \gamma_\tau(0) = \tau_0;\,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$` Technically, we should require  manifold `$\mathcal{M}$` to be geodesically complete so that the domain of the expoential map is the whole tangent space. 
Equiavalently, `$\mathbf{I}_\tau $` is the whole `$\mathcal{R}^1$` space in such cases. 


Under intrinsic parametrization $\tau$, (exact) Riemannian gradient descent is defined as 
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \mathrm{Exp}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} ) 
\end{aligned}
$$`

The invariance of this update is due to the uniqueness of ODE and transformation rules for natural-gradients, Fisher information matrix, and Christoffel symbols. We will not discuss this further in this post to avoid complicated derivations. 
Although Riemannian gradient descent is nice, the expoential map or the geodesic often does not have a closed form expression. 
 

# Many faces of Natural-gradient Descent
------
## Natural-gradient Descent as Inexact Riemannian Gradient Descent

Natural-gradient descent can be viewed as a first-order (linear) approximation of the geodesic, which implies that natural-gradient descent is indeed an inexact Riemannian gradient update.
Natural-gradient descent is linearly invariant due to the approximation.

Consider a first-order Taylor approximation at $t=0$ of the geodesic shown below.
`$$
\begin{aligned}
\gamma_\tau(t) \approx  \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (t-0)  
\end{aligned}
$$` 

<span style="color:red">**Warning**</span>:
This approximation does not guarantee that the approximated geodesic stays on the manifold for all $t \neq 0$.

Recall that the  expoential map  is defined via the geodesic  `$\gamma_\tau(1)$`.
We can similarly define an approximated  expoential map (A.K.A. the Euclidean retraction map)  as
`$$
\begin{aligned}
\mathrm{Ret}_{\tau_0}(\mathbf{v}_{\tau_0}) := \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (1-0) =\tau_0 + \mathbf{v}_{\tau_0}
\end{aligned}
$$` 

Therefore, the inexact Riemannian gradient update is defined as 
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \mathrm{Ret}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} )  = \tau_k  - \alpha  \hat{\mathbf{g}}_{\tau_k}
\end{aligned}
$$` which recovers natural-gradient descent.

 
## Natural-gradient Descent as Unconstrained Proximal-gradient Descent

In this section, we will make an additional but key assumption: the parameter space `$\Omega_\tau=\mathcal{R}^K$` has a global (unconstrained) vector space structure.
This assumption, in general, does not hold. However, we present this viewpoint since it closely relates to optimization
methods.

As we mentioned before, the **distances** in the gradient space and the parameter space are defined differently. 
In the [previous section](#riemannian-gradient-descent-and-its-non-linear-invariance), we use the geodesic to define the distance between two points in a parameter space.

We could also use other "distances" denoted by `$\mathrm{D}(.,.)$` (e.g., Kullback–Leibler divergence or f-divergence) to define the length between two points in a parameter space.
Given such a  "distance", we can perform the unconstrained proximal-gradient descent as shown below.
`$$
\begin{aligned}
\tau_{k+1} = \arg\min_{y \in  \mathcal{R}^K  } \{ \langle \mathbf{g}_{\tau_k}, y\rangle   + \mathrm{D}(y,\tau_k) \}
\end{aligned}
$$` where `$\mathbf{g}_{\tau_k}$` is a Eulcidean gradient and the parameter space is unconstrained.


When `$\mathrm{D}(y,\tau_k)$` is a secord-order Taylor approximation of  the KL divergence `$\mathrm{KL} [q(w|\tau_k) || q(w|y)]$` at `$y=\tau_k$`, this  proximal-gradient descent  recovers natural-gradient descent.

In Part V, we will show that  `$\mathrm{D}(y,\tau_k)$` can also be an exact KL divergence for  expoential family.
In such cases,  unconstrained mirror descent also recovers natural-gradient descent.


<span style="color:red">**Warning**</span>:
The connection bewteen natural-gradient descent and proximal-gradient/mirror descent breaks down when
the parameter space `$\Omega_\tau$` is constrained and open. In constraint cases, these methods are distinct.



# Natural-gradient Descent in Non-intrinstic Parameter Spaces
------
As mentioned in [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations), an intrinsic parametrization creates a nice parameter space (e.g., a local vector space structure) and guarantees a non-singular FIM.
We now discuss issues when it comes to  natural-gradient descent over non-intrinsic parametrizations including overparameterization. 

1. We may not have a local vector space structure in a non-intrinstic parameter space. Therefore, natural-gradient descent in this parameter space is pointless since the updated parameter will leave the parameter space. Indeed, the FIM is not well-defined in such cases. We will illustrate this by an example.
Another example (Von Mises–Fisher family) will be given in Part V.
    >Example: (Invalid NGD)
>
>Consider Bernoulli family  $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big\| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \\}$ with parameter $\tau = (\pi_0,\pi_1)$.
> As we shown in [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#caveats-of-the-fisher-matrix-computation), the FIM is ill-defined due to this eqaulity constraint.
> Moreover, the NGD update will violate the eqaulity constraint.


2. The FIM is singular in a non-intrinstic space. In theory, Moore–Penrose inverse could be used to compute natural-gradients so that natural-gradient descent is linearly invariant in this case. However, Moore–Penrose inverse often has to use the singular value decomposition (SVD) and  destroies structures of the FIM.  In practice, the iteration cost of Moore–Penrose inverse is very high as illustrated in the following example. Moreover, natural-gradient descent in non-intrinsic parameter spaces could easily violate parameter constraints.
    >Example:  (Parameter constraint violation)
>
>Consider a univarate  Gaussian family with zero mean `$ \{ \mathcal{N}(w |0,  \sigma_1 \sigma_2) \Big|  \sigma_1>0, \sigma_2 > 0  \}$` with `$\tau = (\sigma_1,\sigma_2) $`, where `$ \sigma_1 \sigma_2$` is the varaince of this Gaussian family.
>
> Natural-gradient descent in this parameter space $\Omega_\tau$ could easily violate the parameter constraints when $\sigma_1$ is very closed to $0$ even when the variance $\sigma_1 \sigma_2 $ is big enough.
>
> On the other hand, consider an intrinsic parameterization of this family as `$ \{ \mathcal{N}(w |0,  \sigma) \Big|  \sigma > 0  \}$` with `$\lambda = \sigma $`, where `$ \sigma$` is the varaince of this Gaussian family.
> Natural-gradient descent in this parameter space $\Omega_\lambda$ less likely violate the parameter constraints when the variance $\sigma$ is big enough.
    
    >Example:  (High iteation cost)
>
>Consider a $d$-dimensional Gaussian mixture family `$ \{ \frac{1}{C} \sum_{k=1}^{C} \mathcal{N}(w |\mathbf{\mu}_k,  \mathbf{\Sigma}_k)  \Big|  \mathbf{\mu}_k \in \mathcal{R}^d,  \mathbf{\Sigma}_k  \succ \mathbf{0}  \}$` with `$\tau = \{ \mu_k,\Sigma_k\}_{k=1}^{C} $`. This is a non-intrinstic parameterization.
>
> If we  use the following initialization such that all $K$ components have the same mean $\mu_0$ and the same variance $\Sigma_0$, this family becomes a Gaussian family. In this case, the FIM of this mixture is singular.
> The iteration cost of natural-gradient descent in this parameter space $\Omega_\tau$ will be $O(C^3 d^6)$ if  Moore–Penrose inverse is employed.
>
> Now, consider the equivalent Gaussian family `$ \{ \mathcal{N}(w |\mathbf{\mu}_0,  \mathbf{\Sigma}_0)  \Big|  \mathbf{\mu}_0 \in \mathcal{R}^d,  \mathbf{\Sigma}_0  \succ \mathbf{0}  \}$` with `$\lambda =( \mu_0,\Sigma_0 ) $`, where $\lambda$ is an intrinsic parameterization of the Gaussian family.
>
> As we will shown in Part V, the iteration cost of natural-gradient descent in this parameter space $\Omega_\lambda$ will be $O(d^3)$ if we exploit structures of the exact non-singular FIM.


3. It is tempting to approximate the FIM by an emprical FIM with a scalar damping term and use Woodbury matrix identity to reduce the iteration cost of computing natural-gradients. However,
an emprical FIM can be a bad approximation to the exact FIM.
Moreover, damping introduces an additional tuning hyper-parameter and destories the linear invariance property of  natural-gradient descent.
 


# Euclidean Gradient Descent is NOT (Linearly) Invariant
------
For simplicity, consider an unconstrained optimization problem.
`$$
\begin{aligned}
\min_{\tau \in \mathcal{R}^K } h_\tau(\tau)
\end{aligned}
$$` 

Euclidean gradient descent (GD) in parametrization `$\tau$` is
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha {\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{5}\label{5}
$$` where `${\mathbf{g}}_{\tau_k} := \nabla_\tau h_\tau(\tau_k)$` is a Euclidean gradient.



Consider a reparametrization  `$\lambda$` so that `$\lambda=\mathbf{U} \tau$`, where `$\mathbf{U}$` is a constant (square) invertible matrix. 
`$$
\begin{aligned}
\min_{\lambda \in \mathcal{R}^K } h_\lambda(\lambda):= h_\tau( \mathbf{U}^{-1} \lambda)
\end{aligned}
$$` 

The Euclidean gradient descent (GD) in parametrization `$\lambda$` is
`$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} - \alpha {\mathbf{g}}_{\lambda_k} 
\end{aligned}\tag{6}\label{6}
$$` where `${\mathbf{g}}_{\lambda_k} := \nabla_\lambda h_\lambda(\lambda_k)$` is a Euclidean gradient.


Note that Euclidean gradients follow the [transformation rule]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#transformation-rules-for-natural-gradients-and-euclidean-gradients)  as

`$$
\begin{aligned}
\mathbf{g}_\tau^T =  \mathbf{g}_\lambda^T \mathbf{J}
\end{aligned}
$$` where  `$J_{ki}:=\frac{\partial \lambda^k(\tau) }{ \partial \tau^i }$`

We can verify that `$\mathbf{J}=\mathbf{U}$` and `$\mathbf{g}_\tau = \mathbf{U}^T \mathbf{g}_\lambda $`.

Notice that `$\tau_0 = \mathbf{U}^{-1} \lambda_0$` by construction.
The update in  `$\eqref{5}$` at iteration $k=1$ then can be re-expressed as
`$$
\begin{aligned}
\tau_{1} \leftarrow \tau_{0} -  \alpha  {\mathbf{g}}_{\tau_0} = \mathbf{U}^{-1} \lambda_0 -  \alpha  \mathbf{U}^{T}  {\mathbf{g}}_{\lambda_0} \neq \mathbf{U}^{-1} \lambda_1
\end{aligned}
$$`

It is easy to see that
updates in `$\eqref{5}$` and `$\eqref{6}$` are NOT equivalent.
Therefore,  Euclidean gradient descent is not invariant.




# Newton's Method is Linearly Invariant
------
For simplicity, consider an unconstrained convex optimization problem.
`$$
\begin{aligned}
\min_{\tau \in \mathcal{R}^K } h_\tau(\tau)
\end{aligned}
$$` where `$h_\tau(\tau)$` is strongly convex and twice continuously differentiable w.r.t. `$\tau$`.

Newton's method in parametrization `$\tau$` is
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \mathbf{H}^{-1}_\tau(\tau_k) {\mathbf{g}}_{\tau_k} 
\end{aligned}
$$` where `${\mathbf{g}}_{\tau_k} := \nabla_\tau h_\tau(\tau_k)$` is a Euclidean gradient and `$\mathbf{H}_\tau(\tau_k):=\nabla_\tau^2 h_\tau(\tau_k)$` is the Hessian.



Consider a reparametrization  `$\lambda$` so that `$\lambda=\mathbf{U} \tau$`, where `$\mathbf{U}$` is a constant (square) invertible matrix. 
`$$
\begin{aligned}
\min_{\lambda \in \mathcal{R}^K } h_\lambda(\lambda):= h_\tau( \mathbf{U}^{-1} \lambda)
\end{aligned}
$$` where `$h_\lambda(\lambda)$` is also stronly convex w.r.t. `$\lambda$` due to  `$\eqref{7}$`.

Newton's method  in parametrization `$\lambda$` is
`$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} - \alpha \mathbf{H}^{-1}_\lambda(\lambda_k) {\mathbf{g}}_{\lambda_k} 
\end{aligned} 
$$` where `${\mathbf{g}}_{\lambda_k} := \nabla_\lambda h_\lambda(\lambda_k)$` is a Euclidean gradient and
`$\mathbf{H}_\tau(\lambda_k):=\nabla_\lambda^2 h_\lambda(\lambda_k)$` is the Hessian.

As we discussed in the previous section, 
Euclidean gradients follow the [transformation rule]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#transformation-rules-for-natural-gradients-and-euclidean-gradients)  as `$\mathbf{g}_\tau^T =  \mathbf{g}_\lambda^T \mathbf{J}$`, where
`$\mathbf{J}=\mathbf{U}$`.

Surprisingly, for a linear transformation, the Hessian follows the [transformation rule]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#transformation-rule-for-the-fisher-information-matrix)  like the Fisher information
matrix as

`$$
\begin{aligned}
\mathbf{H}_{\tau} (\tau_k) &= \nabla_\tau ( \mathbf{g}_{\tau_k} ) \\
&=\nabla_\tau ( \mathbf{J}^T \mathbf{g}_{\lambda_k} ) \\
&=\mathbf{J}^T\nabla_\tau (  \mathbf{g}_{\lambda_k} ) \,\,\,\,\text{(for a linear transformation, } \mathbf{J} = \mathbf{U} \text{ is a
constant matrix)}   \\
&=\mathbf{J}^T\nabla_\lambda (  \mathbf{g}_{\lambda_k} )\mathbf{J} \\ 
&=\mathbf{J}^T \mathbf{H}_{\lambda} (\lambda_k)\mathbf{J} 
\end{aligned}\tag{7}\label{7}
$$`


<span style="color:red">**Warning**</span>: The Hessian is not valid metric since it does not follow the transformation
rule of a metric in non-linear cases.

Therefore, the direction in Newton's method denoted by `$\tilde{\mathbf{g}}_{\tau_k} := \mathbf{H}^{-1}_\tau(\tau_k) \mathbf{g}_{\tau_k}$` is transformed like natural-gradients in **linear** cases as

`$$
\begin{aligned}
\tilde{\mathbf{g}}_{\tau_k} &:= \mathbf{H}^{-1}_\tau(\tau_k) \mathbf{g}_{\tau_k} \\
&= [ \mathbf{J}^T \mathbf{H}_{\lambda} (\lambda_k)\mathbf{J} ]^{-1} \mathbf{g}_{\tau_k} \\
&=  \mathbf{J}^{-1} \mathbf{H}^{-1}_{\lambda} (\lambda_k)\mathbf{J}^{-T} [ \mathbf{J}^{T}\mathbf{g}_{\lambda_k} ] \\
&=  \mathbf{J}^{-1} \mathbf{H}^{-1}_{\lambda} (\lambda_k) \mathbf{g}_{\lambda_k}  \\
&=  \mathbf{J}^{-1}  \tilde{\mathbf{g}}_{\lambda_k}  \\
&=  \mathbf{Q}  \tilde{\mathbf{g}}_{\lambda_k}  \\
\end{aligned} 
$$` where by the definition we have `$\mathbf{Q}= \mathbf{J}^{-1}$`.


The consequence is that Newton's method like natural-gradient descent is linearly invariant.

