---
title: 'Part IV: Natural-Gradient Descent and  Riemannian Gradient Descent'
date: 2021-11-15
permalink: /posts/2021/11/Geomopt04/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Goal
------
This blog post should help readers to understand natural-gradient descent and Riemannian gradient descent.

We will give an informal introduction with a focus on high level of ideas.



# Two kinds of Spaces
------
As we disucssed in [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional), the parameter space $\Omega_\tau$ and the tangent space denoted by `$T\mathcal{M}_{\tau_0}$` at point $\tau_0$ are different spaces. Recall that the tangent space is a vector space and `$T\mathcal{M}_{\tau_0}=\mathcal{R}^K$` while the parameter space $\Omega_\tau$ is like a local vector space in `$\mathcal{R}^K$`, where $K$ is the dimension of the manifold. Moreover, $\Omega_\tau \subset T\mathcal{M}_{\tau_0}$ since  $\tau$ is an [intrinsic parametrization]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations).

The following figure illustrates the difference between the two spaces.

<img src="/img/sphere.png"  width="500"/>
 

# Natural-gradient Descent in an Intrinsic Parameter Space
------
Using intrinstic parametrization $\tau$, an intuitive update like the Euclidean case is natural-gradient descent.
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - t \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{1}\label{1}
$$` where `$\hat{\mathbf{g}}_{\tau_k}$` is a natural/Riemannian gradient evaluated at point `$\tau_{k}$`.

The update in Eq. `$\eqref{1}$` is valid since the parameter space $\Omega_\tau$  has a local vector-space structure due to the intrinsic parametrization.
However, when $\Omega_\tau$ is a proper subset of $T\mathcal{M}_{\tau_k}$ (i.e., `$\Omega_\tau \neq T\mathcal{M}_{\tau_k} $`), the update in Eq. `$\eqref{1}$` is valid only when the step-size $t$ is small enough so that  `$\tau_{k+1} \in \Omega_\tau$`.


>Example:
>
>Consider a 1-dimensional  Gaussian family.
>We specify an intrinsic parameterization $\mathbf{\tau}$  as `$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with `$\tau = (\mu,\sigma) $`. <br /> 
>
>We have to properly select the step-size $t$ for natural-gradient descent in  `$\eqref{1}$` due to the positivity constraint in $\sigma$.
>
>In multivariate Gaussian cases, we may have to handle a positive-definite constraint.



# Natural-gradient Descent is Linearly Invariant
Recall that in [Part III]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#Pparameter-transform-and-invariance), we show that natural-gradients are invaraint under an intrinsic parameter transform.
The parameter transform can be non-linear.

It is natural to expect that natural-gradient descent has a similar property. However, natural-gradient descent is only invariant under  an intrinsic **linear** transform.

Let's consider the following (scalar) optimization problem on a manifold $\mathcal{M}$ with the Fisher-Rao metric $F$.
`$$
\begin{aligned}
\min_{x \in \mathcal{M}} h(x)
\end{aligned}\tag{2}\label{2}
$$` 

Note that $\mathcal{M}$ in general does not have a vector-space structure. 
We consider an intrinstic parameterization $\tau$ so that the parameter space $\Omega_\tau$ at least has a local  vector-space structure.
The problem in `$\eqref{2}$` can be re-expressed as below.
`$$
\begin{aligned}
\min_{\tau \in \Omega_\tau} h_\tau(\tau)
\end{aligned}
$$` where $h_\tau$ is the parameter representation of scalar smooth function $h$.

Natural gradient descent in this parameter space $\Omega_\tau$ is
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - t \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{3}\label{3}
$$` where `$\hat{\mathbf{g}}_{\tau_k} := [\mathbf{F}_\tau(\tau_k) ]^{-1} \nabla_\tau h_\tau(\tau_k)$` and the step-size $t$ is small enough so that  $\tau_{k+1} \in \Omega_\tau$. 

Consider another intrinstic parameterization $\lambda$ so that $\lambda=\mathbf{U} \tau$, where $\mathbf{U}$ is a constant (square) invertible matrix. We further assume `$\{\mathbf{U}\tau |\tau \in\Omega_\tau \} \subset \Omega_\lambda$`, where $\Omega_\lambda$ is the parameter space of $\lambda$.

Natural gradient descent in this parameter space $\Omega_\lambda$ is
`$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} - t \hat{\mathbf{g}}_{\lambda_k} 
\end{aligned}\tag{4}\label{4}
$$` where `$\hat{\mathbf{g}}_{\lambda_k} := [\mathbf{F}_\lambda(\lambda_k) ]^{-1} \nabla_\lambda h_\lambda(\lambda_k)$` 

Recall that we have the [transform rule]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#parameter-transform-and-invariance) for natural gradients as
`$$
\begin{aligned}
\hat{\mathbf{g}}_\tau= \mathbf{Q}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$` where $Q_{ji}=\frac{\partial \tau^j(\lambda)}{\partial \lambda^i}$.

We can verify that $\mathbf{Q} = \mathbf{U}^{-1}$. Notice that $\tau_0 = \mathbf{U}^{-1} \lambda_0$. 
The update in  `$\eqref{3}$` at iteration $k=1$ can be re-expressed as
`$$
\begin{aligned}
\tau_{1} \leftarrow \tau_{0} - t \hat{\mathbf{g}}_{\tau_0} = \mathbf{U}^{-1} \lambda_0 - t \mathbf{U}^{-1}  \hat{\mathbf{g}}_{\lambda_0} 
\end{aligned}
$$`

Therefore, it is easy to show that $\tau_k = \mathbf{U}^{-1} \lambda_k$ and updates in `$\eqref{3}$` and `$\eqref{4}$` are equivalent when $t$ is small enough.

 



# Euclidean Gradient Descent is NOT (Linearly) Invariant


# Riemannian Gradient Descent and its (Non-linear) Invariance

to do:
intro exact geodesic (IVP) (without intro Christoffel symbols)

to do:
intro exact RGD

to do:
exact RGD is invariant (no proof)

 
# Natural-gradient Descent as an Approximated Method

to do:
ngd as a first-order approx

to do: all kinds of approx to get NGD (point distance != vector distance)



