---
title: 'Part IV: Natural and Riemannian  Gradient Descent'
date: 2021-11-15
permalink: /posts/2021/11/Geomopt04/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Warning: working in Progress (incomplete)

Goal
------
This blog post should help readers to understand natural-gradient descent and Riemannian gradient descent.

We will give an informal introduction with a focus on high level of ideas.




# Two kinds of Spaces
------
As we disucssed in [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional), the parameter space $\Omega_\tau$ and the tangent space denoted by `$T\mathcal{M}_{\tau_0}$` at point $\tau_0$ are different spaces. Recall that the tangent space is a vector space and `$T\mathcal{M}_{\tau_0}=\mathcal{R}^K$` while the parameter space $\Omega_\tau$ is like a local vector space in `$\mathcal{R}^K$`, where $K$ is the dimension of the manifold. Moreover, $\Omega_\tau \subset T\mathcal{M}_{\tau_0}$ since  $\tau$ is an [intrinsic parametrization]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations).

In a manifold case, we have to explicitly distinguish the difference between the representation of a point/parameter and a (Riemannian) vector/gradient.
The following figure illustrates the difference between the **domain** of these two spaces. Moreover, the **norm/distance** in each of these two spaces is defined differently. 


<img src="/img/sphere.png"  width="500"/>
 


# Natural-gradient Descent in an Intrinsic Parameter Space
------

Using intrinstic parametrization $\tau$, we can perform a natural-gradient update known as natural-gradient descent. 
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{1}\label{1}
$$` where `$\hat{\mathbf{g}}_{\tau_k}$` is a natural/Riemannian gradient evaluated at point `$\tau_{k}$` and $\alpha>0$ is a step-size.

The update in Eq. `$\eqref{1}$` is valid since the parameter space $\Omega_\tau$  has a local vector-space structure thanks to the use of an intrinsic parametrization.
However, when $\Omega_\tau$ is a proper subset of $T\mathcal{M}_{\tau_k}$ (i.e., `$\Omega_\tau \neq T\mathcal{M}_{\tau_k} $`), the update in Eq. `$\eqref{1}$` is valid only when the step-size $\alpha$ is small enough so that  `$\tau_{k+1} \in \Omega_\tau$`.


>Example:
>
>Consider a 1-dimensional  Gaussian family.
>We specify an intrinsic parameterization $\mathbf{\tau}$  as `$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with `$\tau = (\mu,\sigma) $`. <br /> 
>
>We have to properly select the step-size $\alpha$ for natural-gradient descent in  `$\eqref{1}$` due to the positivity constraint in $\sigma$.
>
>In multivariate Gaussian cases, we have to handle a positive-definite constraint.



# Natural-gradient Descent is Linearly Invariant
------

Recall that in [Part III]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#parameter-transform-and-invariance), we have shown that natural-gradients are invaraint under any intrinsic parameter transformation.
The parameter transformation can be non-linear.

It is natural to expect that natural-gradient descent has a similar property. However, natural-gradient descent is only invariant under  an intrinsic **linear** transformation. Note that Newton's method is also linearly invariant while Euclidean gradient descent is not.


<span style="color:red">**Warning**</span>: Be aware of the difference of the invaraince property between natural-gradient and natural-gradient descent. 


Let's consider the following (scalar) optimization problem on a manifold $\mathcal{M}$ with the Fisher-Rao metric $F$.
`$$
\begin{aligned}
\min_{x \in \mathcal{M}} h(x)
\end{aligned}\tag{2}\label{2}
$$` 

Note that $\mathcal{M}$ in general does not have a vector-space structure. 
We consider an intrinstic parameterization $\tau$ so that the parameter space $\Omega_\tau$ at least has a local  vector-space structure.
The problem in `$\eqref{2}$` can be re-expressed as below.
`$$
\begin{aligned}
\min_{\tau \in \Omega_\tau} h_\tau(\tau)
\end{aligned}
$$` where $h_\tau$ is the parameter representation of scalar smooth function $h$.

Natural gradient descent in this parameter space $\Omega_\tau$ is
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{3}\label{3}
$$` where `$\hat{\mathbf{g}}_{\tau_k} := [\mathbf{F}_\tau(\tau_k) ]^{-1} \nabla_\tau h_\tau(\tau_k)$` and the step-size $\alpha$ is small enough so that  $\tau_{k+1} \in \Omega_\tau$. 

Consider another intrinstic parameterization $\lambda$ so that $\lambda=\mathbf{U} \tau$, where $\mathbf{U}$ is a constant (square) invertible matrix. 
When $\lambda$ is a valid parameterization, we know that `$\{ \mathbf{U}\tau |\tau \in\Omega_\tau \} 	\cap \Omega_\lambda \neq \emptyset$`.
For simplicity,  we further assume `$\{ \mathbf{U}\tau |\tau \in\Omega_\tau \} = \Omega_\lambda$`, where $\Omega_\lambda$ is the parameter space of $\lambda$. In general, we could use a smaller parameter space either $\Omega_\lambda$ or $\Omega_\tau$ so that this additional assumption holds.

Natural gradient descent in this parameter space $\Omega_\lambda$ is
`$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} -  \alpha  \hat{\mathbf{g}}_{\lambda_k} 
\end{aligned}\tag{4}\label{4}
$$` where `$\hat{\mathbf{g}}_{\lambda_k} := [\mathbf{F}_\lambda(\lambda_k) ]^{-1} \nabla_\lambda h_\lambda(\lambda_k)$` 

Recall that we have the [transformation rule]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#parameter-transformation-and-invariance) for natural gradients as
`$$
\begin{aligned}
\hat{\mathbf{g}}_\tau= \mathbf{Q}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$` where $Q_{ji}=\frac{\partial \tau^j(\lambda)}{\partial \lambda^i}$.

We can verify that $\mathbf{Q} = \mathbf{U}^{-1}$. Notice that $\tau_0 = \mathbf{U}^{-1} \lambda_0$ by construction.
The update in  `$\eqref{3}$` at iteration $k=1$ then can be re-expressed as
`$$
\begin{aligned}
\tau_{1} \leftarrow \tau_{0} -  \alpha  \hat{\mathbf{g}}_{\tau_0} = \mathbf{U}^{-1} \lambda_0 -  \alpha  \mathbf{U}^{-1}  \hat{\mathbf{g}}_{\lambda_0} 
\end{aligned}
$$`

Therefore, it is easy to show that $\tau_k = \mathbf{U}^{-1} \lambda_k$ by induction. Updates in `$\eqref{3}$` and `$\eqref{4}$` are equivalent when $t$ is small enough.





# Euclidean Gradient Descent is NOT (Linearly) Invariant
to do:
add an exapmle


# Riemannian Gradient Descent and its (Non-linear) Invariance

Now we discuss a gradient-based method that is invariant to any intrinsic parameter transformation.
We will first introduce the concept of a (one-dimensional) geodesic $\gamma(t)$, which is the "shortest curve" on a manifold with a Riemannian metric (e.g., the Fisher-Rao metric).
Recall that in  [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional) we only define a distance between two Riemannian gradients evaluated at the same point. We can use the length of a geodesic to define the distance between two points on the manifold.

Consider an intrinsic parametrization $\tau$, where $\gamma_\tau(t)$ is the parameter representation of the geodesic.
To specify a geodesic, we need to provide a starting point $\tau_0$ on the manifold and a Riemannian gradient $\mathbf{v}_{\tau_0}$ evluated at point $\tau_0$.
The geodeisc is the solution of a system of second-order non-linear ordinary differential equations (ODE) with the following initial conditions.
`$$
\begin{aligned}
\gamma_\tau(0) = \tau_0; \,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$` where the geodesic is determined by the initial conditions and the domain `$\mathbf{I}_\tau$` of the geodesic contains 0 and 1.


We will use the following map to define Riemannian gradient descent without specifying complicated  differential equations (e.g., Christoffel symbols) in the geodesic.
We define a manifold expoential map at point $\tau_0$ for a manifold  via the geodesic as 
`$$
\begin{aligned}
\mathrm{Exp}_{\tau_0}\colon  T\mathcal{M}_{\tau_0} & \mapsto \mathcal{M}\\
\mathbf{v}_{\tau_0} & \mapsto \gamma_\tau(1) \,\,\,\, \textrm{s.t.} \,\,\,\,\,\, \gamma_\tau(0) = \tau_0;\,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$` Technically, we should require  manifold `$\mathcal{M}$` to be geodesically complete so that the domain of the expoential map is the whole tangent space. 
Equiavalently, `$\mathbf{I}_\tau $` is the whole `$\mathcal{R}^1$` space in such cases. 


Under intrinsic parametrization $\tau$, (exact) Riemannian gradient descent is defined as 
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \mathrm{Exp}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} ) 
\end{aligned}
$$`

The invariance of this update is due to the uniqueness of ODE and transformation rules for natural-gradients, Fisher information matrix, and Christoffel symbols. We will not discuss this further in this post to avoid complicated derivations. 
Although Riemannian gradient descent is nice, the expoential map or the geodesic often does not have a closed form expression. 
 
# Natural-gradient Descent as Inexact Riemannian Gradient Descent

Natural-gradient descent can be viewed as a first-order (linear) approximation of the geodesic, which implies that natural-gradient descent is indeed an inexact Riemannian gradient update.
Natural-gradient descent only is linearly invariant due to the approximation.

Consider a first-order Taylor approximation at $t=0$ of the geodesic shown below.
`$$
\begin{aligned}
\gamma_\tau(t) \approx  \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (t-0)  
\end{aligned}
$$` Note that this approximation does not guarantee that the approximated geodesic stays on the manifold for all $t \neq 0$.

Recall that the  expoential map  is defined via the geodesic  `$\gamma_\tau(1)$`.
We can similarly define an approximated  expoential map (A.K.A. the Euclidean retraction map)  as
`$$
\begin{aligned}
\mathrm{Ret}_{\tau_0}(\mathbf{v}_{\tau_0}) := \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (1-0) =\tau_0 + \mathbf{v}_{\tau_0}
\end{aligned}
$$` 

Therefore, the inexact Riemannian gradient update is defined as 
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \mathrm{Ret}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} )  = \tau_k  - \alpha  \hat{\mathbf{g}}_{\tau_k}
\end{aligned}
$$` which recovers natural-gradient descent.

 
# Issues of Natural-gradient Descent in Non-intrinsic Parameter Spaces
------
As mentioned in [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations), an intrinsic parametrization creates a nice parameter space (e.g., a local vector space structure) and guarantees a non-singular FIM.
We now discuss issues when it comes to  natural-gradient descent over non-intrinsic parametrizations including overparameterization. 

The first issue of using a non-intrinsic parametrization is that we may not have a local vector space structure in the parameter space. Therefore, natural-gradient descent in this parameter space is pointless since the updated parameter will leave the parameter space. Indeed, the FIM is not well-defined in such cases. We will illustrated by examples.

>Example: Bernoulli family:
>
>Consider Bernoulli family  $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big\| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \\}$ with parameter $\tau = (\pi_0,\pi_1)$.
> As we shown in [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#caveats-of-the-fisher-matrix-computation), the FIM is ill-defined due to this eqaulity constraint.


>Example: Von Mises–Fisher family:
>
>Consider $2$-dimensional Von Mises–Fisher  family  $ \\{  C(\kappa) \exp(\kappa (w_1\mu_1+w_2\mu_2) ) \Big\| \kappa>0, \mu_1^2+\mu_2^2 =1  \\}$ with parameter $\tau = (\kappa,\mu_1,\mu_2)$, where $ C(\kappa)$ is the normalization constant, $\mathbf{w}=(w_1,w_2)$ is a random unit vector defined in a circle,
$\kappa$ is a positive scalar, and `$\mathbf{\mu}=(\mu_1,\mu_2)$` is also a unit vector.
> We can show that the FIM is ill-defined under this parametrization due to this eqaulity constraint.



The second issue is that we have to deal with a singular FIM. In theory, Moore–Penrose inverse could be used to compute natural-gradients so that natural-gradient descent is linearly invariant in this case. However, Moore–Penrose inverse often has to use the singular value decomposition (SVD) and the singular FIM often destroy structures in a non-singular FIM.  In practice, the iteration cost of Moore–Penrose inverse is very high as illustrated in the following example. Moreover, natural-gradient descent in non-intrinsic parameter spaces could easily violate parameter constraints.

>Example:  (parameter constraint violation)
>
>Consider a univarate  Gaussian family with zero mean `$ \{ \mathcal{N}(w |0,  \sigma_1 \sigma_2) \Big|  \sigma_1>0, \sigma_2 > 0  \}$` with `$\tau = (\sigma_1,\sigma_2) $`, where `$ \sigma_1 \sigma_2$` is the varaince of this Gaussian family.
>
> Natural-gradient descent in this parameter space $\Omega_\tau$ could easily violate the parameter constraints when $\sigma_1$ is very closed to $0$ even when the variance $\sigma_1 \sigma_2 $ is big enough.
>
> Let's consider an intrinsic parameterization of this family as `$ \{ \mathcal{N}(w |0,  \sigma) \Big|  \sigma > 0  \}$` with `$\lambda = \sigma $`, where `$ \sigma$` is the varaince of this Gaussian family.
> Natural-gradient descent in this parameter space $\Omega_\lambda$ less likely violate the parameter constraints when the variance $\sigma_1 \sigma_2 $ is big enough. 
 

>Example:  (high iteation cost)
>
>Consider a $d$-dimensional Gaussian mixture family `$ \{ \frac{1}{C} \sum_{k=1}^{C} \mathcal{N}(w |\mathbf{\mu}_k,  \mathbf{\Sigma}_k)  \Big|  \mathbf{\mu}_k \in \mathcal{R}^d,  \mathbf{\Sigma}_k  \succ \mathbf{0}  \}$` with `$\tau = \{ \mu_k,\Sigma_k\}_{k=1}^{C} $`.
> If we  use the following initialization such that all $K$ components have the same mean $\mu_0$ and the same variance $\Sigma_0$, this family becomes a Gaussian family. In this case, the FIM of this mixture is singular.
>
> The iteration cost of natural-gradient descent in this parameter space $\Omega_\tau$ will be $O(C^3 d^6)$ if  Moore–Penrose inverse is employed.
>
> Now, consider the equivalent Gaussian family `$ \{ \mathcal{N}(w |\mathbf{\mu}_0,  \mathbf{\Sigma}_0)  \Big|  \mathbf{\mu}_0 \in \mathcal{R}^d,  \mathbf{\Sigma}_0  \succ \mathbf{0}  \}$` with `$\lambda =( \mu_0,\Sigma_0 ) $`, where $\lambda$ is an intrinsic parameterization of the Gaussian family.
>
> As we will shown in Part V, the iteration cost of natural-gradient descent in this parameter space $\Omega_\lambda$ will be $O(d^3)$ if we exploit structures of the exact non-singular FIM.




To reduce the iteration cost of computing natural-gradients, it is tempting to use an emprical FIM with a scalar damping term to handle the singularity of the FIM. However, damping introduces an additional tuning hyper-parameter and destories the linear invariance property of  natural-gradient descent.
 






# Natural-gradient Descent as an Proximal Gradient Method
As we mentioned before, the **distances** in  the gradient space and the parameter space are defined differently. 
In the [previous section](#riemannian-gradient-descent-and-its-non-linear-invariance), we can use the geodesic to define distance between two points in a parameter space.

We could also use other "distances" denoted by `$\mathrm{D}(.,.)$` (e.g., Kullback–Leibler divergence or f-divergence) to define the length between two points in a parameter space.
Given such of definition of "distance", we could obtain new gradient-based updates via the proximal gradient framework as shown below.
`$$
\begin{aligned}
\tau_{k+1} = \arg\max_{y \in  \Omega_\tau  } \{ \langle \mathbf{g}_{\tau_k}, y\rangle   + \mathrm{D}(y,\tau_k) \}
\end{aligned}
$$` where `$\mathbf{g}_{\tau_k}$` is a Eulcidean gradient.


When $\mathrm{D}(y,\tau_k)$ is a secord-order Taylor approximation of the KL divergence `$\mathrm{KL} [q(w|\tau_k) || q(w|y)]$` (or in general a f-divergence)  at $\tau_k$,  proximal gradient descent also recovers natural-gradient method when the parameter space $\Omega_\tau=\mathcal{R}^K$ is unconstrainted.

In Part V, we will show that proximal gradient descent recovers natural-gradient descent for expoential family when $\mathrm{D}(y,\tau_k)$  can also be the exact KL divergence and the parameter space $\Omega_\tau=\mathcal{R}^K$ is unconstrainted. 

In the expoential family cases, we will show in Part V that natural-gradient method becomes mirror descent under the natural-parameterization $\tau$ as long as the parameter space $\Omega_\tau=\mathcal{R}^K$ is unconstrainted.

