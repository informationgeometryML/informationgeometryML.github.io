---
title: 'Part IV: Natural and Riemannian  Gradient Descent'
date: 2021-11-15
permalink: /posts/2021/11/Geomopt04/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Goal
------
This blog post should help readers to understand natural-gradient descent and Riemannian gradient descent.

We will give an informal introduction with a focus on high level of ideas.

Working in Progress (incomplete)

# Two kinds of Spaces
------
As we disucssed in [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional), the parameter space $\Omega_\tau$ and the tangent space denoted by `$T\mathcal{M}_{\tau_0}$` at point $\tau_0$ are different spaces. Recall that the tangent space is a vector space and `$T\mathcal{M}_{\tau_0}=\mathcal{R}^K$` while the parameter space $\Omega_\tau$ is like a local vector space in `$\mathcal{R}^K$`, where $K$ is the dimension of the manifold. Moreover, $\Omega_\tau \subset T\mathcal{M}_{\tau_0}$ since  $\tau$ is an [intrinsic parametrization]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations).

The following figure illustrates the difference between the two spaces.

<img src="/img/sphere.png"  width="500"/>
 

# Natural-gradient Descent in an Intrinsic Parameter Space
------
Using intrinstic parametrization $\tau$, an intuitive update like the Euclidean case is natural-gradient descent.
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{1}\label{1}
$$` where `$\hat{\mathbf{g}}_{\tau_k}$` is a natural/Riemannian gradient evaluated at point `$\tau_{k}$` and $\alpha>0$ is a step-size.

The update in Eq. `$\eqref{1}$` is valid since the parameter space $\Omega_\tau$  has a local vector-space structure due to the intrinsic parametrization.
However, when $\Omega_\tau$ is a proper subset of $T\mathcal{M}_{\tau_k}$ (i.e., `$\Omega_\tau \neq T\mathcal{M}_{\tau_k} $`), the update in Eq. `$\eqref{1}$` is valid only when the step-size $\alpha$ is small enough so that  `$\tau_{k+1} \in \Omega_\tau$`.


>Example:
>
>Consider a 1-dimensional  Gaussian family.
>We specify an intrinsic parameterization $\mathbf{\tau}$  as `$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with `$\tau = (\mu,\sigma) $`. <br /> 
>
>We have to properly select the step-size $\alpha$ for natural-gradient descent in  `$\eqref{1}$` due to the positivity constraint in $\sigma$.
>
>In multivariate Gaussian cases, we may have to handle a positive-definite constraint.



# Natural-gradient Descent is Linearly Invariant
Recall that in [Part III]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#Pparameter-transform-and-invariance), we show that natural-gradients are invaraint under an intrinsic parameter transform.
The parameter transform can be non-linear.

It is natural to expect that natural-gradient descent has a similar property. However, natural-gradient descent is only invariant under  an intrinsic **linear** transform. Note that Newton's method is also linearly invariant while Euclidean gradient descent is not.

Let's consider the following (scalar) optimization problem on a manifold $\mathcal{M}$ with the Fisher-Rao metric $F$.
`$$
\begin{aligned}
\min_{x \in \mathcal{M}} h(x)
\end{aligned}\tag{2}\label{2}
$$` 

Note that $\mathcal{M}$ in general does not have a vector-space structure. 
We consider an intrinstic parameterization $\tau$ so that the parameter space $\Omega_\tau$ at least has a local  vector-space structure.
The problem in `$\eqref{2}$` can be re-expressed as below.
`$$
\begin{aligned}
\min_{\tau \in \Omega_\tau} h_\tau(\tau)
\end{aligned}
$$` where $h_\tau$ is the parameter representation of scalar smooth function $h$.

Natural gradient descent in this parameter space $\Omega_\tau$ is
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \tau_{k} - \alpha \hat{\mathbf{g}}_{\tau_k} 
\end{aligned}\tag{3}\label{3}
$$` where `$\hat{\mathbf{g}}_{\tau_k} := [\mathbf{F}_\tau(\tau_k) ]^{-1} \nabla_\tau h_\tau(\tau_k)$` and the step-size $\alpha$ is small enough so that  $\tau_{k+1} \in \Omega_\tau$. 

Consider another intrinstic parameterization $\lambda$ so that $\lambda=\mathbf{U} \tau$, where $\mathbf{U}$ is a constant (square) invertible matrix. 
When $\lambda$ is a valid parameterization, we know that `$\{ \mathbf{U}\tau |\tau \in\Omega_\tau \} 	\cap \Omega_\lambda \neq \emptyset$`.
For simplicity,  we further assume `$\{ \mathbf{U}\tau |\tau \in\Omega_\tau \} = \Omega_\lambda$`, where $\Omega_\lambda$ is the parameter space of $\lambda$. In general, we could use a smaller parameter space either $\Omega_\lambda$ or $\Omega_\tau$ so that this additional assumption holds.

Natural gradient descent in this parameter space $\Omega_\lambda$ is
`$$
\begin{aligned}
\lambda_{k+1} \leftarrow \lambda_{k} -  \alpha  \hat{\mathbf{g}}_{\lambda_k} 
\end{aligned}\tag{4}\label{4}
$$` where `$\hat{\mathbf{g}}_{\lambda_k} := [\mathbf{F}_\lambda(\lambda_k) ]^{-1} \nabla_\lambda h_\lambda(\lambda_k)$` 

Recall that we have the [transform rule]({{ site.baseurl }}{% post_url 2021-11-02-Geomopt03 %}#parameter-transform-and-invariance) for natural gradients as
`$$
\begin{aligned}
\hat{\mathbf{g}}_\tau= \mathbf{Q}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$` where $Q_{ji}=\frac{\partial \tau^j(\lambda)}{\partial \lambda^i}$.

We can verify that $\mathbf{Q} = \mathbf{U}^{-1}$. Notice that $\tau_0 = \mathbf{U}^{-1} \lambda_0$ by construction.
The update in  `$\eqref{3}$` at iteration $k=1$ then can be re-expressed as
`$$
\begin{aligned}
\tau_{1} \leftarrow \tau_{0} -  \alpha  \hat{\mathbf{g}}_{\tau_0} = \mathbf{U}^{-1} \lambda_0 -  \alpha  \mathbf{U}^{-1}  \hat{\mathbf{g}}_{\lambda_0} 
\end{aligned}
$$`

Therefore, it is easy to show that $\tau_k = \mathbf{U}^{-1} \lambda_k$ by induction. Updates in `$\eqref{3}$` and `$\eqref{4}$` are equivalent when $t$ is small enough.

 


# Euclidean Gradient Descent is NOT (Linearly) Invariant
to do:
add an exapmle


# Riemannian Gradient Descent and its (Non-linear) Invariance

Now we discuss a gradient-based method that is invariant to any intrinsic parameter transform.
We will first introduce the concept of a (one-dimensional) geodesic $\gamma(t)$, which is the "shortest curve" on a manifold.
To specify a geodesic, we need to provide a starting point $x_0$ on the manifold and a tangent vector $\mathbf{v}_{x_0}$ evluated at point $x_0$.
The geodeisc is a solution of a system of second-order non-linear ordinary differential equations (ODE) with the following initial conditions.
`$$
\begin{aligned}
\gamma(0) = x_0; \,\,\,\,\,\,
\frac{d \gamma(t) }{d t} \Big|_{t=0} = \mathbf{v}_{x_0}
\end{aligned}
$$` where the geodesic is determined by the initial conditions.


Consider an intrinsic parametrization $\tau$, we can re-expressed  the  initial conditions as
`$$
\begin{aligned}
\gamma_\tau(0) = \tau_0; \,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$`

We can define a manifold expoential map for a (geodesically complete) manifold  via the geodesic as 
`$$
\begin{aligned}
\mathrm{Exp}_{\tau_0}\colon  T\mathcal{M}_{\tau_0} & \mapsto \mathcal{M}\\
\mathbf{v}_{\tau_0} & \mapsto \gamma_\tau(1) \,\,\,\, \textrm{s.t.} \,\,\,\,\,\, \gamma_\tau(0) = \tau_0;\,\,\,\,\,\,
\frac{d \gamma_\tau(t) }{d t} \Big|_{t=0} = \mathbf{v}_{\tau_0}
\end{aligned}
$$` where the completeness is required if the domain of the expoential map is the whole tangent space.


We will use the expoential map to define Riemannian gradient descent without specifying complicated  differential equations (e.g., Christoffel symbols) in the geodesic.

Under intrinsic parametrization $\tau$, (exact) Riemannian gradient descent is defined as 
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \mathrm{Exp}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} ) 
\end{aligned}
$$`

The invariance of this update is due to the uniqueness of ODE and transform rules for natural-gradients, Fisher information matrix, and Christoffel symbols. We will not discuss this further in this post to avoid complicated derivations. 
Although Riemannian gradient descent is nice, the expoential map or the geodesic often does not have a closed form expression. 
 
# Natural-gradient Descent as an Approximated Method

Natural-gradient descent can be viewed as a first-order (linear) approximation of the geodesic, which implies that natural-gradient descent is indeed an inexact Riemannian gradient update.
Natural-gradient descent only is linearly invariant due to the approximation.

Consider a first-order Taylor approximation at $t=0$ of the geodesic shown below.
`$$
\begin{aligned}
\gamma_\tau(t) \approx \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (t-0)  
\end{aligned}
$$`

Recall that the  expoential map  is defined via the geodesic  `$\gamma_\tau(1)$`.
We can similarly define an approximated  expoential map (A.K.A. the Euclidean retraction map)  as
`$$
\begin{aligned}
\mathrm{Ret}_{\tau_0}(\mathbf{v}_{\tau_0}) := \gamma_\tau(0) + \frac{d  \gamma_\tau(t)}{d t} \Big|_{t=0} (1-0) =\tau_0 + \mathbf{v}_{\tau_0}
\end{aligned}
$$` 

Therefore, the inexact Riemannian gradient update is defined as 
`$$
\begin{aligned}
\tau_{k+1} \leftarrow \mathrm{Ret}_{\tau_k} (- \alpha  \hat{\mathbf{g}}_{\tau_k} )  = \tau_k  - \alpha  \hat{\mathbf{g}}_{\tau_k}
\end{aligned}
$$` which is eactly natural-gradient descent.

---
to do: all kinds of approx to get NGD (point distance != vector distance)
We can also show that NGD can be dervied from 
`$$
\begin{aligned}
\tau_{k+1} = \arg\max_{y \in \mathcal{R}^K } \{ \langle \mathbf{g}_{\tau_k}, y\rangle   + \mathrm{D}(y,\tau_k) \}
\end{aligned}
$$` where `$\mathbf{g}_{\tau_k}$` is a Eulcidean gradient and $\mathrm{D}(y,\tau_k)$ is a secord-order Taylor approximation of the KL divergence `$\mathrm{KL} [q(w|\tau_k) || q(w|y)]$`  at $\tau_k$

to do: mention  mirror descent and `$\mathrm{D}(y,\tau_k)$` becomes exact for expoential family (will be discussed more in Part V)



