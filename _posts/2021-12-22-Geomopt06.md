---
title: 'Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods'
date: 2021-12-22
permalink: /posts/2021/12/Geomopt06/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
  - Exponential Family
---

Warning: working in Progress (incomplete)

Goal
------
This blog post should show that we can efficiently implement natural-gradient methods in many cases.

We will give an informal introduction with a focus on high level of ideas.


# Handling Parameter Constraints
------
Unfortunately, the connection between natural-gradient descent and mirror desecent breaks down when the natural
parameter space `$\Omega_\eta$` is constrained.
Since the Legendre transformation is defined in `$\Omega_\eta$` , which implies the dual (expectation) parameter space `$\Omega_m$` is constrained in general.

The following example illustrates this point.

>Example: Univariate Gaussian family
>
> We consider this family as discussed in [the previous section](#exponential-family)
>`$ \{ \mathcal{N}(w |\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \}$` with mean $\mu$ and variance $\sigma$.
>
> It can be re-expressed in an exponential form as
>
>`$$
>\begin{aligned}
>p({w}|\mathbf{\eta})
>&= \underbrace{ \exp(0) }_{  h_\eta({w}) }  \exp( \langle \underbrace{\begin{bmatrix} -\frac{1}{2\sigma} \\ \frac{\mu}{\sigma}  \end{bmatrix}}_{\mathbf{\eta} }  ,  \underbrace{\begin{bmatrix} w^2 \\ w  \end{bmatrix}}_{ \mathbf{T}_\eta ({w}) } \rangle  -   \frac{1}{2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ]     )   \\
>\end{aligned}
>$$`
> where  `$\sigma= -\frac{1}{2\eta_1} $`,  `$\mu = -\frac{\eta_2}{2\eta_1}$`, and  `$A_\eta(\mathbf{\eta}) = \frac{1} {2} [ \log ( 2\pi ) + \log \sigma + \frac{\mu^2}{\sigma} ] = \frac{1}{2} [ \log ( 2\pi ) + \log (-\frac{1}{2\eta_1})-\frac{\eta_2^2}{2\eta_1} ] $`.
>
> The natural parameter space `$\Omega_\eta= \{ (\eta_1,\eta_2) | \eta_1<0 , \eta_2 \in \mathcal{R} \}$` is a constrained open set in $\mathcal{R}^2$, where $K=2$.
>
> The corresponding expectation parameter is `$\mathbf{m} = E_{q(w|\eta)}[ \mathbf{T}_\eta (w) ] = [ \mu^2+\sigma , \mu ] $`. 
>
> The expectation parameter space `$\Omega_m= \{ (m_1,m_2) | m_1 - m_2^2 >0 , m_2 \in \mathcal{R} \}$` is a constrained open set in $\mathcal{R}^2$.

Recall that when the natural parameter space `$\Omega_\eta$` is constrained,
the expectation space `$\Omega_m $` in general is also constrained (`$\Omega_m \neq \mathcal{R}^K$`).
In this case,
(constrained) mirror descent in `$\eqref{4}$` in general does not give us the same update in `$\eqref{5}$` as
natural-gradient descent.

> Proof by contradiction:
>
> Suppose when `$\Omega_m \neq \mathcal{R}^K$`,  `$\eqref{4}$` gives the same update in `$\eqref{5}$` in general. 
>
> By the definition of (constrained) mirror descent in `$\eqref{4}$`, the expectation parameter must satisfy `$\mathbf{m}_{k+1} \in \Omega_m$`.
>
> Therefore, the corresponding natural parameter must satisfy `$\mathbf{\eta}_{k+1} \in \Omega_\eta$`.
>
> By our hypothesis, `$\mathbf{\eta}_{k+1}$` is updated according to `$\eqref{5}$` and it must satisfy the natural parameter constraint.
>
> However, it is obvious that `$\eqref{5}$` in general does not satisfy the natural parameter constraint when the step-size $\alpha$ is
> large enough since `$\Omega_\eta$`  is just a proper open subset of `$\mathcal{R}^K$`.
>
> This is a contradiction.


The following example shows that it is also possible that
the natural parameter space `$\Omega_\eta$` is unconstrained (`$\Omega_\eta = \mathcal{R}^K$`) while
the expectation space `$\Omega_m $` is still constrained (`$\Omega_m \neq \mathcal{R}^K$`).


Recall that  in Part IV, we discuss 
[many faces of NGD]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#natural-gradient-faces-of-natural-gradient-descent) in unconstrained cases.  These methods could also be exteneded in constrained cases to handle the parameter constraint.


## Proximal NGD,  Projected NGD, and (Constrained) Mirror Descent

As we discussed before, natural-gradient descent and mirror desecent in general are **distinct** methods when the natural parameter space `$\Omega_\eta$` is constrained.

A straightforward approach from natural-gradient descent is the projected natural-gradient descent.
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \arg\min_{ \color{blue} {z} \in \Omega_\eta} \|\eta_k - \alpha
\mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k) -\color{blue} {\mathbf{z}} \|^2_{ \color{red}{ \mathbf{F}_\eta(\eta_k)} }
\end{aligned}\tag{6}\label{6}
$$` where we should use 
the [weighted inner product]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#distance-induced-by-the-fisher-rao-metric) with the same FIM highlighted in red.


On the other hand, the constrained mirror descent in the expectation space remains the same as in  `$\eqref{4}$`. 
`$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{ \color{blue} {x} \in \Omega_m}\{ \langle \nabla_m \ell(\mathbf{m}_k), \color{blue}{\mathbf{x}}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\color{blue}{\mathbf{x}},\mathbf{m}_k) \}
\end{aligned}
$$`
where 
`$\nabla_m \ell(\mathbf{m}_k) = \nabla_m f( \underbrace{ \eta(\mathbf{m}_k)}_{=\eta_k} )=  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$`.

We could also perform the constrained mirror descent in the natural parameter space as
`$$
\begin{aligned}
\mathbf{\eta}_{k+1} \leftarrow \arg \min_{\color{blue}{y} \in \Omega_\eta}\{ \langle \nabla_\eta f(\mathbf{\eta}_k), \color{blue}{\mathbf{y}}-\mathbf{\eta}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A_\eta}(\color{blue}{\mathbf{y}},\mathbf{\eta}_k) \}
\end{aligned}\tag{7}\label{7}
$$`


Recall that in
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#natural-gradient-descent-as-unconstrained-proximal-gradient-descent),
we show that natural-gradient descent can be viewed as an unconstrained proximal-gradient method, where we use the
second-order Taylor expansion of `$\mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\mathbf{y})]$` at `$y=\eta_k$`.
We could also obtain proximal natural-gradient descent without the Taylor expansion as

`$$
\begin{aligned}
\eta_{k+1} \leftarrow \arg\min_{\color{blue}{y} \in  \Omega_\eta  } \{ \langle \nabla_\eta f(\eta_k),\color{blue}{\mathbf{y}}-\eta_k \rangle   + \frac{1}{\alpha} \underbrace{ \mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\color{blue}{\mathbf{y}})]}_{ = \mathrm{B}_{A_\eta}(\mathbf{\eta}_k,\color{blue}{\mathbf{y}})}  \} 
\end{aligned}\tag{8}\label{8}
$$` 

These methods could be very difficult to solve since `$\Omega_m$` can be an arbitrary open subset in `$\mathcal{R}^K$`.
Moreover, in classical settings, a Bregman divergence is often defined in a closed set instead of an
open constrained subset. 


## Using an Adaptive Step-size

When the step-size `$\alpha$` is small enough, the connection between natural-gradient descent and mirror desecent could
still hold.

Therefore, one idea is to use an adaptive step-size to satisfy the parameter constraint at each iteration.
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha_k \nabla_m \ell(\mathbf{m}_k)
\end{aligned}\tag{9}\label{9}
$$` where 
`$\nabla_m \ell(\mathbf{m}_k) =  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$` and the step-size `$\alpha_k$` is selected  so that
`$\eta_{k+1} \in \Omega_\eta$`.

Since `$\Omega_m$` is an open set in `$\mathcal{R}^K$`, this update is valid when the step-size `$\alpha_k$` is small enough.

However, for a general parameter constraint `$\Omega_m$`, this approach can be inefficient due to the selection precedure and will often select an extremally small step-size
`$\alpha_k$`,
which greatly slows down the progression of the method.

## Riemannian Gradient Descent

An alternative approach is to use Riemannian gradient descent as we discussed in 
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#riemannian-gradient-descent-and-its-non-linear-invariance), which is a generalization of natural-gradient descent. 
Note that this approach cannot be derived from mirror descent.

To avoid solving the geodeisc ODE to get the manifold exponential map, we could use an (inexact) geodesic, which
induces a retraction map.
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \mathrm{Ret}_{\eta_k} (- \alpha  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k) )  
\end{aligned}\tag{10}\label{10}
$$` 

As mentioned in 
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#natural-gradient-descent-as-inexact-riemannian-gradient-descent),
we have to carefully select a retraction map to handle the parameter constraint.

For a general parameter constraint `$\Omega_m$`, it can be difficult to come out an efficient retraction map to satisfy
the constraint.
