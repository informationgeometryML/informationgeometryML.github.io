---
title: 'Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods'
date: 2021-12-22
permalink: /posts/2021/12/Geomopt06/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
  - Exponential Family
---

Warning: working in Progress (incomplete)

Goal
------
This blog post should show that we can efficiently implement natural-gradient methods in many cases.

We will give an informal introduction with a focus on high level of ideas.


# Handling Parameter Constraints
------

Recall that  in Part IV, we discuss 
[many faces of NGD]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#natural-gradient-faces-of-natural-gradient-descent) in unconstrained cases.  These methods could also be exteneded in constrained cases to handle the parameter constraint.


## Proximal NGD,  Projected NGD, and (Constrained) Mirror Descent

As we discussed before, natural-gradient descent and mirror desecent in general are **distinct** methods when the natural parameter space `$\Omega_\eta$` is constrained.

A straightforward approach from natural-gradient descent is the projected natural-gradient descent.
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \arg\min_{ \color{blue} {z} \in \Omega_\eta} \|\eta_k - \alpha
\mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k) -\color{blue} {\mathbf{z}} \|^2_{ \color{red}{ \mathbf{F}_\eta(\eta_k)} }
\end{aligned}\tag{6}\label{6}
$$` where we should use 
the [weighted inner product]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#distance-induced-by-the-fisher-rao-metric) with the same FIM highlighted in red.


On the other hand, the constrained mirror descent in the expectation space remains the same as in [Part V]({{ site.baseurl }}{% post_url 2021-12-14-Geomopt05 %}#natural-gradient-descent-as-mirror-descent).

`$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{ \color{blue} {x} \in \Omega_m}\{ \langle \nabla_m \ell(\mathbf{m}_k), \color{blue}{\mathbf{x}}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\color{blue}{\mathbf{x}},\mathbf{m}_k) \}
\end{aligned}
$$`
where 
`$\nabla_m \ell(\mathbf{m}_k) = \nabla_m f( \underbrace{ \eta(\mathbf{m}_k)}_{=\eta_k} )=  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$`.

We could also perform the constrained mirror descent in the natural parameter space as
`$$
\begin{aligned}
\mathbf{\eta}_{k+1} \leftarrow \arg \min_{\color{blue}{y} \in \Omega_\eta}\{ \langle \nabla_\eta f(\mathbf{\eta}_k), \color{blue}{\mathbf{y}}-\mathbf{\eta}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A_\eta}(\color{blue}{\mathbf{y}},\mathbf{\eta}_k) \}
\end{aligned}\tag{7}\label{7}
$$`


Recall that in
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#natural-gradient-descent-as-unconstrained-proximal-gradient-descent),
we show that natural-gradient descent can be viewed as an unconstrained proximal-gradient method, where we use the
second-order Taylor expansion of `$\mathrm{D}_f(\mathbf{y},\eta_k) = \mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\mathbf{y})]$` at `$y=\eta_k$`, where the KL divergence is also a f-divergence.


<div class="notice--success" markdown="1">
Note:

The KL divergence is the only divergence that is both a f-divergence and a Bregman divergence.
</div>

We could also obtain proximal natural-gradient descent without the Taylor expansion as below since the KL divergence is not symmetric.

`$$
\begin{aligned}
\eta_{k+1} \leftarrow \arg\min_{\color{blue}{y} \in  \Omega_\eta  } \{ \langle \nabla_\eta f(\eta_k),\color{blue}{\mathbf{y}}-\eta_k \rangle   + \frac{1}{\alpha} \underbrace{ \mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\color{blue}{\mathbf{y}})]}_{ = \mathrm{B}_{A_\eta}(\mathbf{\eta}_k,\color{blue}{\mathbf{y}})}  \} 
\end{aligned}\tag{8}\label{8}
$$` 

These methods could be very difficult to solve since `$\Omega_m$` can be an arbitrary open subset in `$\mathcal{R}^K$`.
Moreover, in classical settings, a Bregman divergence is often defined in a closed set instead of an
open constrained subset. 


## Using an Adaptive Step-size

When the step-size `$\alpha$` is small enough, the connection between natural-gradient descent and mirror desecent could
still hold.

Therefore, one idea is to use an adaptive step-size to satisfy the parameter constraint at each iteration.
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha_k \nabla_m \ell(\mathbf{m}_k)
\end{aligned}\tag{9}\label{9}
$$` where 
`$\nabla_m \ell(\mathbf{m}_k) =  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$` and the step-size `$\alpha_k$` is selected  so that
`$\eta_{k+1} \in \Omega_\eta$`.

Since `$\Omega_m$` is an open set in `$\mathcal{R}^K$`, this update is valid when the step-size `$\alpha_k$` is small enough.

However, for a general parameter constraint `$\Omega_m$`, this approach can be inefficient due to the selection precedure and will often select an extremally small step-size
`$\alpha_k$`,
which greatly slows down the progression of the method.

## Riemannian Gradient Descent

An alternative approach is to use Riemannian gradient descent as we discussed in 
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#riemannian-gradient-descent-and-its-non-linear-invariance), which is a generalization of natural-gradient descent. 
Note that this approach cannot be derived from mirror descent.

To avoid solving the geodeisc ODE to get the manifold exponential map, we could use an (inexact) geodesic, which
induces a retraction map.
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \mathrm{Ret}_{\eta_k} (- \alpha  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k) )  
\end{aligned}\tag{10}\label{10}
$$` 

As mentioned in 
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#natural-gradient-descent-as-inexact-riemannian-gradient-descent),
we have to carefully select a retraction map to handle the parameter constraint.

For a general parameter constraint `$\Omega_m$`, it can be difficult to come out an efficient retraction map to satisfy
the constraint.
