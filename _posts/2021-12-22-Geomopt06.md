---
title: 'Part VI: Handling Parameter Constraints of Exponential Family In Natural-gradient Methods'
date: 2021-12-22
permalink: /posts/2021/12/Geomopt06/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
  - Exponential Family
---

Warning: working in Progress (incomplete)


# Handling Parameter Constraints
------

Recall that  in Part IV, we discuss 
[many faces of NGD]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#many-faces-of-natural-gradient-descent) in unconstrained cases.  These methods could also be exteneded in constrained cases to handle the parameter constraint.

<div class="notice--success" markdown="1">
**Note**:

* Even when `$\Omega_m$` is constrained, updates in `$\eqref{1}$`, `$\eqref{2}$`, and `$\eqref{3}$` are
equivalent in exponential family cases as long as `$\Omega_\eta$` is unconstrained.

* When `$\Omega_\eta$` is constrained, these updates are **distinct** methods.

* In constrained cases, either  `$\Omega_m$` or `$\Omega_\eta$`  can be an arbitrary open subset in `$\mathcal{R}^K$`.  `$\eqref{1}$`, `$\eqref{2}$`, `$\eqref{3}$`, and `$\eqref{4}$` can be difficult to solve in general.
</div>


## Projected Natural Gradient Descent
A straightforward approach from natural-gradient descent is the projected natural-gradient descent.
`$$
\begin{aligned}
\eta_{k+1} & \leftarrow  \arg\min_{ \color{blue} {y} \in \Omega_\eta} \| \color{blue} {\mathbf{y}} - \eta_k + \alpha
\mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k) \|^2_{ \color{red}{ \mathbf{F}_\eta(\eta_k)} }\\
&= \arg\min_{ y \in \Omega_\eta} \big[ (\mathbf{y}-\eta_k) + \alpha\mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)
\big]^T \mathbf{F}_{\eta}(\eta_k) \big[ (\mathbf{y}-\eta_k) + \alpha\mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)
\big]\\
&= \arg\min_{ y \in \Omega_\eta} 2\alpha \big[ \frac{1}{2\alpha} (\mathbf{y}-\eta_k)^T \mathbf{F}_{\eta}(\eta_k) (\mathbf{y}-\eta_k) + (\mathbf{y}-\eta_k)^T  \nabla_\eta f(\eta_k) + \underbrace{ \frac{\alpha}{2} \nabla_\eta^T f(\eta_k) \mathbf{F}^{-1}_\eta(\eta_k) \nabla_\eta f(\eta_k)}_{\text{constant w.r.t. } y} \big] \\
&= \arg\min_{\color{blue}{y} \in  \Omega_\eta  } \{ \langle \nabla_\eta f(\eta_k),\color{blue}{\mathbf{y}}-\eta_k \rangle   + \frac{1}{2\alpha} (\mathbf{y}-\eta_k)^T \mathbf{F}_{\eta}(\eta_k) (\mathbf{y}-\eta_k) \} 
\end{aligned}\tag{1}\label{1}
$$` where we should use 
the [weighted inner product]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#distance-induced-by-the-fisher-rao-metric) with the same FIM highlighted in red.

This approach is closely related to proximial-gradient descent.
Recall that in
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#natural-gradient-descent-as-unconstrained-proximal-gradient-descent),
we show that natural-gradient descent can be viewed as an  proximal-gradient method, where we use the
second-order Taylor expansion of `$\mathrm{D}_f(\mathbf{y},\eta_k) = \mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\mathbf{y})]$` at `$y=\eta_k$`, where the KL divergence is  a f-divergence.

`$$
\begin{aligned}
\eta_{k+1} \leftarrow
\arg\min_{\color{blue}{y} \in  \Omega_\eta  } \{ \langle \nabla_\eta f(\eta_k),\color{blue}{\mathbf{y}}-\eta_k \rangle   + \frac{1}{2\alpha} (\mathbf{y}-\eta_k)^T \mathbf{F}_{\eta}(\eta_k) (\mathbf{y}-\eta_k) \} 
\end{aligned}
$$` 

## Proximal Gradient Descent

We could also obtain proximal natural-gradient descent without the Taylor expansion.
Recall that the KL divergence is also a Bregman divergence `$\mathrm{B}_{A_\eta}(\cdot,\cdot)$` equipped with the log-partition function `$A_\eta$`.

`$$
\begin{aligned}
\eta_{k+1} \leftarrow \arg\min_{\color{blue}{y} \in  \Omega_\eta  } \{ \langle \nabla_\eta f(\eta_k),\color{blue}{\mathbf{y}}-\eta_k \rangle   + \frac{1}{\alpha} \underbrace{ \mathrm{KL} [p(\mathbf{w}|\eta_k) || p(\mathbf{w}|\color{blue}{\mathbf{y}})]}_{ = \mathrm{B}_{A_\eta}(\mathbf{\eta}_k,\color{blue}{\mathbf{y}})}  \} 
\end{aligned}\tag{2}\label{2}
$$` 

<div class="notice--success" markdown="1">
**Note**:

The KL divergence is the only divergence that is both a Csiszar f-divergence and a Bregman divergence.
</div>



## Mirror Descent
Mirror descent in the expectation space remains the same as in [Part V]({{ site.baseurl }}{% post_url 2021-12-14-Geomopt05 %}#natural-gradient-descent-as-mirror-descent).
`$$
\begin{aligned}
\mathbf{m}_{k+1} \leftarrow \arg \min_{ \color{blue} {x} \in \Omega_m}\{ \langle \nabla_m \ell(\mathbf{m}_k), \color{blue}{\mathbf{x}}-\mathbf{m}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A^*_\eta}(\color{blue}{\mathbf{x}},\mathbf{m}_k) \}
\end{aligned}\tag{3}\label{3}
$$`
where 
`$\nabla_m \ell(\mathbf{m}_k) = \nabla_m f( \underbrace{ \eta(\mathbf{m}_k)}_{=\eta_k} )=  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)$`.


We could also perform the constrained mirror descent in the natural parameter space as
`$$
\begin{aligned}
\mathbf{\eta}_{k+1} \leftarrow \arg \min_{\color{blue}{y} \in \Omega_\eta}\{ \langle \nabla_\eta f(\mathbf{\eta}_k), \color{blue}{\mathbf{y}}-\mathbf{\eta}_k  \rangle + \frac{1}{\alpha}  \mathrm{B}_{A_\eta}(\color{blue}{\mathbf{y}},\mathbf{\eta}_k) \}
\end{aligned}\tag{4}\label{4}
$$` 

Clearly,  `$\eqref{2}$` and  `$\eqref{4}$`  gives distinct updates since the KL divergence is not symmetric.


## Adaptive Step-size Selection
Since `$\Omega_\eta$` is an open set in `$\mathcal{R}^K$`, the standard natural-gradient descent is still valid when the step-size `$\alpha_k$` is small enough.

One idea is to use an adaptive step-size for natural-gradient descent without a projection.
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \eta_k - \alpha_k \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k)
\end{aligned}\tag{5}\label{5}
$$` where  the step-size `$\alpha_k$` is selected  so that
`$\eta_{k+1} \in \Omega_\eta$`.


However, for a general parameter constraint `$\Omega_\eta$`, this approach results in a slow progression of the method.
The step-size selection precedure has to check the constraint at each iteration and could select an extremely small step-size
`$\alpha_k$`.

## Riemannian Gradient Descent

An alternative approach is to use Riemannian gradient descent as we discussed in 
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#riemannian-gradient-descent-and-its-non-linear-invariance), which is a generalization of natural-gradient descent. 
Note that this approach is completely different from mirror descent.

To avoid solving the geodeisc ODE to get the manifold exponential map, we could use an (inexact) geodesic, which
induces a retraction map.
`$$
\begin{aligned}
\eta_{k+1} \leftarrow \mathrm{Ret}_{\eta_k} (- \alpha  \mathbf{F}_\eta^{-1} (\eta_k) \nabla_\eta f(\eta_k) )  
\end{aligned}\tag{6}\label{6}
$$` 

As mentioned in 
[Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#natural-gradient-descent-as-inexact-riemannian-gradient-descent),
we have to carefully select a retraction map to handle the parameter constraint.
For a general parameter constraint `$\Omega_\eta$`, it can be difficult to come out an efficient retraction map to satisfy
the constraint.

For positive-definite constraints in `$\Omega_\eta$`, please see {% cite lin2020handling %} as an example to derive efficient Riemannian gradient updates.

------
# References
{% bibliography --cited %}

