---
title: 'Part III: Invariance of Natural-Gradients'
date: 2021-11-02
permalink: /posts/2021/11/Geomopt03/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---


Warning: working in Progress (incomplete)

Goal
------
This blog post should help readers to understand the invariance of natural-gradients.
We will also discuss why the Euclidean steepest direction is NOT invariant.

We will give an informal introduction with a focus on high level of ideas.



# Parameter Transformation and Invariance
------
In [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional), we show that a Riemannian gradient is a parametric representation of the tangent direction of a curve in a  manifold.
Since a curve and its tangent direction are geometric obejects, they should be invariant to the choice of parametrization.
In other words, geometric properties should be persevered in any valid coordinate system. This is a coordinate-free argument. 

The argument could be abstract for beginners. To be more concrete, we consider the corresponding coordinate-dependent argument: geometric properties should remain unchanged if we perform a (valid) parameter transformation.

In this blog post, we will show that two key **geometric properties** remains the same under an **intrinsic** parameter transformation.
1. Directional derivative 
2. Length of a Riemannian vector/gradient induced by the Fisher-Rao metric

Thanks to these properties, we can easily show that the optimal solution of the [Riemannian steepest direction]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-steepest-direction) considered in Part II is equivalent  under an intrinsic parameter transformation  since both the length and the directional derivative remain the same.


In [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional), we consider a point $\mathbf{x}_0$ in a manifold $\mathcal{M}$, a (1-dimensional) curve $\gamma(t)$, and a smooth scalar function $h: \mathcal{M} \to \mathcal{R}$.
Given an intrinsic parametrization $\tau$ containing the point, we consider the following parametric representations.
 
|   geometric object   |   parametric representation  |
|:------------|:-------------:|
| point `$\mathbf{x}_0$` |  `$\tau_0$`   | 
| curve  `$\gamma(t)$`  | `$\gamma_\tau(t) $`  | 
| function  `$h(x_0)$`  | `$h_\tau(\tau_0) $`  |   


Intuitively, the following identity should hold for any two (intrinsic) parametrizations $\tau$ and $\lambda$.
`$$
\begin{aligned}
h(\gamma(t)) = h_\tau(\gamma_\tau(t)) = h_\lambda(\gamma_\lambda(t))
\end{aligned}
$$` where we consider $t$ is fixed. 

Technically speaking,  domain `$\mathbf{I}_\tau$` of curve `$\gamma_\tau(t)$` and  domain `$\mathbf{I}_\lambda$` of curve `$\gamma_\lambda(t)$` may be different. However, both domains are open intervals containing 0 since both `$\gamma_\tau(0)=\tau_0$` and `$\gamma_\lambda(0)=\lambda_0$`  are parametric representations of the same point $\mathbf{x}_0$.


From the above expression, we can see that directional derivatives should be the same at $t=0$
`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} = \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} 
\end{aligned}\tag{1}\label{1}
$$`

In [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional), we have shown that 
`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &=  [\nabla h_\tau(\mathbf{\tau}_0)  ]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}   \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} & = [\nabla h_\lambda(\mathbf{\lambda}_0) ]^T  \frac{d {\gamma}_\lambda(t) }{d t} \Big|_{t=0}  
\end{aligned}
$$` where $\nabla$ is the standard (coordinate) derivative.


Recall that in [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional),  we have shown that `$\frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$` is a  parametric representation of a Riemannian vector, which is a Riemannian gradient.
Notice that `$\nabla h_\lambda(\mathbf{\lambda}_0)$` is a Euclidean gradient. 

We will use the following the notations to simplify expressions.

|  Notation   |   Meanings  |
|:------------|:-------------:|
| Euclidean gradient `$(g_\tau)_i$`  |  $i$-th entry  under parametrization $\tau$  | 
| Riemannian gradient `$(\hat{g}_\tau)^j$` |  $j$-th entry under parametrization $\tau$  | 
| Parameter `$\tau^j$` |  $j$-th parameter under parametrization   $\tau$  | 
   
Using these notations, the derivational derivatives then can be re-expressed as

`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &= \sum_{i}  (g_\tau)_i  (\hat{g}_\tau)^i  = {\mathbf{g}_\tau} \hat{\mathbf{g}}_\tau \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} & =\sum_{i}  (g_\lambda)_i (\hat{g}_\lambda)^i =  {\mathbf{g}}_\lambda \hat{\mathbf{g}}_\lambda
\end{aligned}\tag{2}\label{2}
$$`
where `$\mathbf{g}_\lambda$` and `$\mathbf{g}_\tau$` (e.g.,  `$\mathbf{g}_\tau=(\nabla h_\tau(\tau_0) )^T$`) are **row** vectors while `$\hat{\mathbf{g}}_\lambda$` and `$\hat{\mathbf{g}}_\tau$` (e.g., `$\hat{\mathbf{g}}_\tau=\mathbf{v}_\tau(\tau_0) =\frac{d \gamma_\tau(0) }{d t}$`) are **column** vectors. Moreover, `$\mathbf{g}_\lambda$` and `$\mathbf{g}_\tau$` are Euclidean gradients while  `$\hat{\mathbf{g}}_\lambda$` and `$\hat{\mathbf{g}}_\tau$`  are  Riemannian gradients.



By `$\eqref{1}$` and `$\eqref{2}$`, we have the following identity obtained from the **geometric property** of directional derivatives.
`$$
\begin{aligned}
 \mathbf{g}_\tau \hat{\mathbf{g}}_\tau  =  \mathbf{g}_\lambda \hat{\mathbf{g}}_\lambda 
\end{aligned}\tag{3}\label{3}
$$`




Now, we discuss the parameter transform between $\tau$ and $\lambda$, where we can express $\lambda$ in terms of $\tau$ denoted by $\lambda(\tau)$.


By the (standard) chain rule for a Euclidean gradient, we has
`$$
\begin{aligned}
(g_\tau)_i = \sum_{k}  \color{red}{ (g_\lambda)_k} \frac{\color{red}{ \partial \lambda^k(\tau) }}{ \partial \tau^i } 
\end{aligned}
\tag{4}\label{4}
$$` 


Let $J_{ki}:=\frac{\partial \lambda^k(\tau) }{ \partial \tau^i }$ denotes the $(k,i)$ entry of the Jacobian matrix. We can express the Jacobian matrix as below.

`$$
\begin{aligned}
\begin{matrix}
& \\
\mathbf{J} = 
    \left ( \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    i=1 & i=2 \\ \hline
    J_{11} & J_{12}  \\
   J_{21} & J_{22} \\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right )
    \begin{matrix}
    k=1 \\ k=2 
    \end{matrix}
\end{matrix}
\end{aligned}\tag{5}\label{5}
$$`

Eq. `$\eqref{4}$` gives us the transformation rule for Eulcidean gradients (denoted by a row vector)  as below in a vector form.

`$$
\begin{aligned}
\mathbf{g}_\tau =  \mathbf{g}_\lambda \mathbf{J}
\end{aligned},
$$`
where row vector `${\mathbf{g}}_\tau$` can be computed via a vector-Jacobian product in any standard Auto-Diff toolbox given that `${\mathbf{g}}_\lambda$` is pre-computed.



By Eq `$\eqref{3}$`, we obtain the transformation rule for Riemannian gradients as
`$$
\begin{aligned}
 \hat{\mathbf{g}}_\tau= \mathbf{J}^{-1}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$` where $\mathbf{Q}:=\mathbf{J}^{-1}$ is also a Jacobian matrix and $Q_{ki}=\frac{\partial \tau^k(\lambda)}{\partial \lambda^i}$ is the $(k,i)$ entry of matrix $\mathbf{Q}$.


The elementwise expression of the transformation rule for Riemannian gradients (denoted by a column vector) is
`$$
\begin{aligned}
(g_\tau)^k
 = \sum_{i}  \frac{ \partial \tau^k(\lambda)  }{\color{red} {\partial  \lambda^i} }  \color{red} {(g_\lambda)^i}
\end{aligned},
$$` 
where  `$\hat{\mathbf{g}}_\tau$` can be computed via a Jacobian-vector product used in forward-mode differentiation given that `$\hat{\mathbf{g}}_\lambda$` is pre-computed.

Note that these transformation rules are valid  when the Jacobian matrix is square and non-singular.
As we discussed in Part I about [intrinsic parameterizations]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations), the transformation map between $\tau$ and $\lambda$ must be bi-jective, which implies the Jacoabian matrix is square. 
Moreover, the map and its inverse map should be smooth, which implies that Jacobian matrix is well-defined and non-singular.


Finally, we give a transformation rule for the Fisher information matrix as defined at [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#fisher-rao-metric).


`$$
\begin{aligned}
 F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau) \Big) ]
\end{aligned}
$$`
Since $ \log p(w|\tau )$ can be considered as a scalar function $h$ defined on the manifold for any valid $w$, we have 
`$$
\begin{aligned}
 \log p(w|\tau_0  ) = h_\tau(\tau_0) = h_\lambda(\lambda_0) =  \log p(w|\lambda_0  )
\end{aligned}
$$`

Thus, the Fisher metric can be computed as
`$$
\begin{aligned}
 F_{ij}(\tau_0) &= E_{p(w|\tau_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ] \\
&=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ]\\
\end{aligned}
$$`

Recall that by the standard chain rule, we have 
`$$
\begin{aligned}
\partial_{\tau_i} \log p(w|\tau_0 ) = \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 )
\end{aligned}
$$`
 
Moreover, the Jacobian matrix does not depent on $w$. Therefore, we have
`$$
\begin{aligned}
 F_{ij}(\tau_0) 
&=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ]\\
&=  E_{p(w|\lambda_0) }  [ \Big( \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \sum_l \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  E_{p(w|\lambda_0) }  [ \Big(  \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } F_{kl}(\lambda_0)
\end{aligned}
$$`

We can re-express the above expression in a matrix form as below. This is the transformation rule for the Fisher information matrix.

`$$
\begin{aligned}
\mathbf{F}_{\tau} (\tau_0) = \underbrace{\mathbf{J}^T}_{  \frac{ \partial \lambda^i(\tau_0) }{ \partial \tau^k }  } \mathbf{F}_{\lambda} (\lambda_0) \underbrace{\mathbf{J}}_{  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  } 
\end{aligned}
$$`

By using this transformation rule, we can show that another **geometric property**: the length of a Riemannian vector is preserved.

We can see that the length of a Riemannian vector is also invariant.
`$$
\begin{aligned}
\| \hat{\mathbf{g}}_\tau \|^2_{F_\tau} &= [\hat{\mathbf{g}}_\tau]^T \mathbf{F}_{\tau} (\tau_0) \hat{\mathbf{g}}_\tau \\
&= [\mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda]^T \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda \\
&= [\hat{\mathbf{g}}_\lambda]^T [ \mathbf{J}^{-T}  \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} ] \hat{\mathbf{g}}_\lambda \\
&= [\hat{\mathbf{g}}_\lambda]^T  \mathbf{F}_{\lambda} (\lambda_0)  \hat{\mathbf{g}}_\lambda = \| \hat{\mathbf{g}}_\lambda \|^2_{F_\lambda}
\end{aligned}
$$`

# Riemannian Steepest Direction is Invariant
------
Now, we can show that the optimal solution of [Riemannian steepest direction]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-steepest-direction) considered in Part II  under parametrization $\tau$ and $\lambda$ are equivalent since both the length and the directional derivative remain the same.
`$$
\begin{aligned}
(\nabla  f_\tau(\mathbf{\tau}_0)  )^T   \mathbf{v}^{(opt)}_{\tau}  &=   (\nabla  f_\lambda(\mathbf{\lambda}_0) )^T \mathbf{v}^{(opt)}_{\lambda} & \,\,\, \text{(invariance of directional derivative)}  \\
[\nabla  f_\tau(\mathbf{\tau}_0)]^T & =  [\nabla  f_\lambda(\mathbf{\lambda}_0)]^T \mathbf{J} & \,\,\, \text{(parameter transformation for a Euclidean gradient)} \\
\|  \mathbf{v}^{(opt)}_{\tau} \|^2_{\color{red}{F_{\tau_0}}} & = \|  \mathbf{v}^{(opt)}_{\lambda} \|^2_{\color{red}{F_{\lambda_0}} } & \,\,\, \text{(invariance of the length of the Riemannian steepest direction)}  \\
\mathbf{v}^{(opt)}_{\tau} & = \mathbf{J}^{-1}  \mathbf{v}^{(opt)}_{\lambda} & \,\,\, \text{(parameter transformation for the Riemannian steepest direction)}  
\end{aligned}.
$$` 
In other words, the Riemannian steepest direction (which is indeed  a normalized Riemannian gradient) is transformed according to the transformation rule for a Riemannian gradient.

As we will discuss in [Part IV]({{ site.baseurl }}{% post_url 2021-11-15-Geomopt04 %}#natural-gradient-descent-is-linearly-invariant), this invariance property implies that natural-gradient descent is linearly invariant.
 
# Euclidean Steepest Direction is NOT Invariant
------
Recall that we have shown that a Euclidean gradient is the optimal solution of  [Euclidean steepest direction]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#euclidean-steepest-direction-and directional-derivative) in Part II.

By the standard chain rule, we have the parameter transformation rule of a Euclidean gradient as 

`$$
\begin{aligned}
\mathbf{g}_\tau =  \mathbf{g}_\lambda \mathbf{J}
\end{aligned},
$$` where `$\mathbf{g}_\tau$` and `$\mathbf{g}_\lambda$` are the same Euclidean gradient under parameterization `$\tau$` and `$\lambda$`, respectively,  and `$\mathbf{J}$` is the Jacobian matrix defined at Eq. `$\eqref{5}$`.

We can show that the (standard) length of a Euclidean gradient is not invariant under a parameter transformation due to the Jacobian matrix (i.e., `$\| \mathbf{g}_\tau \| \neq \| \mathbf{g}_\lambda \|$`). This is a reason why we use the [weighted inner product]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#distance-induced-by-the-fisher-rao-metric) to define the length of a gradient vector.

Let `$\mathbf{g}_\lambda:= (\nabla  f_\lambda(\mathbf{\lambda}_0) )^T$` and  `$\mathbf{g}_\tau:= (\nabla  f_\tau(\mathbf{\tau}_0) )^T = (\nabla  f_\lambda(\mathbf{\lambda}(\tau_0)) )^T$`


Recall that the optimal solution of the [Euclidean steepest direction]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#euclidean-steepest-direction-and-directional-derivative) is
`$$
\begin{aligned}
\mathbf{v}_{\lambda}^{(opt)}= -\frac{\nabla_\lambda f_\lambda(\mathbf{\lambda}_0) }{\|\nabla_\lambda f_\lambda(\mathbf{\lambda}_0)  \|} = -\frac{\mathbf{g}_\lambda^T}{\|\mathbf{g}_\lambda\|} \\
\mathbf{v}_{\tau}^{(opt)}= -\frac{\nabla_\tau f_\tau(\mathbf{\tau}_0) }{\|\nabla_\tau f_{\tau}(\mathbf{\tau}_0) \|} = -\frac{\mathbf{g}_\tau^T}{\|\mathbf{g}_\tau\|} 
\end{aligned}
$$`

The optimal solution does not obey the parameter transformation rule for Euclidean gradients.
`$$
\begin{aligned}
(\mathbf{v}_{\tau}^{(opt)})^T \neq (\mathbf{v}_{\lambda}^{(opt)})^T \mathbf{J} 
\end{aligned}
$$`


Moreover, the optimal value of the Euclidean steepest direction is not invariant under a parameter transformation as
`$$
\begin{aligned}
(\nabla  f_\lambda(\mathbf{\lambda}_0) )^T \mathbf{v}^{(opt)}_{\lambda}  = - \|\mathbf{g}_\lambda\| \neq - \|\mathbf{g}_\tau\| =(\nabla  f_\tau(\mathbf{\tau}_0) )^T \mathbf{v}^{(opt)}_{\tau} 
\end{aligned}
$$`  

In summary, the Euclidean steepest direction (which is a normalized Euclidean gradient) is NOT transformed according to the transformation rule for a Euclidean gradient.
This also implies that Euclidean gradient descent is not invariant under a parameter transformation. We will cover more about this in Part IV.








