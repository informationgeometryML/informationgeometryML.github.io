---
title: 'Part III: Invariance of Natural-Gradients'
date: 2021-11-02
permalink: /posts/2021/11/Geomopt03/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Goal
------
This blog post should help readers to understand the invariance of natural-gradients.
We will also discuss why standard Euclidean gradients are NOT invariant.

We will give an informal introduction with a focus on high level of ideas.



# Parameter Transform and Invariance
------
In [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional), we show that a Riemannian gradient is a parametric representation of the tangent direction of a curve in a  manifold.
Since a curve and its tangent direction are geometric obejects, they should be invariant to the choice of parametrization.
In other words, geometric properties should be persevered in any valid coordinate system. This is a coordinate-free argument. 

The argument could be abstract for beginners. To be more concrete, we consider the corresponding coordinate-dependent argument: geometric properties should remain unchanged if we perform a (valid) parameter transform.

In this blog post, we focus on two key **geometric properties**: 
1. Any directional derivative remains the same 
2. Length of a Riemannian vector/gradient remains the same

Thanks to these properties, we can easily show that the optimal solution of [Riemannian steepest direction]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-steepest-direction) considered in Part II  under any two  intrinsic parametrizations are equivalent since both the length and the directional derivative remain the same.



In [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional), we consider a point $\mathbf{x}_0$ in a manifold $\mathcal{M}$, a (1-dimensional) curve $\gamma(t)$, and a smooth scalar function $h: \mathcal{M} \to \mathcal{R}$.
Given an intrinsic parametrization $\tau$ containing the point, we consider the following parametric representations.
 
|   geometric object   |   parametric representation  |
|:------------|:-------------:|
| point `$\mathbf{x}_0$` |  `$\tau_0$`   | 
| curve  `$\gamma(t)$`  | `$\gamma_\tau(t) $`  | 
| function  `$h(x_0)$`  | `$h_\tau(\tau_0) $`  |   


Intuitively, the following identity should hold for any two (intrinsic) parametrizations $\tau$ and $\lambda$.
`$$
\begin{aligned}
h(\gamma(t)) = h_\tau(\gamma_\tau(t)) = h_\lambda(\gamma_\lambda(t))
\end{aligned}
$$` where we consider $t$ is fixed.

From the above expression, we can see that directional derivatives should be the same. 
`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} = \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} 
\end{aligned}\tag{1}\label{1}
$$`

In [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional), we have shown that 
`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &=  [\nabla h_\tau(\mathbf{\tau}_0)  ]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}   \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} & = [\nabla h_\lambda(\mathbf{\lambda}_0) ]^T  \frac{d {\gamma}_\lambda(t) }{d t} \Big|_{t=0}  
\end{aligned}
$$` where $\nabla$ is the standard (coordinate) derivate.


Recall that in [Part II]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-gradients-as-tangent-vectors-optional),  we have shown that `$\frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$` is a  parametric representation of a Riemannian vector, which is a Riemannian gradient.
Notice that `$\nabla h_\lambda(\mathbf{\lambda}_0)$` is a Euclidean gradient. 

We will use the following the notations to simplify the expressions.

|  Notation   |   Meanings  |
|:------------|:-------------:|
| Euclidean gradient `$(g_\tau)_i$`  |  $i$-th entry  under parametrization $\tau$  | 
| Riemannian gradient `$(\hat{g}_\tau)^j$` |  $j$-th entry under parametrization $\tau$  | 
| Parameter `$\tau^j$` |  $j$-th parameter under parametrization   $\tau$  | 
   
Using this notations, the derivational derivatives then can be re-expressed as

`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &= \sum_{i}  (g_\tau)_i  (\hat{g}_\tau)^i  = {\mathbf{g}_\tau} \hat{\mathbf{g}}_\tau \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} & =\sum_{i}  (g_\lambda)_i (\hat{g}_\lambda)^i =  {\mathbf{g}}_\lambda \hat{\mathbf{g}}_\lambda
\end{aligned}\tag{2}\label{2}
$$`
where `$\mathbf{g}_\lambda$` and `$\mathbf{g}_\tau$` (e.g.,  `$\mathbf{g}_\tau=(\nabla h_\tau(\tau_0) )^T$`) are **row** vectors while `$\hat{\mathbf{g}}_\lambda$` and `$\hat{\mathbf{g}}_\tau$` (e.g.,`$\hat{\mathbf{g}}_\tau=\mathbf{v}_\tau(\tau_0) =\frac{d \gamma_\tau(0) }{d t}$`) are **column** vectors. Moreover, `$\mathbf{g}_\lambda$` and `$\mathbf{g}_\tau$` are Euclidean gradients while  `$\hat{\mathbf{g}}_\lambda$` and `$\hat{\mathbf{g}}_\tau$`  are  Riemannian gradients.



By `$\eqref{1}$` and `$\eqref{2}$`, we have the following identity obtained from the **geometric property** of directional derivatives.
`$$
\begin{aligned}
 \mathbf{g}_\tau \hat{\mathbf{g}}_\tau  =  \mathbf{g}_\lambda \hat{\mathbf{g}}_\lambda 
\end{aligned}\tag{3}\label{3}
$$`




Now, we discuss the parameter transform between $\tau$ and $\lambda$, where we can express $\lambda$ in terms of $\tau$ denoted by $\lambda(\tau)$.


By the (standard) chain rule for a Euclidean gradient, we has
`$$
\begin{aligned}
(g_\tau)_i = \sum_{k}  \color{red}{ (g_\lambda)_k} \frac{\color{red}{ \partial \lambda^k(\tau) }}{ \partial \tau^i } 
\end{aligned}
\tag{4}\label{4}
$$` 


Let $J_{ki}:=\frac{\partial \lambda^k(\tau) }{ \partial \tau^i }$ denotes the $(k,i)$ entry of the Jacobian matrix. We can express the Jacobian matrix as below.

`$$
\begin{aligned}
\begin{matrix}
& \\
\mathbf{J} = 
    \left ( \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    i=1 & i=2 \\ \hline
    J_{11} & J_{12}  \\
   J_{21} & J_{22} \\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right )
    \begin{matrix}
    k=1 \\ k=2 
    \end{matrix}
\end{matrix}
\end{aligned}\tag{5}\label{5}
$$`

Eq. `$\eqref{4}$` gives us the transformation rule for Eulcidean gradients as below in a vector form.

`$$
\begin{aligned}
\mathbf{g}_\tau =  \mathbf{g}_\lambda \mathbf{J}
\end{aligned},
$$`
where `${\mathbf{g}}_\tau$` can be computed via a vector-Jacobian product in any standard Auto-Diff toolbox given that `${\mathbf{g}}_\lambda$` is pre-computed.



By Eq `$\eqref{3}$`, we obtain the transformation rule for Riemannian gradients as
`$$
\begin{aligned}
 \hat{\mathbf{g}}_\tau= \mathbf{J}^{-1}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$` where $\mathbf{Q}:=\mathbf{J}^{-1}$ is also a Jacobian matrix and $Q_{ki}=\frac{\partial \tau^k(\lambda)}{\partial \lambda^i}$ is the $(k,i)$ entry of the matrix.


The elementwise expression of the transformation rule for Riemannian gradients  is
`$$
\begin{aligned}
(g_\tau)^k
 = \sum_{i}  \frac{ \partial \tau^k(\lambda)  }{\color{red} {\partial  \lambda^i} }  \color{red} {(g_\lambda)^i}
\end{aligned},
$$` 
where `$\hat{\mathbf{g}}_\tau$` can be computed via a Jacobian-vector product used in forward-mode differentiation given that `$\hat{\mathbf{g}}_\lambda$` is pre-computed.

Note that these transformation rules are valid  when the Jacobian matrix is square and non-singular.
As we discussed in Part I about [intrinsic parameterizations]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations), the transformation map between $\tau$ and $\lambda$ must be bi-jective, which implies the Jacoabian matrix is square. 
Moreover, the map and its inverse map should be smooth, which implies that Jacobian matrix is well-defined and non-singular.


Finally, we give a transformation rule for the Fisher information matrix as defined at [Part I]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#fisher-rao-metric).


`$$
\begin{aligned}
 F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau) \Big) ]
\end{aligned}
$$`
Since $ \log p(w|\tau )$ can be considered as a scalar function $h$ defined on the manifold for any valid $w$, we have 
`$$
\begin{aligned}
 \log p(w|\tau_0  ) = h_\tau(\tau_0) = h_\lambda(\lambda_0) =  \log p(w|\lambda_0  )
\end{aligned}
$$`

Thus, the Fisher metric can be computed as
`$$
\begin{aligned}
 F_{ij}(\tau_0) &= E_{p(w|\tau_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ] \\
&=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ]\\
\end{aligned}
$$`

Recall that by the standard chain rule, we have 
`$$
\begin{aligned}
\partial_{\tau_i} \log p(w|\tau_0 ) = \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 )
\end{aligned}
$$`
 
Moreover, the Jacobian matrix does not depent on $w$. Therefore, we have
`$$
\begin{aligned}
 F_{ij}(\tau_0) 
&=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ]\\
&=  E_{p(w|\lambda_0) }  [ \Big( \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \sum_l \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  E_{p(w|\lambda_0) }  [ \Big(  \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } F_{kl}(\lambda_0)
\end{aligned}
$$`

We can re-express the above expression in a matrix form as below. This is this transformation rule for the Fisher information matrix.

`$$
\begin{aligned}
\mathbf{F}_{\tau} (\tau_0) = \underbrace{\mathbf{J}^T}_{  \frac{ \partial \lambda^i(\tau_0) }{ \partial \tau^k }  } \mathbf{F}_{\lambda} (\lambda_0) \underbrace{\mathbf{J}}_{  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  } 
\end{aligned}
$$`

By using this transformation rule, we can show that another **geometric property**: the length of a Riemannian vector is preserved.

We can see that the length of a Riemannian vector is also invariant.
`$$
\begin{aligned}
\| \hat{\mathbf{g}}_\tau \|^2_F &= [\hat{\mathbf{g}}_\tau]^T \mathbf{F}_{\tau} (\tau_0) \hat{\mathbf{g}}_\tau \\
&= [\mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda]^T \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda \\
&= [\hat{\mathbf{g}}_\lambda]^T [ \mathbf{J}^{-T}  \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} ] \hat{\mathbf{g}}_\lambda \\
&= [\hat{\mathbf{g}}_\lambda]^T  \mathbf{F}_{\lambda} (\lambda_0)  \hat{\mathbf{g}}_\lambda = \| \hat{\mathbf{g}}_\lambda \|^2_F
\end{aligned}
$$`


Finally, we can show that the optimal solution of [Riemannian steepest direction]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#riemannian-steepest-direction) consider in Part II  under parametrization $\tau$ and $\lambda$ are equivalent since both the length and the directional derivative remain the same.
`$$
\begin{aligned}
(\nabla  f_\tau(\mathbf{\tau}_0)  )^T   \mathbf{v}^{(opt)}_{\tau}  &=   (\nabla  f_\lambda(\mathbf{\lambda}_0) )^T \mathbf{v}^{(opt)}_{\lambda} & \,\,\, \text{(invariance of directional derivative)}  \\
\|  \mathbf{v}^{(opt)}_{\tau} \|^2_{\color{red}{F_\tau}} & = \|  \mathbf{v}^{(opt)}_{\lambda} \|^2_{\color{red}{F_\lambda} } & \,\,\, \text{(invariance of the length of a Riemannian gradient)}  \\
\mathbf{v}^{(opt)}_{\tau} & = \mathbf{J}^{-1}  \mathbf{v}^{(opt)}_{\lambda} & \,\,\, \text{(parameter transform for a Riemannian gradient)}  \\
[\nabla  f_\tau(\mathbf{\tau}_0)]^T & =  [\nabla  f_\lambda(\mathbf{\lambda}_0)]^T \mathbf{J} & \,\,\, \text{(parameter transform for a Euclidean gradient)} 
\end{aligned}.
$$` 


# Standard Euclidean Gradients are NOT Invariant
------
Recall that we have shown that a Euclidean gradient is the optimal solution of  [Euclidean steepest direction]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#euclidean-steepest-direction-and directional-derivative) in Part II.

By the standard chain rule, we have the parameter transform rule of a Euclidean gradient as 

`$$
\begin{aligned}
\mathbf{g}_\tau =  \mathbf{g}_\lambda \mathbf{J}
\end{aligned},
$$` where `$\mathbf{g}_\tau$` and `$\mathbf{g}_\lambda$` are the same Euclidean gradient under parameterization `$\tau$` and `$\lambda$`, respectively,  and `$\mathbf{J}$` is the Jacobian matrix defined at Eq. `$\eqref{5}$`.

We can show that the (standard) length of a Euclidean gradient is not invariant under a parameter transform due to the Jacobian matrix (i.e., `$\| \mathbf{g}_\tau \| \neq \| \mathbf{g}_\lambda \|$`). The main reason is that we should use the [weighted inner product]({{ site.baseurl }}{% post_url 2021-10-04-Geomopt02 %}#distance-induced-by-the-fisher-rao-metric) to define the length of a gradient vector.

Moreover, the optimal solution of the Euclidean steepest direction is not invariant under a parameter transform as shown below.
`$$
\begin{aligned}
(\nabla  f_\lambda(\mathbf{\lambda}_0) )^T \mathbf{v}^{(opt)}_{\lambda}  = - \|\mathbf{g}_\lambda\| \neq - \|\mathbf{g}_\tau\| =(\nabla  f_\tau(\mathbf{\tau}_0) )^T \mathbf{v}^{(opt)}_{\tau} 
\end{aligned}
$$` where `$\mathbf{g}_\lambda:= (\nabla  f_\lambda(\mathbf{\lambda}_0) )^T$` and  `$\mathbf{g}_\tau:= (\nabla  f_\tau(\mathbf{\tau}_0) )^T = (\nabla  f_\lambda(\mathbf{\lambda}(\tau_0)) )^T$`

This also implies that Euclidean gradient descent is not invariant under a parameter transform.




 
